{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new:\n",
    "    def __init__(self, input_layer_len, output_layer_len, hidden_layers_len,\n",
    "                 acti_funs,acti_funs_grad,cost_fun,cost_fun_grad,seed=123, problem=\"\"):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.acti_funs=acti_funs\n",
    "        self.acti_funs_grad=acti_funs_grad\n",
    "        \n",
    "        self.cost_fun=cost_fun\n",
    "        self.cost_fun_grad=cost_fun_grad\n",
    "        \n",
    "        if problem==\"classification\" or problem==\"C\":\n",
    "            self.acti_funs=self.acti_funs+[softmax]\n",
    "            self.acti_funs_grad=self.acti_funs_grad+[softmax_grad]\n",
    "            \n",
    "        if problem==\"regression\" or problem==\"R\":\n",
    "            self.acti_funs=self.acti_funs+[identity]\n",
    "            self.acti_funs_grad=self.acti_funs_grad+[identity_grad]\n",
    "            \n",
    "        \n",
    "        self.input_layer = np.zeros((1,input_layer_len))\n",
    "        self.output_layer = np.zeros((1,output_layer_len))\n",
    "        self.output_before =np.zeros((1,output_layer_len))\n",
    "        self.weights=[]\n",
    "        \n",
    "        self.weights.append(np.random.random((hidden_layers_len[0],input_layer_len))*2-1)\n",
    "        \n",
    "        for i in range(1,len(hidden_layers_len)):\n",
    "            self.weights.append(np.random.random((hidden_layers_len[i],hidden_layers_len[i-1]))*2-1)\n",
    "            \n",
    "        self.weights.append(np.random.random((output_layer_len,hidden_layers_len[-1]))*2-1)\n",
    "        \n",
    "        self.biases=[]\n",
    "        \n",
    "        for i in range(len(hidden_layers_len)):\n",
    "            self.biases.append(np.zeros((hidden_layers_len[i],1)))\n",
    "            \n",
    "        \n",
    "        self.biases.append(np.zeros((output_layer_len,1)))\n",
    "        \n",
    "        self.hidden_layers=[]\n",
    "        \n",
    "        for i in range(len(hidden_layers_len)):\n",
    "            self.hidden_layers.append(np.zeros((hidden_layers_len[i],1)))\n",
    "        \n",
    "    \n",
    "        \n",
    "    def calculate(self,input_layer,memory=False):\n",
    "        self.input_layer=input_layer\n",
    "        \n",
    "        input_layer=input_layer.T\n",
    "        \n",
    "        if(memory):\n",
    "            for i in range(len(self.weights)-1):\n",
    "                \n",
    "                self.hidden_layers[i]=self.weights[i].dot(input_layer)+self.biases[i]\n",
    "                input_layer=self.acti_funs[i](self.hidden_layers[i])\n",
    "                \n",
    "        else:    \n",
    "            for i in range(len(self.weights)-1):\n",
    "                input_layer=self.acti_funs[i](self.weights[i].dot(input_layer)+self.biases[i])\n",
    "                \n",
    "        \n",
    "       \n",
    "        output_layer=self.acti_funs[-1](self.weights[-1].dot(input_layer)+self.biases[-1])\n",
    "        self.output_before=(self.weights[-1].dot(input_layer)+self.biases[-1]).T\n",
    "        self.output_layer=output_layer.T\n",
    "        \n",
    "        return(output_layer.T)\n",
    "        \n",
    "    def train(self,train_input_layer,train_output_layer,max_epoch=2000,rate=0.1,beta=0,batch=1,stop=10**-6,RMSprop=False,L1=False,L2=False,eps=10**-8,alpha=0.01,\n",
    "              validate=False,early_stop=3,test_input_layer=0,test_output_layer=0,return_error_list=True,return_gradients=False,Verbose=True):\n",
    "    \n",
    "        #creating batches\n",
    "        lin=np.linspace(0,train_input_layer.shape[0],int(1/batch)+1).round().astype(int)\n",
    "        #initiating errors\n",
    "        output_layer=self.calculate(train_input_layer)\n",
    "        error=self.cost_fun(train_output_layer,output_layer)\n",
    "        #initiating early stopping\n",
    "        warning=0\n",
    "        if validate:\n",
    "            output_layer=self.calculate(test_input_layer)\n",
    "            error_test=self.cost_fun(test_output_layer,output_layer)\n",
    "        \n",
    "        print(\"start error \"+str(error))\n",
    "        #initiating old gradient list\n",
    "        old_grad=[[0]*(len(lin)-1) for x in range(len(self.weights))]\n",
    "        old_grad_bias=[[0]*(len(lin)-1) for x in range(len(self.weights))]\n",
    "        error_list=[error]\n",
    "        \n",
    "        \n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            \n",
    "            for b in range(len(lin)-1):\n",
    "\n",
    "                input_layer=train_input_layer[lin[b]:lin[b+1]]\n",
    "                output_layer=self.calculate(input_layer,memory=True)\n",
    "                \n",
    "                batch_size=output_layer.shape[0]\n",
    "                errors=[[0]*len(self.weights) for x in range(batch_size)]\n",
    "                \n",
    "                #backpropagate errors\n",
    "                for x in range(batch_size):\n",
    "                    #all cost funs should be independent from size of vectors because we rescale them later\n",
    "                    #last acti fun is used at output_layer so for classification it should be softmax and for regresion identity\n",
    "                    errors[x][-1]=((self.cost_fun_grad(train_output_layer[lin[b]:lin[b+1]][x:x+1],output_layer[x:x+1]))*self.acti_funs_grad[-1](self.output_before[x:x+1])).T\n",
    "                    for i in range(len(self.weights)-2,-1,-1): \n",
    "                        errors[x][i]=self.weights[i+1].T.dot(errors[x][i+1])*self.acti_funs_grad[i](self.hidden_layers[i][:,x:x+1])\n",
    "                \n",
    "                #calculate gradients\n",
    "                for l in range(1,len(self.weights)):\n",
    "                    #we rescale by batch size here\n",
    "                    #new_grad=np.clip(new_grad,-10**6,10**6) is used to prevent weights exploding\n",
    "                    #we update grads beta here is momentum\n",
    "                    if(RMSprop):\n",
    "                        new_grad=sum([errors[x][l].dot(self.acti_funs[l-1](self.hidden_layers[l-1][:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        #regularization\n",
    "                        if L1:\n",
    "                            new_grad+=alpha*np.sign(self.weights[l])#/batch_size\n",
    "                        elif L2:\n",
    "                            new_grad+=alpha*self.weights[l]#/batch_size\n",
    "                        #updating gradients\n",
    "                        old_grad[l][b]=beta*old_grad[l][b]+(1-beta)*new_grad**2\n",
    "                        self.weights[l]=self.weights[l]-new_grad*(rate/(np.sqrt(old_grad[l][b])+eps))\n",
    "\n",
    "                        new_grad_bias=sum([errors[x][l] for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        old_grad_bias[l][b]=beta*old_grad_bias[l][b]+(1-beta)*new_grad_bias**2\n",
    "                        self.biases[l]=self.biases[l]-new_grad_bias*(rate/(np.sqrt(old_grad_bias[l][b])+eps))\n",
    "                    else:\n",
    "                        new_grad=sum([errors[x][l].dot(self.acti_funs[l-1](self.hidden_layers[l-1][:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        if L1:\n",
    "                            new_grad+=alpha*np.sign(self.weights[l])#/batch_size\n",
    "                        elif L2:\n",
    "                            new_grad+=alpha*self.weights[l]#/batch_size\n",
    "                        new_grad=beta*old_grad[l][b]+(1-beta)*new_grad\n",
    "                        old_grad[l][b]=new_grad\n",
    "                        self.weights[l]=self.weights[l]-new_grad*rate\n",
    "\n",
    "                        new_grad_bias=sum([errors[x][l] for x in range(batch_size)])/batch_size\n",
    "                        new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                        new_grad_bias=beta*old_grad_bias[l][b]+(1-beta)*new_grad_bias\n",
    "                        old_grad_bias[l][b]=new_grad_bias\n",
    "                        self.biases[l]=self.biases[l]-new_grad_bias*rate\n",
    "                \n",
    "                #calculate gradient for input layer\n",
    "                if(RMSprop):\n",
    "                    new_grad=sum([errors[x][0].dot((self.input_layer.T[:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                    new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                    if L1:\n",
    "                        new_grad+=alpha*np.sign(self.weights[0])#/batch_size\n",
    "                    elif L2:\n",
    "                        new_grad+=alpha*self.weights[0]#/batch_size\n",
    "                    old_grad[0][b]=beta*old_grad[0][b]+(1-beta)*new_grad**2\n",
    "                    self.weights[0]=self.weights[0]-new_grad*(rate/(np.sqrt(old_grad[0][b])+eps))\n",
    "\n",
    "                    new_grad_bias=sum([errors[x][0] for x in range(batch_size)])/batch_size\n",
    "                    new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                    old_grad_bias[0][b]=beta*old_grad_bias[0][b]+(1-beta)*new_grad_bias**2\n",
    "                    self.biases[0]=self.biases[0]-new_grad_bias*(rate/(np.sqrt(old_grad_bias[0][b])+eps))\n",
    "                else:\n",
    "                    new_grad=sum([errors[x][0].dot((self.input_layer.T[:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                    new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                    if L1:\n",
    "                        new_grad+=alpha*np.sign(self.weights[0])#/batch_size\n",
    "                    elif L2:\n",
    "                        new_grad+=alpha*self.weights[0]#/batch_size\n",
    "                    new_grad=beta*old_grad[0][b]+(1-beta)*new_grad\n",
    "                    old_grad[0][b]=new_grad\n",
    "                    self.weights[0]=self.weights[0]-new_grad*rate\n",
    "\n",
    "                    new_grad_bias=sum([errors[x][0] for x in range(batch_size)])/batch_size\n",
    "                    new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                    new_grad_bias=beta*old_grad_bias[0][b]+(1-beta)*new_grad_bias\n",
    "                    old_grad_bias[0][b]=new_grad_bias\n",
    "                    self.biases[0]=self.biases[0]-new_grad_bias*rate\n",
    "\n",
    "\n",
    "            output_layer=self.calculate(train_input_layer)\n",
    "            new_error=self.cost_fun(train_output_layer,output_layer)\n",
    "            \n",
    "            #check stopping conditions if worse early_stop a row then stop\n",
    "            if validate:\n",
    "                output_layer=self.calculate(test_input_layer)\n",
    "                new_error_test=self.cost_fun(test_output_layer,output_layer)\n",
    "                if new_error_test>error_test:\n",
    "                    if warning>=early_stop:\n",
    "                        break\n",
    "                    else:\n",
    "                        error_test=new_error_test\n",
    "                        warning+=1\n",
    "                else:\n",
    "                    error_test=new_error_test\n",
    "                    warning=0\n",
    "                \n",
    "            if return_error_list:\n",
    "                error_list.append(new_error)\n",
    "            \n",
    "            if(epoch % 100 == 0 and Verbose):\n",
    "                print(\"epoch \"+str(epoch))\n",
    "                \n",
    "            if new_error<=0:\n",
    "                break\n",
    "            \n",
    "            if ((abs(new_error-error)/error)<stop).all():\n",
    "                break\n",
    "\n",
    "            error=new_error\n",
    "        \n",
    "        print(\"end in epoch \"+str(epoch)+ \" with error \"+str(new_error))\n",
    "        if return_error_list:\n",
    "            return error_list\n",
    "        elif return_gradients:\n",
    "            return old_grad, old_grad_bias\n",
    "        else:\n",
    "            return new_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(s):\n",
    "    s=softmax(s.T)\n",
    "    return (s*(1-s)).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_grad(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    e = np.exp(2*z)\n",
    "    return 1-2/(e+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_grad(z):\n",
    "    return 1-tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(Y,Y_hat):\n",
    "    return np.sum((Y-Y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad(Y,Y_hat):\n",
    "     return Y_hat-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(Y,Y_hat):\n",
    "    return np.sum(np.abs(Y-Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_grad(Y,Y_hat):\n",
    "    return (Y_hat>=Y)*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return(sigmoid(x)*(1-sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(targets,predictions,  epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = abs(np.sum(targets*np.log(predictions)))\n",
    "    return ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_grad(Y,Y_hat,eps=10**-10):\n",
    "    Y_hat=Y_hat+eps\n",
    "    return (Y_hat-eps-Y)/Y_hat/(1-Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_grad(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/train.csv\", delimiter=',', skiprows=1, usecols=range(0,785))\n",
    "np.random.shuffle(test)\n",
    "X=test[:,1:785]\n",
    "Y=test[:,0:1]\n",
    "Y=Y==np.unique(Y)\n",
    "Y=np.array(Y,dtype=int)\n",
    "train_input_layer=X/255\n",
    "train_output_layer=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/test.csv\", delimiter=',', skiprows=1, usecols=range(0,784))\n",
    "np.random.shuffle(test)\n",
    "X1=test[:,0:784]\n",
    "test_input_layer=X1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[64,64]\n",
    "mlp_test2=MLP_new(784,10,L,[tanh]*len(L),[tanh_grad]*len(L),cross_entropy,cross_entropy_grad,problem=\"C\",seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start error 263535.401168552\n",
      "epoch 100\n",
      "epoch 200\n",
      "epoch 300\n",
      "epoch 400\n",
      "epoch 500\n",
      "epoch 600\n",
      "epoch 700\n",
      "epoch 800\n",
      "epoch 900\n",
      "epoch 1000\n",
      "epoch 1100\n",
      "epoch 1200\n",
      "end in epoch 1200 with error 2845.0182058937567\n",
      "195 minutes\n",
      "accuracy_train 0.984\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (64,784) and (300,21000) not aligned: 784 (dim 1) != 300 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-87b30e28c7b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy_train \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_test2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mscore_validate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-09a12511d76e>\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self, input_layer, memory)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0minput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macti_funs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (64,784) and (300,21000) not aligned: 784 (dim 1) != 300 (dim 0)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "er=mlp_test2.train(train_input_layer,train_output_layer,max_epoch=1200,Verbose=True,beta=0.9,batch=0.05)\n",
    "end_time = time.time()\n",
    "t=end_time - start_time\n",
    "print(str(round(t/60))+\" minutes\")\n",
    "    \n",
    "output_layer=mlp_test2.calculate(train_input_layer)\n",
    "score_train=cross_entropy(train_output_layer,output_layer)/output_layer.shape[0]\n",
    "\n",
    "accuracy_train=round(np.mean(np.argmax(train_output_layer,axis=1)==np.argmax(output_layer,axis=1)),3)\n",
    "print(\"accuracy_train \"+str(accuracy_train))\n",
    "\n",
    "output_layer=mlp_test2.calculate(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer=mlp_test2.calculate(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9645984a60>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(X[1,:]).reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA to save time Validation and L2 to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/train.csv\", delimiter=',', skiprows=1)\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(test)\n",
    "X=test[:,1:]\n",
    "Y=test[:,0:1]\n",
    "Y=Y==np.unique(Y)\n",
    "Y=np.array(Y,dtype=int)\n",
    "X=X/255\n",
    "pca=PCA(n_components=300)\n",
    "pca=pca.fit(X)\n",
    "train_input_layer=pca.transform(X[::2,:])\n",
    "train_output_layer=Y[::2,:]\n",
    "validate_input=pca.transform(X[1::2,:])\n",
    "validate_output=Y[1::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/test.csv\", delimiter=',', skiprows=1)\n",
    "X1=test\n",
    "test_input_layer=X1/255\n",
    "test_input_layer=pca.transform(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[64,64]\n",
    "mlp_test=MLP_new(300,10,L,[tanh]*len(L),[tanh_grad]*len(L),cross_entropy,cross_entropy_grad,problem=\"C\",seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start error 148944.94132982014\n",
      "epoch 100\n",
      "epoch 200\n",
      "epoch 300\n",
      "epoch 400\n",
      "epoch 500\n",
      "epoch 600\n",
      "epoch 700\n",
      "epoch 800\n",
      "epoch 900\n",
      "epoch 1000\n",
      "epoch 1100\n",
      "epoch 1200\n",
      "end in epoch 1200 with error 4881.672218677337\n",
      "45 minutes\n",
      "accuracy_train 0.945\n",
      "accuracy_validate 0.934\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "er=mlp_test.train(train_input_layer,train_output_layer,max_epoch=1200,Verbose=True,beta=0.9,batch=0.05,validate=True, early_stop=10, test_input_layer=validate_input,test_output_layer=validate_output,L2=True,alpha=0.01)\n",
    "end_time = time.time()\n",
    "t=end_time - start_time\n",
    "print(str(round(t/60))+\" minutes\")\n",
    "    \n",
    "output_layer=mlp_test.calculate(train_input_layer)\n",
    "score_train=cross_entropy(train_output_layer,output_layer)/output_layer.shape[0]\n",
    "\n",
    "accuracy_train=round(np.mean(np.argmax(train_output_layer,axis=1)==np.argmax(output_layer,axis=1)),3)\n",
    "print(\"accuracy_train \"+str(accuracy_train))\n",
    "\n",
    "output_layer=mlp_test.calculate(validate_input)\n",
    "score_validate=cross_entropy(validate_output,output_layer)/output_layer.shape[0]\n",
    "\n",
    "accuracy_validate=round(np.mean(np.argmax(validate_output,axis=1)==np.argmax(output_layer,axis=1)),3)\n",
    "print(\"accuracy_validate \"+str(accuracy_validate))\n",
    "\n",
    "output_layer=mlp_test.calculate(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23246058184177795"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26175895548278344"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"hidden1\",mlp_test.hidden_layers[0])\n",
    "np.savetxt(\"hidden2\",mlp_test.hidden_layers[1])\n",
    "np.savetxt(\"bias1\",mlp_test.biases[0])\n",
    "np.savetxt(\"bias2\",mlp_test.biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "{\"ImageId\" : np.arange(1,output_layer.shape[0]+1),\n",
    "\"Label\" : np.argmax(output_layer,axis=1)},\n",
    "index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submition3.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Label.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
