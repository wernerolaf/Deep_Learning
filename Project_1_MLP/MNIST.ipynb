{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_new:\n",
    "    def __init__(self, input_layer_len, output_layer_len, hidden_layers_len,\n",
    "                 acti_funs,acti_funs_grad,cost_fun,cost_fun_grad,seed=123, problem=\"\"):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.acti_funs=acti_funs\n",
    "        self.acti_funs_grad=acti_funs_grad\n",
    "        \n",
    "        self.cost_fun=cost_fun\n",
    "        self.cost_fun_grad=cost_fun_grad\n",
    "        \n",
    "        if problem==\"classification\" or problem==\"C\":\n",
    "            self.acti_funs=self.acti_funs+[softmax]\n",
    "            self.acti_funs_grad=self.acti_funs_grad+[softmax_grad]\n",
    "            \n",
    "        if problem==\"regression\" or problem==\"R\":\n",
    "            self.acti_funs=self.acti_funs+[identity]\n",
    "            self.acti_funs_grad=self.acti_funs_grad+[identity_grad]\n",
    "            \n",
    "        \n",
    "        self.input_layer = np.zeros((1,input_layer_len))\n",
    "        self.output_layer = np.zeros((1,output_layer_len))\n",
    "        self.output_before =np.zeros((1,output_layer_len))\n",
    "        self.weights=[]\n",
    "        \n",
    "        self.weights.append(np.random.random((hidden_layers_len[0],input_layer_len))*2-1)\n",
    "        \n",
    "        for i in range(1,len(hidden_layers_len)):\n",
    "            self.weights.append(np.random.random((hidden_layers_len[i],hidden_layers_len[i-1]))*2-1)\n",
    "            \n",
    "        self.weights.append(np.random.random((output_layer_len,hidden_layers_len[-1]))*2-1)\n",
    "        \n",
    "        self.biases=[]\n",
    "        \n",
    "        for i in range(len(hidden_layers_len)):\n",
    "            self.biases.append(np.zeros((hidden_layers_len[i],1)))\n",
    "            \n",
    "        \n",
    "        self.biases.append(np.zeros((output_layer_len,1)))\n",
    "        \n",
    "        self.hidden_layers=[]\n",
    "        \n",
    "        for i in range(len(hidden_layers_len)):\n",
    "            self.hidden_layers.append(np.zeros((hidden_layers_len[i],1)))\n",
    "        \n",
    "    \n",
    "        \n",
    "    def calculate(self,input_layer,memory=False):\n",
    "        self.input_layer=input_layer\n",
    "        \n",
    "        input_layer=input_layer.T\n",
    "        \n",
    "        if(memory):\n",
    "            for i in range(len(self.weights)-1):\n",
    "                \n",
    "                self.hidden_layers[i]=self.weights[i].dot(input_layer)+self.biases[i]\n",
    "                input_layer=self.acti_funs[i](self.hidden_layers[i])\n",
    "                \n",
    "        else:    \n",
    "            for i in range(len(self.weights)-1):\n",
    "                input_layer=self.acti_funs[i](self.weights[i].dot(input_layer)+self.biases[i])\n",
    "                \n",
    "        \n",
    "       \n",
    "        output_layer=self.acti_funs[-1](self.weights[-1].dot(input_layer)+self.biases[-1])\n",
    "        self.output_before=(self.weights[-1].dot(input_layer)+self.biases[-1]).T\n",
    "        self.output_layer=output_layer.T\n",
    "        \n",
    "        return(output_layer.T)\n",
    "        \n",
    "    def train(self,train_input_layer,train_output_layer,max_epoch=2000,rate=0.1,beta=0,batch=1,stop=10**-6,RMSprop=False,L1=False,L2=False,eps=10**-8,alpha=0.01,\n",
    "              validate=False,early_stop=3,test_input_layer=0,test_output_layer=0,return_error_list=True,return_gradients=False,Verbose=True):\n",
    "    \n",
    "        #creating batches\n",
    "        lin=np.linspace(0,train_input_layer.shape[0],int(1/batch)+1).round().astype(int)\n",
    "        #initiating errors\n",
    "        output_layer=self.calculate(train_input_layer)\n",
    "        error=self.cost_fun(train_output_layer,output_layer)\n",
    "        #initiating early stopping\n",
    "        warning=0\n",
    "        if validate:\n",
    "            output_layer=self.calculate(test_input_layer)\n",
    "            error_test=self.cost_fun(test_output_layer,output_layer)\n",
    "        \n",
    "        print(\"start error \"+str(error))\n",
    "        #initiating old gradient list\n",
    "        old_grad=[[0]*(len(lin)-1) for x in range(len(self.weights))]\n",
    "        old_grad_bias=[[0]*(len(lin)-1) for x in range(len(self.weights))]\n",
    "        error_list=[error]\n",
    "        \n",
    "        \n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            \n",
    "            for b in range(len(lin)-1):\n",
    "\n",
    "                input_layer=train_input_layer[lin[b]:lin[b+1]]\n",
    "                output_layer=self.calculate(input_layer,memory=True)\n",
    "                \n",
    "                batch_size=output_layer.shape[0]\n",
    "                errors=[[0]*len(self.weights) for x in range(batch_size)]\n",
    "                \n",
    "                #backpropagate errors\n",
    "                for x in range(batch_size):\n",
    "                    #all cost funs should be independent from size of vectors because we rescale them later\n",
    "                    #last acti fun is used at output_layer so for classification it should be softmax and for regresion identity\n",
    "                    errors[x][-1]=((self.cost_fun_grad(train_output_layer[lin[b]:lin[b+1]][x:x+1],output_layer[x:x+1]))*self.acti_funs_grad[-1](self.output_before[x:x+1])).T\n",
    "                    for i in range(len(self.weights)-2,-1,-1): \n",
    "                        errors[x][i]=self.weights[i+1].T.dot(errors[x][i+1])*self.acti_funs_grad[i](self.hidden_layers[i][:,x:x+1])\n",
    "                \n",
    "                #calculate gradients\n",
    "                for l in range(1,len(self.weights)):\n",
    "                    #we rescale by batch size here\n",
    "                    #new_grad=np.clip(new_grad,-10**6,10**6) is used to prevent weights exploding\n",
    "                    #we update grads beta here is momentum\n",
    "                    if(RMSprop):\n",
    "                        new_grad=sum([errors[x][l].dot(self.acti_funs[l-1](self.hidden_layers[l-1][:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        #regularization\n",
    "                        if L1:\n",
    "                            new_grad+=alpha*np.sign(self.weights[l])#/batch_size\n",
    "                        elif L2:\n",
    "                            new_grad+=alpha*self.weights[l]#/batch_size\n",
    "                        #updating gradients\n",
    "                        old_grad[l][b]=beta*old_grad[l][b]+(1-beta)*new_grad**2\n",
    "                        self.weights[l]=self.weights[l]-new_grad*(rate/(np.sqrt(old_grad[l][b])+eps))\n",
    "\n",
    "                        new_grad_bias=sum([errors[x][l] for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        old_grad_bias[l][b]=beta*old_grad_bias[l][b]+(1-beta)*new_grad_bias**2\n",
    "                        self.biases[l]=self.biases[l]-new_grad_bias*(rate/(np.sqrt(old_grad_bias[l][b])+eps))\n",
    "                    else:\n",
    "                        new_grad=sum([errors[x][l].dot(self.acti_funs[l-1](self.hidden_layers[l-1][:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                        new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                        if L1:\n",
    "                            new_grad+=alpha*np.sign(self.weights[l])#/batch_size\n",
    "                        elif L2:\n",
    "                            new_grad+=alpha*self.weights[l]#/batch_size\n",
    "                        new_grad=beta*old_grad[l][b]+(1-beta)*new_grad\n",
    "                        old_grad[l][b]=new_grad\n",
    "                        self.weights[l]=self.weights[l]-new_grad*rate\n",
    "\n",
    "                        new_grad_bias=sum([errors[x][l] for x in range(batch_size)])/batch_size\n",
    "                        new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                        new_grad_bias=beta*old_grad_bias[l][b]+(1-beta)*new_grad_bias\n",
    "                        old_grad_bias[l][b]=new_grad_bias\n",
    "                        self.biases[l]=self.biases[l]-new_grad_bias*rate\n",
    "                \n",
    "                #calculate gradient for input layer\n",
    "                if(RMSprop):\n",
    "                    new_grad=sum([errors[x][0].dot((self.input_layer.T[:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                    new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                    if L1:\n",
    "                        new_grad+=alpha*np.sign(self.weights[0])#/batch_size\n",
    "                    elif L2:\n",
    "                        new_grad+=alpha*self.weights[0]#/batch_size\n",
    "                    old_grad[0][b]=beta*old_grad[0][b]+(1-beta)*new_grad**2\n",
    "                    self.weights[0]=self.weights[0]-new_grad*(rate/(np.sqrt(old_grad[0][b])+eps))\n",
    "\n",
    "                    new_grad_bias=sum([errors[x][0] for x in range(batch_size)])/batch_size\n",
    "                    new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                    old_grad_bias[0][b]=beta*old_grad_bias[0][b]+(1-beta)*new_grad_bias**2\n",
    "                    self.biases[0]=self.biases[0]-new_grad_bias*(rate/(np.sqrt(old_grad_bias[0][b])+eps))\n",
    "                else:\n",
    "                    new_grad=sum([errors[x][0].dot((self.input_layer.T[:,x:x+1]).T) for x in range(batch_size)])/batch_size\n",
    "                    new_grad=np.clip(new_grad,-10**6,10**6)\n",
    "                    if L1:\n",
    "                        new_grad+=alpha*np.sign(self.weights[0])#/batch_size\n",
    "                    elif L2:\n",
    "                        new_grad+=alpha*self.weights[0]#/batch_size\n",
    "                    new_grad=beta*old_grad[0][b]+(1-beta)*new_grad\n",
    "                    old_grad[0][b]=new_grad\n",
    "                    self.weights[0]=self.weights[0]-new_grad*rate\n",
    "\n",
    "                    new_grad_bias=sum([errors[x][0] for x in range(batch_size)])/batch_size\n",
    "                    new_grad_bias=np.clip(new_grad_bias,-10**6,10**6)\n",
    "                    new_grad_bias=beta*old_grad_bias[0][b]+(1-beta)*new_grad_bias\n",
    "                    old_grad_bias[0][b]=new_grad_bias\n",
    "                    self.biases[0]=self.biases[0]-new_grad_bias*rate\n",
    "\n",
    "\n",
    "            output_layer=self.calculate(train_input_layer)\n",
    "            new_error=self.cost_fun(train_output_layer,output_layer)\n",
    "            \n",
    "            #check stopping conditions if worse early_stop a row then stop\n",
    "            if validate:\n",
    "                output_layer=self.calculate(test_input_layer)\n",
    "                new_error_test=self.cost_fun(test_output_layer,output_layer)\n",
    "                if new_error_test>error_test:\n",
    "                    if warning>=early_stop:\n",
    "                        break\n",
    "                    else:\n",
    "                        error_test=new_error_test\n",
    "                        warning+=1\n",
    "                else:\n",
    "                    error_test=new_error_test\n",
    "                    warning=0\n",
    "                \n",
    "            if return_error_list:\n",
    "                error_list.append(new_error)\n",
    "            \n",
    "            if(epoch % 100 == 0 and Verbose):\n",
    "                print(\"epoch \"+str(epoch))\n",
    "                \n",
    "            if new_error<=0:\n",
    "                break\n",
    "            \n",
    "            if ((abs(new_error-error)/error)<stop).all():\n",
    "                break\n",
    "\n",
    "            error=new_error\n",
    "        \n",
    "        print(\"end in epoch \"+str(epoch)+ \" with error \"+str(new_error))\n",
    "        if return_error_list:\n",
    "            return error_list\n",
    "        elif return_gradients:\n",
    "            return old_grad, old_grad_bias\n",
    "        else:\n",
    "            return new_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(s):\n",
    "    s=softmax(s.T)\n",
    "    return (s*(1-s)).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_grad(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    e = np.exp(2*z)\n",
    "    return 1-2/(e+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_grad(z):\n",
    "    return 1-tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(Y,Y_hat):\n",
    "    return np.sum((Y-Y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad(Y,Y_hat):\n",
    "     return Y_hat-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(Y,Y_hat):\n",
    "    return np.sum(np.abs(Y-Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_grad(Y,Y_hat):\n",
    "    return (Y_hat>=Y)*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return(sigmoid(x)*(1-sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(targets,predictions,  epsilon=1e-12):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    ce = abs(np.sum(targets*np.log(predictions)))\n",
    "    return ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_grad(Y,Y_hat,eps=10**-10):\n",
    "    Y_hat=Y_hat+eps\n",
    "    return (Y_hat-eps-Y)/Y_hat/(1-Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_grad(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/train.csv\", delimiter=',', skiprows=1, usecols=range(0,785))\n",
    "np.random.shuffle(test)\n",
    "X=test[::100,1:785]\n",
    "Y=test[::100,0:1]\n",
    "Y=Y==np.unique(Y)\n",
    "Y=np.array(Y,dtype=int)\n",
    "train_input_layer=X/255\n",
    "train_output_layer=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/test.csv\", delimiter=',', skiprows=1, usecols=range(0,784))\n",
    "np.random.shuffle(test)\n",
    "X1=test[:,0:784]\n",
    "test_input_layer=X1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start error 3215.680417779605\n",
      "end in epoch 4000 with error 0.025038770032371004\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cost_fun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0169db757462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscore_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_fun' is not defined"
     ]
    }
   ],
   "source": [
    "L=[100,100,100,100]\n",
    "mlp_test=MLP_new(784,10,L,[tanh]*len(L),[tanh_grad]*len(L),cross_entropy,cross_entropy_grad,problem=\"C\",seed=123)\n",
    "        \n",
    "start_time = time.time()\n",
    "er=mlp_test.train(train_input_layer,train_output_layer,max_epoch=4000,Verbose=False,beta=0.9,batch=0.1)\n",
    "end_time = time.time()\n",
    "t=end_time - start_time\n",
    "    \n",
    "output_layer=mlp_test.calculate(train_input_layer)\n",
    "score_train=cross_entropy(train_output_layer,output_layer)/output_layer.shape[0]\n",
    "\n",
    "accuracy_train=round(np.mean(np.argmax(train_output_layer,axis=1)==np.argmax(output_layer,axis=1)),3)\n",
    "\n",
    "output_layer=mlp_test.calculate(test_input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/train.csv\", delimiter=',', skiprows=1)\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(test)\n",
    "X=test[:,1:]\n",
    "Y=test[:,0:1]\n",
    "Y=Y==np.unique(Y)\n",
    "Y=np.array(Y,dtype=int)\n",
    "train_input_layer=X/255\n",
    "pca=PCA(n_components=200)\n",
    "pca=pca.fit(train_input_layer)\n",
    "train_input_layer=pca.transform(train_input_layer[::10,:])\n",
    "train_output_layer=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"MNIST/test.csv\", delimiter=',', skiprows=1)\n",
    "X1=test\n",
    "test_input_layer=X1/255\n",
    "test_input_layer=pca.transform(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start error 376475.9817210172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-96ac66fa4997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_output_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVerbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-09a12511d76e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_input_layer, train_output_layer, max_epoch, rate, beta, batch, stop, RMSprop, L1, L2, eps, alpha, validate, early_stop, test_input_layer, test_output_layer, return_error_list, return_gradients, Verbose)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnew_grad_bias\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_grad_bias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mnew_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0mnew_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L=[100,100,100,100]\n",
    "mlp_test=MLP_new(200,10,L,[tanh]*len(L),[tanh_grad]*len(L),cross_entropy,cross_entropy_grad,problem=\"C\",seed=123)\n",
    "        \n",
    "start_time = time.time()\n",
    "er=mlp_test.train(train_input_layer,train_output_layer,max_epoch=3000,Verbose=True,beta=0.9,batch=0.1)\n",
    "end_time = time.time()\n",
    "t=end_time - start_time\n",
    "    \n",
    "output_layer=mlp_test.calculate(train_input_layer)\n",
    "score_train=cross_entropy(train_output_layer,output_layer)/output_layer.shape[0]\n",
    "\n",
    "accuracy_train=round(np.mean(np.argmax(train_output_layer,axis=1)==np.argmax(output_layer,axis=1)),3)\n",
    "\n",
    "output_layer=mlp_test.calculate(test_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
