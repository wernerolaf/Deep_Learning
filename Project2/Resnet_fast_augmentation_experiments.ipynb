{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_fast_augmentation_experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEb09GE9RplR"
      },
      "source": [
        "from inspect import signature\n",
        "import copy\n",
        "from collections import namedtuple, defaultdict\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import singledispatch\n",
        "\n",
        "#####################\n",
        "# utils\n",
        "#####################\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self, synch=None):\n",
        "        self.synch = synch or (lambda: None)\n",
        "        self.synch()\n",
        "        self.times = [time.perf_counter()]\n",
        "        self.total_time = 0.0\n",
        "\n",
        "    def __call__(self, include_in_total=True):\n",
        "        self.synch()\n",
        "        self.times.append(time.perf_counter())\n",
        "        delta_t = self.times[-1] - self.times[-2]\n",
        "        if include_in_total:\n",
        "            self.total_time += delta_t\n",
        "        return delta_t\n",
        "    \n",
        "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
        "\n",
        "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
        "\n",
        "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
        "    formats = formats or default_table_formats\n",
        "    type_ = lambda val: float if isinstance(val, (float, np.float)) else type(val)\n",
        "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
        "\n",
        "def every(n, col): \n",
        "    return lambda data: data[col] % n == 0\n",
        "\n",
        "class Table():\n",
        "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
        "        self.keys, self.report, self.formatter = keys, report, formatter\n",
        "        self.log = []\n",
        "        \n",
        "    def append(self, data):\n",
        "        self.log.append(data)\n",
        "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
        "        self.keys = self.keys or data.keys()\n",
        "        if len(self.log) is 1:\n",
        "            print(*(self.formatter(k, True) for k in self.keys))\n",
        "        if self.report(data):\n",
        "            print(*(self.formatter(data[k]) for k in self.keys))\n",
        "            \n",
        "    def df(self):\n",
        "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])     \n",
        "\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "def preprocess(dataset, transforms):\n",
        "    dataset = copy.copy(dataset) #shallow copy\n",
        "    for transform in transforms:\n",
        "        dataset['data'] = transform(dataset['data'])\n",
        "    return dataset\n",
        "\n",
        "@singledispatch\n",
        "def normalise(x, mean, std):\n",
        "    return (x - mean) / std\n",
        "\n",
        "@normalise.register(np.ndarray) \n",
        "def _(x, mean, std): \n",
        "    #faster inplace for numpy arrays\n",
        "    x = np.array(x, np.float32)\n",
        "    x -= mean\n",
        "    x *= 1.0/std\n",
        "    return x\n",
        "\n",
        "unnormalise = lambda x, mean, std: x*std + mean\n",
        "\n",
        "@singledispatch\n",
        "def pad(x, border):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@pad.register(np.ndarray)\n",
        "def _(x, border): \n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "@singledispatch\n",
        "def transpose(x, source, target):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@transpose.register(np.ndarray)\n",
        "def _(x, source, target):\n",
        "    return x.transpose([source.index(d) for d in target]) \n",
        "\n",
        "#####################\n",
        "## data augmentation\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        return x[..., y0:y0+self.h, x0:x0+self.w]\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
        "    \n",
        "    def output_shape(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return (*_, self.h, self.w)\n",
        "\n",
        "@singledispatch\n",
        "def flip_lr(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@flip_lr.register(np.ndarray)\n",
        "def _(x): \n",
        "    return x[..., ::-1].copy()\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def __call__(self, x, choice):\n",
        "        return flip_lr(x) if choice else x \n",
        "        \n",
        "    def options(self, shape):\n",
        "        return [{'choice': b} for b in [True, False]]\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]    \n",
        "    \n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, dataset, transforms):\n",
        "        self.dataset, self.transforms = dataset, transforms\n",
        "        self.choices = None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "           \n",
        "    def __getitem__(self, index):\n",
        "        data, labels = self.dataset[index]\n",
        "        data = data.copy()\n",
        "        for choices, f in zip(self.choices, self.transforms):\n",
        "            data = f(data, **choices[index])\n",
        "        return data, labels\n",
        "    \n",
        "    def set_random_choices(self):\n",
        "        self.choices = []\n",
        "        x_shape = self.dataset[0][0].shape\n",
        "        N = len(self)\n",
        "        for t in self.transforms:\n",
        "            self.choices.append(np.random.choice(t.options(x_shape), N))\n",
        "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
        "\n",
        "\n",
        "#####################\n",
        "## dict utils\n",
        "#####################\n",
        "\n",
        "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
        "\n",
        "def path_iter(nested_dict, pfx=()):\n",
        "    for name, val in nested_dict.items():\n",
        "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
        "        else: yield ((*pfx, name), val)  \n",
        "\n",
        "def map_nested(func, nested_dict):\n",
        "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
        "\n",
        "def group_by_key(items):\n",
        "    res = defaultdict(list)\n",
        "    for k, v in items: \n",
        "        res[k].append(v) \n",
        "    return res\n",
        "\n",
        "#####################\n",
        "## graph building\n",
        "#####################\n",
        "sep = '/'\n",
        "\n",
        "def split(path):\n",
        "    i = path.rfind(sep) + 1\n",
        "    return path[:i].rstrip(sep), path[i:]\n",
        "\n",
        "def normpath(path):\n",
        "    #simplified os.path.normpath\n",
        "    parts = []\n",
        "    for p in path.split(sep):\n",
        "        if p == '..': parts.pop()\n",
        "        elif p.startswith(sep): parts = [p]\n",
        "        else: parts.append(p)\n",
        "    return sep.join(parts)\n",
        "\n",
        "has_inputs = lambda node: type(node) is tuple\n",
        "\n",
        "def pipeline(net):\n",
        "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
        "\n",
        "def build_graph(net):\n",
        "    flattened = pipeline(net)\n",
        "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
        "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}    \n",
        "\n",
        "#####################\n",
        "## training utils\n",
        "#####################\n",
        "\n",
        "@singledispatch\n",
        "def cat(*xs):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "@singledispatch\n",
        "def to_numpy(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        " \n",
        "class Const(namedtuple('Const', ['val'])):\n",
        "    def __call__(self, x):\n",
        "        return self.val\n",
        "\n",
        "#####################\n",
        "## network visualisation (requires pydot)\n",
        "#####################\n",
        "class ColorMap(dict):\n",
        "    palette = ['#'+x for x in (\n",
        "        'bebada,ffffb3,fb8072,8dd3c7,80b1d3,fdb462,b3de69,fccde5,bc80bd,ccebc5,ffed6f,1f78b4,33a02c,e31a1c,ff7f00,'\n",
        "        '4dddf8,e66493,b07b87,4e90e3,dea05e,d0c281,f0e189,e9e8b1,e0eb71,bbd2a4,6ed641,57eb9c,3ca4d4,92d5e7,b15928'\n",
        "    ).split(',')]\n",
        "\n",
        "    def __missing__(self, key):\n",
        "        self[key] = self.palette[len(self) % len(self.palette)]\n",
        "        return self[key]\n",
        "\n",
        "    def _repr_html_(self):\n",
        "        css = (\n",
        "        '.pill {'\n",
        "            'margin:2px; border-width:1px; border-radius:9px; border-style:solid;'\n",
        "            'display:inline-block; width:100px; height:15px; line-height:15px;'\n",
        "        '}'\n",
        "        '.pill_text {'\n",
        "            'width:90%; margin:auto; font-size:9px; text-align:center; overflow:hidden;'\n",
        "        '}'\n",
        "        )\n",
        "        s = '<div class=pill style=\"background-color:{}\"><div class=pill_text>{}</div></div>'\n",
        "        return '<style>'+css+'</style>'+''.join((s.format(color, text) for text, color in self.items()))\n",
        "\n",
        "def make_dot_graph(nodes, edges, direction='LR', **kwargs):\n",
        "    from pydot import Dot, Cluster, Node, Edge\n",
        "    class Subgraphs(dict):\n",
        "        def __missing__(self, path):\n",
        "            parent, label = split(path)\n",
        "            subgraph = Cluster(path, label=label, style='rounded, filled', fillcolor='#77777744')\n",
        "            self[parent].add_subgraph(subgraph)\n",
        "            return subgraph\n",
        "    g = Dot(rankdir=direction, directed=True, **kwargs)\n",
        "    g.set_node_defaults(\n",
        "        shape='box', style='rounded, filled', fillcolor='#ffffff')\n",
        "    subgraphs = Subgraphs({'': g})\n",
        "    for path, attr in nodes:\n",
        "        parent, label = split(path)\n",
        "        subgraphs[parent].add_node(\n",
        "            Node(name=path, label=label, **attr))\n",
        "    for src, dst, attr in edges:\n",
        "        g.add_edge(Edge(src, dst, **attr))\n",
        "    return g\n",
        "\n",
        "class DotGraph():\n",
        "    def __init__(self, graph, size=15, direction='LR'):\n",
        "        self.nodes = [(k, v) for k, (v,_) in graph.items()]\n",
        "        self.edges = [(src, dst, {}) for dst, (_, inputs) in graph.items() for src in inputs]\n",
        "        self.size, self.direction = size, direction\n",
        "\n",
        "    def dot_graph(self, **kwargs):\n",
        "        return make_dot_graph(self.nodes, self.edges, size=self.size, direction=self.direction,  **kwargs)\n",
        "\n",
        "    def svg(self, **kwargs):\n",
        "        return self.dot_graph(**kwargs).create(format='svg').decode('utf-8')\n",
        "    try:\n",
        "        import pydot\n",
        "        _repr_svg_ = svg\n",
        "    except ImportError:\n",
        "        def __repr__(self): return 'pydot is needed for network visualisation'\n",
        "\n",
        "walk = lambda dct, key: walk(dct, dct[key]) if key in dct else key\n",
        "   \n",
        "def remove_by_type(net, node_type):  \n",
        "    #remove identity nodes for more compact visualisations\n",
        "    graph = build_graph(net)\n",
        "    remap = {k: i[0] for k,(v,i) in graph.items() if isinstance(v, node_type)}\n",
        "    return {k: (v, [walk(remap, x) for x in i]) for k, (v,i) in graph.items() if not isinstance(v, node_type)}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX5m4uZvs-1H"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import namedtuple \n",
        "from itertools import count\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device(\"cpu\")\n",
        "\n",
        "@cat.register(torch.Tensor)\n",
        "def _(*xs):\n",
        "    return torch.cat(xs)\n",
        "\n",
        "@to_numpy.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return x.detach().cpu().numpy()  \n",
        "\n",
        "@pad.register(torch.Tensor)\n",
        "def _(x, border):\n",
        "    return nn.ReflectionPad2d(border)(x)\n",
        "\n",
        "@transpose.register(torch.Tensor)\n",
        "def _(x, source, target):\n",
        "    return x.permute([source.index(d) for d in target]) \n",
        "\n",
        "def to(*args, **kwargs): \n",
        "    return lambda x: x.to(*args, **kwargs)\n",
        "\n",
        "@flip_lr.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return torch.flip(x, [-1])\n",
        "\n",
        "\n",
        "#####################\n",
        "## dataset\n",
        "#####################\n",
        "from functools import lru_cache as cache\n",
        "\n",
        "@cache(None)\n",
        "def cifar10(root='./data'):\n",
        "    try: \n",
        "        import torchvision\n",
        "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
        "    except ImportError:\n",
        "        from tensorflow.keras import datasets\n",
        "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
        "        return {\n",
        "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
        "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
        "        }\n",
        "             \n",
        "cifar10_mean, cifar10_std = [\n",
        "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
        "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
        "]\n",
        "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')\n",
        "\n",
        "\n",
        "#####################\n",
        "## data loading\n",
        "#####################\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "    \n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices() \n",
        "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.dataloader)\n",
        "\n",
        "#GPU dataloading\n",
        "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
        "\n",
        "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
        "\n",
        "def shuffled(xs, inplace=False):\n",
        "    xs = xs if inplace else copy.copy(xs) \n",
        "    np.random.shuffle(xs)\n",
        "    return xs\n",
        "\n",
        "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
        "    i = torch.randperm(len(data), device=device)\n",
        "    data = data[i]\n",
        "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
        "    data = torch.cat([transform(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
        "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
        "\n",
        "class GPUBatches():\n",
        "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
        "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
        "        N = len(dataset['data'])\n",
        "        self.splits = list(range(0, N+1, batch_size))\n",
        "        if not drop_last and self.splits[-1] != N:\n",
        "            self.splits.append(N)\n",
        "     \n",
        "    def __iter__(self):\n",
        "        data, targets = self.dataset['data'], self.dataset['targets']\n",
        "        for transform in self.transforms:\n",
        "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
        "        if self.shuffle:\n",
        "            i = torch.randperm(len(data), device=device)\n",
        "            data, targets = data[i], targets[i]\n",
        "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.splits) - 1\n",
        "\n",
        "#####################\n",
        "## Layers\n",
        "#####################\n",
        "\n",
        "#Network\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, net):\n",
        "        super().__init__()\n",
        "        self.graph = build_graph(net)\n",
        "        for path, (val, _) in self.graph.items(): \n",
        "            setattr(self, path.replace('/', '_'), val)\n",
        "    \n",
        "    def nodes(self):\n",
        "        return (node for node, _ in self.graph.values())\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        outputs = dict(inputs)\n",
        "        for k, (node, ins) in self.graph.items():\n",
        "            #only compute nodes that are not supplied as inputs.\n",
        "            if k not in outputs: \n",
        "                outputs[k] = node(*[outputs[x] for x in ins])\n",
        "        return outputs\n",
        "    \n",
        "    def half(self):\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
        "                node.half()\n",
        "        return self\n",
        "\n",
        "class Identity(namedtuple('Identity', [])):\n",
        "    def __call__(self, x): return x\n",
        "\n",
        "class Add(namedtuple('Add', [])):\n",
        "    def __call__(self, x, y): return x + y \n",
        "    \n",
        "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
        "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
        "\n",
        "class Mul(nn.Module):\n",
        "    def __init__(self, weight):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "    def __call__(self, x): \n",
        "        return x*self.weight\n",
        "    \n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def forward(self, *xs): return torch.cat(xs, 1)\n",
        "\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
        "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
        "        self.weight.requires_grad = not weight_freeze\n",
        "        self.bias.requires_grad = not bias_freeze\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False): #lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "        return super().train(mode)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return nn.functional.batch_norm(\n",
        "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W) \n",
        "        else:\n",
        "            return nn.functional.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "\n",
        "# Losses\n",
        "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
        "    def __call__(self, log_probs, target):\n",
        "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
        "    \n",
        "class KLLoss(namedtuple('KLLoss', [])):        \n",
        "    def __call__(self, log_probs):\n",
        "        return -log_probs.mean(dim=1)\n",
        "\n",
        "class Correct(namedtuple('Correct', [])):\n",
        "    def __call__(self, classifier, target):\n",
        "        return classifier.max(dim = 1)[1] == target\n",
        "\n",
        "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
        "    def __call__(self, x):\n",
        "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
        "\n",
        "x_ent_loss = Network({\n",
        "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
        "  'acc': (Correct(), ['logits', 'target'])\n",
        "})\n",
        "\n",
        "label_smoothing_loss = lambda alpha: Network({\n",
        "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
        "        'KL':  (KLLoss(), ['logprobs']),\n",
        "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
        "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
        "        'acc': (Correct(), ['logits', 'target']),\n",
        "    })\n",
        "\n",
        "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "#####################\n",
        "## Optimisers\n",
        "##################### \n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    dw.add_(weight_decay, w).mul_(-lr)\n",
        "    v.mul_(momentum).add_(dw)\n",
        "    w.add_(dw.add_(momentum, v))\n",
        "\n",
        "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
        "\n",
        "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
        "\n",
        "def zeros_like(weights):\n",
        "    return [torch.zeros_like(w) for w in weights]\n",
        "\n",
        "def optimiser(weights, param_schedule, update, state_init):\n",
        "    weights = list(weights)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
        "\n",
        "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
        "    step_number += 1\n",
        "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
        "    for w, v in zip(weights, opt_state):\n",
        "        if w.requires_grad:\n",
        "            update(w.data, w.grad.data, v, **param_values)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
        "\n",
        "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
        "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)\n",
        "  \n",
        "#####################\n",
        "## training\n",
        "#####################\n",
        "from itertools import chain\n",
        "\n",
        "def reduce(batches, state, steps):\n",
        "    #state: is a dictionary\n",
        "    #steps: are functions that take (batch, state)\n",
        "    #and return a dictionary of updates to the state (or None)\n",
        "    \n",
        "    for batch in chain(batches, [None]): \n",
        "    #we send an extra batch=None at the end for steps that \n",
        "    #need to do some tidying-up (e.g. log_activations)\n",
        "        for step in steps:\n",
        "            updates = step(batch, state)\n",
        "            if updates:\n",
        "                for k,v in updates.items():\n",
        "                    state[k] = v                  \n",
        "    return state\n",
        "  \n",
        "#define keys in the state dict as constants\n",
        "MODEL = 'model'\n",
        "LOSS = 'loss'\n",
        "VALID_MODEL = 'valid_model'\n",
        "OUTPUT = 'output'\n",
        "OPTS = 'optimisers'\n",
        "ACT_LOG = 'activation_log'\n",
        "WEIGHT_LOG = 'weight_log'\n",
        "\n",
        "#step definitions\n",
        "def forward(training_mode):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training != training_mode: #without the guard it's slow!\n",
        "            model.train(training_mode)\n",
        "        return {OUTPUT: state[LOSS](model(batch))}\n",
        "    return step\n",
        "\n",
        "def forward_tta(tta_transforms):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training:\n",
        "            model.train(False)\n",
        "        logits = torch.mean(torch.stack([model({'input': transform(batch['input'].clone())})['logits'].detach() for transform in tta_transforms], dim=0), dim=0)\n",
        "        return {OUTPUT: state[LOSS](dict(batch, logits=logits))}\n",
        "    return step\n",
        "\n",
        "def backward(dtype=None):\n",
        "    def step(batch, state):\n",
        "        state[MODEL].zero_grad()\n",
        "        if not batch: return\n",
        "        loss = state[OUTPUT][LOSS]\n",
        "        if dtype is not None:\n",
        "            loss = loss.to(dtype)\n",
        "        loss.sum().backward()\n",
        "    return step\n",
        "\n",
        "def opt_steps(batch, state):\n",
        "    if not batch: return\n",
        "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
        "\n",
        "def log_activations(node_names=('loss', 'acc')):\n",
        "    def step(batch, state):\n",
        "        if '_tmp_logs_' not in state: \n",
        "            state['_tmp_logs_'] = []\n",
        "        if batch:\n",
        "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
        "        else:\n",
        "            res = {k: to_numpy(torch.cat(xs)).astype(np.float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
        "            del state['_tmp_logs_']\n",
        "            return {ACT_LOG: res}\n",
        "    return step\n",
        "\n",
        "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
        "\n",
        "def update_ema(momentum, update_freq=1):\n",
        "    n = iter(count())\n",
        "    rho = momentum**update_freq\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        if (next(n) % update_freq) != 0: return\n",
        "        for v, ema_v in zip(state[MODEL].state_dict().values(), state[VALID_MODEL].state_dict().values()):\n",
        "            if not v.dtype.is_floating_point: continue #skip things like num_batches_tracked.\n",
        "            ema_v *= rho\n",
        "            ema_v += (1-rho)*v\n",
        "    return step\n",
        "\n",
        "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
        "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
        "\n",
        "\n",
        "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
        "                on_epoch_end=(lambda state: state)):\n",
        "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
        "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
        "    return {\n",
        "        'train': union({'time': train_time}, train_summary), \n",
        "        'valid': union({'time': valid_time}, valid_summary), \n",
        "        'total time': timer.total_time\n",
        "    }\n",
        "\n",
        "#on_epoch_end\n",
        "def log_weights(state, weights):\n",
        "    state[WEIGHT_LOG] = state.get(WEIGHT_LOG, [])\n",
        "    state[WEIGHT_LOG].append({k: to_numpy(v.data) for k,v in weights.items()})\n",
        "    return state\n",
        "\n",
        "def fine_tune_bn_stats(state, batches, model_key=VALID_MODEL):\n",
        "    reduce(batches, {MODEL: state[model_key]}, [forward(True)])\n",
        "    return state\n",
        "\n",
        "#misc\n",
        "def warmup_cudnn(model, loss, batch):\n",
        "    #run forward and backward pass of the model\n",
        "    #to allow benchmarking of cudnn kernels \n",
        "    reduce([batch], {MODEL: model, LOSS: loss}, [forward(True), backward()])\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "#####################\n",
        "## input whitening\n",
        "#####################\n",
        "\n",
        "def cov(X):\n",
        "    X = X/np.sqrt(X.size(0) - 1)\n",
        "    return X.t() @ X\n",
        "\n",
        "def patches(data, patch_size=(3, 3), dtype=torch.float32):\n",
        "    h, w = patch_size\n",
        "    c = data.size(1)\n",
        "    return data.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1, c, h, w).to(dtype)\n",
        "\n",
        "def eigens(patches):\n",
        "    n,c,h,w = patches.shape\n",
        "    Σ = cov(patches.reshape(n, c*h*w))\n",
        "    Λ, V = torch.symeig(Σ, eigenvectors=True)\n",
        "    return Λ.flip(0), V.t().reshape(c*h*w, c, h, w).flip(0)\n",
        "\n",
        "def whitening_filter(Λ, V, eps=1e-2):\n",
        "    filt = nn.Conv2d(3, 27, kernel_size=(3,3), padding=(1,1), bias=False)\n",
        "    filt.weight.data = (V/torch.sqrt(Λ+eps)[:,None,None,None])\n",
        "    filt.weight.requires_grad = False \n",
        "    return filt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQpXMeu5to2A"
      },
      "source": [
        "#Network definition\n",
        "def conv_bn_default(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': BatchNorm(c_out), \n",
        "        'relu': nn.ReLU(True)\n",
        "    }\n",
        "    if pool: block['pool'] = pool\n",
        "    return block\n",
        "\n",
        "def residual(c, conv_bn, **kw):\n",
        "    return {\n",
        "        'in': Identity(),\n",
        "        'res1': conv_bn(c, c, **kw),\n",
        "        'res2': conv_bn(c, c, **kw),\n",
        "        'add': (Add(), ['in', 'res2/relu']),\n",
        "    }\n",
        "\n",
        "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), conv_bn=conv_bn_default, prep=conv_bn_default):\n",
        "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
        "    n = {\n",
        "        'input': (None, []),\n",
        "        'prep': prep(3, channels['prep']),\n",
        "        'layer1': conv_bn(channels['prep'], channels['layer1'], pool=pool),\n",
        "        'layer2': conv_bn(channels['layer1'], channels['layer2'], pool=pool),\n",
        "        'layer3': conv_bn(channels['layer2'], channels['layer3'], pool=pool),\n",
        "        'pool': nn.MaxPool2d(4),\n",
        "        'flatten': Flatten(),\n",
        "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
        "        'logits': Mul(weight),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        n[layer]['residual'] = residual(channels[layer], conv_bn)\n",
        "    for layer in extra_layers:\n",
        "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
        "    return n\n",
        "\n",
        "def tsv(logs):\n",
        "    data = [(output['epoch'], output['total time']/3600, output['valid']['acc']*100) for output in logs]\n",
        "    return '\\n'.join(['epoch\\thours\\ttop1Accuracy']+[f'{epoch}\\t{hours:.8f}\\t{acc:.2f}' for (epoch, hours, acc) in data])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iouYy2hotsK3"
      },
      "source": [
        "import argparse\n",
        "import os.path\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--data_dir', type=str, default='./data')\n",
        "parser.add_argument('--log_dir', type=str, default='.')\n",
        "parser.add_argument('-f')\n",
        "batch_norm = partial(GhostBatchNorm, num_splits=16, weight_freeze=True)\n",
        "relu = partial(nn.CELU, alpha=0.3)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCRPSJ9vtx86"
      },
      "source": [
        "def conv_bn(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': batch_norm(c_out), \n",
        "        'relu': relu(),\n",
        "    }\n",
        "    if pool: block = {'conv': block['conv'], 'pool': pool, 'bn': block['bn'], 'relu': block['relu']}\n",
        "    return block\n",
        "\n",
        "def whitening_block(c_in, c_out, Λ=None, V=None, eps=1e-2):\n",
        "    return {\n",
        "        'whiten': whitening_filter(Λ, V, eps),\n",
        "        'conv': nn.Conv2d(27, c_out, kernel_size=(1, 1), bias=False),\n",
        "        'norm': batch_norm(c_out), \n",
        "        'act':  relu(),\n",
        "    }\n",
        "\n",
        "def main(epochs, ema_epochs, train_transforms):  \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    print('Downloading datasets')\n",
        "    dataset = map_nested(torch.tensor, cifar10(args.data_dir))\n",
        "\n",
        "    lr_schedule = PiecewiseLinear([0, epochs/5, epochs-ema_epochs], [0, 1.0, 0.1])\n",
        "    batch_size = 512\n",
        "    \n",
        "\n",
        "    print('Warming up torch')\n",
        "    random_data = torch.tensor(np.random.randn(1000,3,32,32).astype(np.float16), device=device)\n",
        "    Λ, V = eigens(patches(random_data))\n",
        "\n",
        "    loss = label_smoothing_loss(0.2)\n",
        "    random_batch = lambda batch_size:  {\n",
        "        'input': torch.Tensor(np.random.rand(batch_size,3,32,32)).cuda().half(), \n",
        "        'target': torch.LongTensor(np.random.randint(0,10,batch_size)).cuda()\n",
        "    }\n",
        "    print('Warming up cudnn on random inputs')\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Λ=Λ, V=V))).to(device).half()\n",
        "    for size in [batch_size, len(dataset['valid']['targets']) % batch_size]:\n",
        "        warmup_cudnn(model, loss, random_batch(size))\n",
        "    \n",
        "    print('Starting timer')\n",
        "    timer = Timer(synch=torch.cuda.synchronize)\n",
        "    \n",
        "    print('Preprocessing training data')\n",
        "    dataset = map_nested(to(device), dataset)\n",
        "    T = lambda x: torch.tensor(x, dtype=torch.float16, device=device)\n",
        "    transforms = [\n",
        "        to(dtype=torch.float16),\n",
        "        partial(normalise, mean=T(cifar10_mean), std=T(cifar10_std)),\n",
        "        partial(transpose, source='NHWC', target='NCHW'), \n",
        "    ]\n",
        "    train_set = preprocess(dataset['train'], transforms + [partial(pad, border=4)])\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "    print('Preprocessing test data')\n",
        "    valid_set = preprocess(dataset['valid'], transforms)\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "\n",
        "    Λ, V = eigens(patches(train_set['data'][:10000,:,4:-4,4:-4])) #center crop to remove padding\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Λ=Λ, V=V))).to(device).half()    \n",
        "   \n",
        "    train_batches = GPUBatches(batch_size=batch_size, transforms=train_transforms, dataset=train_set, shuffle=True,  drop_last=True, max_options=200)\n",
        "    valid_batches = GPUBatches(batch_size=batch_size, dataset=valid_set, shuffle=False, drop_last=False)\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    opts = [\n",
        "        SGD(is_bias[False], {'lr': (lambda step: lr_schedule(step/len(train_batches))/batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}),\n",
        "        SGD(is_bias[True], {'lr': (lambda step: lr_schedule(step/len(train_batches))*(64/batch_size)), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)})\n",
        "    ]\n",
        "    logs, state = Table(), {MODEL: model, VALID_MODEL: copy.deepcopy(model), LOSS: loss, OPTS: opts}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches, valid_batches,\n",
        "                                                          train_steps=(*default_train_steps, update_ema(momentum=0.99, update_freq=5)),\n",
        "                                                          valid_steps=(forward_tta([(lambda x: x), flip_lr]), log_activations(('loss', 'acc'))))))\n",
        "\n",
        "    with open(os.path.join(os.path.expanduser(args.log_dir), 'logs.tsv'), 'w') as f:\n",
        "        f.write(tsv(logs.log))\n",
        "\n",
        "    return model        \n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWz_wzjRvWQo",
        "outputId": "a6df7730-313c-4cdd-e438-08e800cf5dee"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(123)\n",
        "\n",
        "model_= main(epochs=96, ema_epochs = 8, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.059 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0024 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       9.5887       2.0873       0.2980       1.0978       2.2923       0.1439       9.6502\n",
            "           2       9.6354       1.5636       0.6214       1.1109       2.0784       0.2919      19.2856\n",
            "           3       9.7952       1.3508       0.7524       1.1296       1.4735       0.7149      29.0808\n",
            "           4       9.8746       1.2653       0.8026       1.1445       1.2014       0.8446      38.9554\n",
            "           5       9.9606       1.2142       0.8303       1.1645       1.1278       0.8813      48.9160\n",
            "           6      10.1171       1.1844       0.8448       1.1629       1.0929       0.8981      59.0331\n",
            "           7       9.9907       1.1645       0.8532       1.1494       1.0675       0.9082      69.0238\n",
            "           8       9.8721       1.1516       0.8593       1.1407       1.0529       0.9146      78.8959\n",
            "           9       9.8692       1.1433       0.8629       1.1319       1.0446       0.9196      88.7652\n",
            "          10       9.8609       1.1379       0.8673       1.1348       1.0392       0.9190      98.6261\n",
            "          11       9.8774       1.1373       0.8673       1.1367       1.0308       0.9219     108.5035\n",
            "          12       9.8534       1.1353       0.8682       1.1416       1.0298       0.9233     118.3569\n",
            "          13       9.8684       1.1399       0.8665       1.1468       1.0282       0.9246     128.2253\n",
            "          14       9.8636       1.1419       0.8633       1.1485       1.0295       0.9231     138.0889\n",
            "          15       9.8611       1.1422       0.8647       1.1454       1.0271       0.9254     147.9500\n",
            "          16       9.8706       1.1471       0.8621       1.1423       1.0298       0.9218     157.8206\n",
            "          17       9.8709       1.1542       0.8565       1.1398       1.0290       0.9251     167.6916\n",
            "          18       9.8681       1.1571       0.8563       1.1382       1.0330       0.9210     177.5596\n",
            "          19       9.8728       1.1591       0.8551       1.1417       1.0297       0.9248     187.4325\n",
            "          20       9.8690       1.1616       0.8528       1.1413       1.0361       0.9213     197.3015\n",
            "          21       9.8639       1.1610       0.8537       1.1433       1.0336       0.9236     207.1653\n",
            "          22       9.8651       1.1592       0.8549       1.1428       1.0308       0.9252     217.0304\n",
            "          23       9.8718       1.1549       0.8579       1.1440       1.0298       0.9245     226.9022\n",
            "          24       9.8644       1.1526       0.8602       1.1423       1.0362       0.9211     236.7667\n",
            "          25       9.8618       1.1511       0.8602       1.1436       1.0284       0.9244     246.6285\n",
            "          26       9.8572       1.1435       0.8644       1.1441       1.0248       0.9272     256.4857\n",
            "          27       9.8672       1.1431       0.8654       1.1416       1.0252       0.9268     266.3529\n",
            "          28       9.8701       1.1422       0.8648       1.1433       1.0208       0.9303     276.2230\n",
            "          29       9.8648       1.1405       0.8667       1.1435       1.0213       0.9285     286.0877\n",
            "          30       9.8635       1.1364       0.8678       1.1437       1.0183       0.9322     295.9512\n",
            "          31       9.8560       1.1317       0.8709       1.1449       1.0176       0.9312     305.8071\n",
            "          32       9.8600       1.1263       0.8740       1.1422       1.0165       0.9300     315.6671\n",
            "          33       9.8627       1.1274       0.8725       1.1428       1.0182       0.9290     325.5298\n",
            "          34       9.8620       1.1245       0.8748       1.1419       1.0146       0.9319     335.3918\n",
            "          35       9.8685       1.1218       0.8754       1.1422       1.0104       0.9343     345.2604\n",
            "          36       9.8730       1.1199       0.8763       1.1406       1.0118       0.9327     355.1334\n",
            "          37       9.8623       1.1168       0.8777       1.1414       1.0084       0.9359     364.9957\n",
            "          38       9.8633       1.1133       0.8803       1.1429       1.0069       0.9368     374.8590\n",
            "          39       9.8620       1.1119       0.8818       1.1435       1.0066       0.9366     384.7210\n",
            "          40       9.8706       1.1066       0.8832       1.1418       1.0069       0.9348     394.5916\n",
            "          41       9.8642       1.1078       0.8833       1.1435       1.0039       0.9358     404.4558\n",
            "          42       9.8654       1.1060       0.8826       1.1436       1.0038       0.9359     414.3212\n",
            "          43       9.8615       1.1000       0.8867       1.1402       1.0027       0.9352     424.1827\n",
            "          44       9.8696       1.0969       0.8898       1.1400       1.0020       0.9370     434.0524\n",
            "          45       9.8624       1.0961       0.8883       1.1421       0.9994       0.9385     443.9147\n",
            "          46       9.8635       1.0942       0.8905       1.1427       0.9984       0.9390     453.7782\n",
            "          47       9.8694       1.0936       0.8914       1.1396       0.9978       0.9406     463.6476\n",
            "          48       9.8772       1.0876       0.8939       1.1408       0.9953       0.9418     473.5248\n",
            "          49       9.8649       1.0885       0.8936       1.1435       0.9952       0.9401     483.3897\n",
            "          50       9.8614       1.0825       0.8965       1.1432       0.9949       0.9410     493.2511\n",
            "          51       9.8621       1.0799       0.8986       1.1420       0.9926       0.9432     503.1132\n",
            "          52       9.8653       1.0774       0.8994       1.1419       0.9917       0.9442     512.9785\n",
            "          53       9.8709       1.0767       0.9009       1.1428       0.9893       0.9438     522.8494\n",
            "          54       9.8683       1.0721       0.9032       1.1417       0.9893       0.9440     532.7177\n",
            "          55       9.8629       1.0690       0.9047       1.1446       0.9898       0.9437     542.5806\n",
            "          56       9.8678       1.0708       0.9032       1.1442       0.9876       0.9436     552.4484\n",
            "          57       9.8636       1.0641       0.9063       1.1432       0.9861       0.9457     562.3120\n",
            "          58       9.8646       1.0606       0.9089       1.1415       0.9864       0.9460     572.1767\n",
            "          59       9.8625       1.0600       0.9085       1.1416       0.9837       0.9455     582.0392\n",
            "          60       9.8785       1.0548       0.9105       1.1414       0.9823       0.9477     591.9177\n",
            "          61       9.8728       1.0524       0.9125       1.1420       0.9811       0.9476     601.7905\n",
            "          62       9.8610       1.0504       0.9140       1.1426       0.9797       0.9483     611.6516\n",
            "          63       9.8637       1.0462       0.9164       1.1420       0.9796       0.9472     621.5153\n",
            "          64       9.8660       1.0418       0.9192       1.1426       0.9786       0.9489     631.3812\n",
            "          65       9.8637       1.0393       0.9213       1.1411       0.9761       0.9501     641.2449\n",
            "          66       9.8733       1.0369       0.9214       1.1420       0.9749       0.9496     651.1182\n",
            "          67       9.8712       1.0334       0.9226       1.1413       0.9743       0.9499     660.9894\n",
            "          68       9.8677       1.0293       0.9251       1.1415       0.9727       0.9506     670.8570\n",
            "          69       9.8668       1.0243       0.9285       1.1407       0.9714       0.9511     680.7238\n",
            "          70       9.8655       1.0209       0.9295       1.1411       0.9705       0.9514     690.5893\n",
            "          71       9.8635       1.0208       0.9288       1.1407       0.9712       0.9509     700.4528\n",
            "          72       9.8686       1.0155       0.9332       1.1425       0.9705       0.9524     710.3214\n",
            "          73       9.8746       1.0082       0.9373       1.1414       0.9706       0.9517     720.1960\n",
            "          74       9.8625       1.0093       0.9363       1.1417       0.9706       0.9498     730.0585\n",
            "          75       9.8686       0.9983       0.9435       1.1409       0.9678       0.9522     739.9271\n",
            "          76       9.8707       0.9981       0.9416       1.1419       0.9662       0.9527     749.7977\n",
            "          77       9.8665       0.9939       0.9451       1.1436       0.9635       0.9537     759.6643\n",
            "          78       9.8636       0.9878       0.9472       1.1429       0.9617       0.9550     769.5279\n",
            "          79       9.8755       0.9788       0.9533       1.1422       0.9609       0.9548     779.4033\n",
            "          80       9.8663       0.9759       0.9550       1.1422       0.9598       0.9562     789.2697\n",
            "          81       9.8656       0.9718       0.9568       1.1425       0.9596       0.9548     799.1353\n",
            "          82       9.8645       0.9662       0.9596       1.1413       0.9579       0.9555     808.9998\n",
            "          83       9.8645       0.9623       0.9620       1.1414       0.9569       0.9560     818.8643\n",
            "          84       9.8709       0.9576       0.9644       1.1419       0.9552       0.9566     828.7351\n",
            "          85       9.8849       0.9525       0.9667       1.1415       0.9539       0.9587     838.6200\n",
            "          86       9.8645       0.9463       0.9698       1.1436       0.9530       0.9590     848.4845\n",
            "          87       9.8599       0.9391       0.9734       1.1404       0.9528       0.9585     858.3443\n",
            "          88       9.8666       0.9328       0.9772       1.1428       0.9514       0.9602     868.2109\n",
            "          89       9.8643       0.9258       0.9803       1.1421       0.9503       0.9598     878.0753\n",
            "          90       9.8657       0.9255       0.9804       1.1424       0.9503       0.9606     887.9409\n",
            "          91       9.8882       0.9265       0.9801       1.1410       0.9491       0.9622     897.8292\n",
            "          92       9.8660       0.9245       0.9812       1.1400       0.9483       0.9603     907.6952\n",
            "          93       9.8709       0.9263       0.9803       1.1424       0.9498       0.9587     917.5661\n",
            "          94       9.8657       0.9267       0.9797       1.1422       0.9497       0.9592     927.4318\n",
            "          95       9.8632       0.9279       0.9804       1.1400       0.9505       0.9590     937.2950\n",
            "          96       9.8744       0.9293       0.9781       1.1434       0.9484       0.9593     947.1694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UOhtkuMwYGR"
      },
      "source": [
        "PATH = \"entire_model_3layers_96epochs_ema8_cutout88.pt\"\n",
        "torch.save(model_, PATH)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSVMFfXJ4fb8",
        "outputId": "21839e93-11bf-4e30-fd41-4b53f506e2bb"
      },
      "source": [
        "seed(123)\n",
        "\n",
        "model_2 = main(96,16,train_transforms=[Crop(32, 32), FlipLR(), Cutout(16,16)])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.061 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0026 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       9.5081       2.1285       0.2642       1.0923       2.2951       0.1440       9.5714\n",
            "           2       9.5320       1.6902       0.5354       1.1007       2.1059       0.3213      19.1034\n",
            "           3       9.6074       1.5075       0.6523       1.1076       1.5534       0.6522      28.7108\n",
            "           4       9.6989       1.4258       0.7026       1.1158       1.2949       0.7839      38.4097\n",
            "           5       9.7734       1.3716       0.7331       1.1233       1.2307       0.8268      48.1831\n",
            "           6       9.8555       1.3376       0.7539       1.1286       1.1913       0.8526      58.0386\n",
            "           7       9.8789       1.3165       0.7659       1.1356       1.1520       0.8738      67.9176\n",
            "           8       9.8566       1.2988       0.7755       1.1449       1.1347       0.8832      77.7741\n",
            "           9       9.8618       1.2866       0.7810       1.1502       1.1173       0.8890      87.6360\n",
            "          10       9.8662       1.2799       0.7858       1.1505       1.1089       0.8938      97.5022\n",
            "          11       9.8593       1.2813       0.7843       1.1479       1.0984       0.8996     107.3615\n",
            "          12       9.8529       1.2750       0.7877       1.1416       1.0924       0.9021     117.2144\n",
            "          13       9.8737       1.2789       0.7848       1.1362       1.0881       0.9031     127.0881\n",
            "          14       9.8714       1.2777       0.7862       1.1387       1.0877       0.9063     136.9595\n",
            "          15       9.8483       1.2837       0.7827       1.1396       1.0897       0.9033     146.8078\n",
            "          16       9.8639       1.2818       0.7835       1.1420       1.0870       0.9055     156.6717\n",
            "          17       9.8613       1.2919       0.7785       1.1440       1.0876       0.9068     166.5330\n",
            "          18       9.8618       1.2919       0.7783       1.1441       1.0934       0.9049     176.3948\n",
            "          19       9.8681       1.3004       0.7719       1.1422       1.1163       0.8898     186.2628\n",
            "          20       9.8664       1.3019       0.7729       1.1420       1.1002       0.9010     196.1292\n",
            "          21       9.8625       1.2967       0.7744       1.1421       1.1007       0.9004     205.9917\n",
            "          22       9.8619       1.2944       0.7774       1.1412       1.0982       0.9017     215.8537\n",
            "          23       9.8665       1.2937       0.7772       1.1423       1.0937       0.9027     225.7201\n",
            "          24       9.8614       1.2847       0.7838       1.1397       1.0905       0.9075     235.5815\n",
            "          25       9.8688       1.2857       0.7815       1.1401       1.0944       0.9042     245.4503\n",
            "          26       9.8735       1.2797       0.7844       1.1417       1.0927       0.9042     255.3238\n",
            "          27       9.8674       1.2763       0.7869       1.1390       1.0849       0.9110     265.1912\n",
            "          28       9.8649       1.2713       0.7901       1.1394       1.0879       0.9069     275.0562\n",
            "          29       9.8672       1.2727       0.7892       1.1410       1.0824       0.9107     284.9233\n",
            "          30       9.8682       1.2693       0.7909       1.1412       1.0782       0.9108     294.7916\n",
            "          31       9.8719       1.2635       0.7952       1.1406       1.0762       0.9121     304.6635\n",
            "          32       9.8700       1.2637       0.7926       1.1405       1.0723       0.9147     314.5335\n",
            "          33       9.8625       1.2595       0.7954       1.1410       1.0689       0.9147     324.3959\n",
            "          34       9.8691       1.2578       0.7959       1.1397       1.0654       0.9161     334.2650\n",
            "          35       9.8667       1.2529       0.8005       1.1409       1.0639       0.9170     344.1317\n",
            "          36       9.8638       1.2514       0.8011       1.1410       1.0622       0.9175     353.9955\n",
            "          37       9.8649       1.2498       0.8008       1.1411       1.0627       0.9196     363.8604\n",
            "          38       9.8727       1.2473       0.8037       1.1401       1.0585       0.9200     373.7331\n",
            "          39       9.8713       1.2379       0.8106       1.1415       1.0583       0.9211     383.6044\n",
            "          40       9.8655       1.2389       0.8088       1.1418       1.0562       0.9201     393.4699\n",
            "          41       9.8664       1.2322       0.8107       1.1411       1.0550       0.9215     403.3363\n",
            "          42       9.8648       1.2340       0.8087       1.1417       1.0557       0.9209     413.2011\n",
            "          43       9.8655       1.2334       0.8098       1.1408       1.0534       0.9207     423.0665\n",
            "          44       9.8688       1.2260       0.8159       1.1403       1.0483       0.9225     432.9353\n",
            "          45       9.8663       1.2243       0.8167       1.1397       1.0455       0.9246     442.8017\n",
            "          46       9.8668       1.2172       0.8196       1.1408       1.0421       0.9259     452.6685\n",
            "          47       9.8705       1.2117       0.8216       1.1398       1.0446       0.9244     462.5390\n",
            "          48       9.8675       1.2137       0.8219       1.1409       1.0428       0.9238     472.4064\n",
            "          49       9.8653       1.2083       0.8255       1.1412       1.0421       0.9279     482.2717\n",
            "          50       9.8678       1.2098       0.8237       1.1422       1.0390       0.9252     492.1396\n",
            "          51       9.8747       1.2018       0.8286       1.1403       1.0367       0.9283     502.0143\n",
            "          52       9.8672       1.1969       0.8329       1.1384       1.0309       0.9281     511.8815\n",
            "          53       9.8660       1.1941       0.8318       1.1393       1.0308       0.9306     521.7474\n",
            "          54       9.8702       1.1893       0.8351       1.1409       1.0283       0.9322     531.6176\n",
            "          55       9.8656       1.1879       0.8359       1.1413       1.0268       0.9322     541.4832\n",
            "          56       9.8673       1.1814       0.8397       1.1394       1.0266       0.9323     551.3505\n",
            "          57       9.8744       1.1803       0.8412       1.1418       1.0244       0.9353     561.2249\n",
            "          58       9.8629       1.1741       0.8437       1.1401       1.0186       0.9374     571.0878\n",
            "          59       9.8667       1.1728       0.8427       1.1395       1.0158       0.9372     580.9544\n",
            "          60       9.8675       1.1664       0.8478       1.1400       1.0151       0.9382     590.8220\n",
            "          61       9.8684       1.1594       0.8521       1.1403       1.0137       0.9363     600.6904\n",
            "          62       9.8707       1.1561       0.8528       1.1396       1.0119       0.9374     610.5610\n",
            "          63       9.8751       1.1527       0.8550       1.1407       1.0121       0.9372     620.4361\n",
            "          64       9.8678       1.1501       0.8581       1.1420       1.0074       0.9386     630.3039\n",
            "          65       9.8664       1.1455       0.8590       1.1407       1.0046       0.9383     640.1703\n",
            "          66       9.8649       1.1389       0.8634       1.1420       1.0035       0.9395     650.0352\n",
            "          67       9.8699       1.1378       0.8635       1.1414       1.0023       0.9409     659.9051\n",
            "          68       9.8621       1.1277       0.8702       1.1397       1.0003       0.9417     669.7672\n",
            "          69       9.8748       1.1233       0.8713       1.1411       0.9998       0.9433     679.6420\n",
            "          70       9.8631       1.1190       0.8720       1.1403       0.9966       0.9438     689.5052\n",
            "          71       9.8767       1.1146       0.8765       1.1384       0.9948       0.9437     699.3819\n",
            "          72       9.8672       1.1137       0.8768       1.1369       0.9923       0.9459     709.2491\n",
            "          73       9.8653       1.1009       0.8825       1.1377       0.9903       0.9474     719.1143\n",
            "          74       9.8705       1.0926       0.8893       1.1399       0.9869       0.9483     728.9849\n",
            "          75       9.8793       1.0870       0.8921       1.1391       0.9860       0.9482     738.8642\n",
            "          76       9.8557       1.0789       0.8957       1.1404       0.9842       0.9486     748.7199\n",
            "          77       9.8631       1.0715       0.9002       1.1416       0.9800       0.9506     758.5830\n",
            "          78       9.8653       1.0629       0.9047       1.1406       0.9789       0.9500     768.4483\n",
            "          79       9.8695       1.0523       0.9104       1.1393       0.9769       0.9515     778.3178\n",
            "          80       9.8673       1.0452       0.9160       1.1385       0.9741       0.9521     788.1851\n",
            "          81       9.8569       1.0410       0.9173       1.1377       0.9720       0.9536     798.0420\n",
            "          82       9.8480       1.0364       0.9194       1.1381       0.9721       0.9546     807.8900\n",
            "          83       9.8515       1.0363       0.9200       1.1405       0.9695       0.9553     817.7415\n",
            "          84       9.8653       1.0343       0.9211       1.1399       0.9692       0.9543     827.6068\n",
            "          85       9.8683       1.0361       0.9196       1.1394       0.9690       0.9529     837.4751\n",
            "          86       9.8692       1.0345       0.9209       1.1408       0.9686       0.9542     847.3442\n",
            "          87       9.8713       1.0353       0.9215       1.1382       0.9693       0.9553     857.2155\n",
            "          88       9.8622       1.0344       0.9208       1.1392       0.9676       0.9547     867.0777\n",
            "          89       9.8673       1.0330       0.9214       1.1388       0.9673       0.9552     876.9450\n",
            "          90       9.8678       1.0349       0.9207       1.1406       0.9661       0.9544     886.8128\n",
            "          91       9.8726       1.0334       0.9224       1.1390       0.9671       0.9543     896.6854\n",
            "          92       9.8665       1.0327       0.9219       1.1400       0.9669       0.9541     906.5519\n",
            "          93       9.8705       1.0355       0.9197       1.1399       0.9667       0.9541     916.4225\n",
            "          94       9.8792       1.0347       0.9212       1.1379       0.9667       0.9545     926.3017\n",
            "          95       9.8661       1.0327       0.9225       1.1385       0.9665       0.9536     936.1678\n",
            "          96       9.8672       1.0350       0.9204       1.1386       0.9673       0.9536     946.0349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTiqG1hB43RS"
      },
      "source": [
        "PATH2 = \"entire_model_3layers_96epochs_ema8_cutout1616.pt\"\n",
        "torch.save(model_2, PATH2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdDkQjnBVFC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byxyBx_x9Aro",
        "outputId": "60df2cff-855d-41e2-9a6d-0d6fa52c8a2b"
      },
      "source": [
        "seed(123)\n",
        "\n",
        "model_3 = main(96,16,train_transforms=[Crop(32, 32), FlipLR(), Cutout(4,4)])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.061 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0025 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       9.6643       2.0788       0.2994       1.1057       2.2931       0.1167       9.7283\n",
            "           2       9.7187       1.5353       0.6414       1.1173       2.0959       0.2679      19.4469\n",
            "           3       9.8277       1.3131       0.7754       1.1246       1.4775       0.7273      29.2746\n",
            "           4       9.8666       1.2208       0.8278       1.1293       1.1823       0.8595      39.1412\n",
            "           5       9.8666       1.1745       0.8542       1.1345       1.1126       0.8886      49.0078\n",
            "           6       9.8591       1.1439       0.8673       1.1378       1.0826       0.9015      58.8669\n",
            "           7       9.8477       1.1241       0.8771       1.1435       1.0569       0.9131      68.7146\n",
            "           8       9.8684       1.1118       0.8829       1.1447       1.0407       0.9184      78.5830\n",
            "           9       9.8680       1.1025       0.8875       1.1452       1.0309       0.9222      88.4510\n",
            "          10       9.8635       1.1026       0.8857       1.1428       1.0221       0.9276      98.3145\n",
            "          11       9.8650       1.0984       0.8898       1.1419       1.0197       0.9289     108.1795\n",
            "          12       9.8652       1.1022       0.8852       1.1422       1.0168       0.9288     118.0447\n",
            "          13       9.8670       1.1005       0.8872       1.1414       1.0134       0.9312     127.9117\n",
            "          14       9.8662       1.1085       0.8831       1.1398       1.0126       0.9311     137.7779\n",
            "          15       9.8656       1.1094       0.8831       1.1399       1.0162       0.9301     147.6435\n",
            "          16       9.8558       1.1148       0.8795       1.1385       1.0138       0.9321     157.4994\n",
            "          17       9.8580       1.1192       0.8764       1.1400       1.0205       0.9283     167.3573\n",
            "          18       9.8642       1.1240       0.8740       1.1408       1.0199       0.9275     177.2216\n",
            "          19       9.8699       1.1265       0.8731       1.1406       1.0195       0.9279     187.0915\n",
            "          20       9.8685       1.1318       0.8704       1.1431       1.0215       0.9255     196.9600\n",
            "          21       9.8649       1.1259       0.8737       1.1406       1.0179       0.9303     206.8249\n",
            "          22       9.8868       1.1243       0.8740       1.1410       1.0174       0.9296     216.7117\n",
            "          23       9.8697       1.1208       0.8764       1.1400       1.0197       0.9281     226.5815\n",
            "          24       9.8680       1.1183       0.8777       1.1425       1.0168       0.9292     236.4494\n",
            "          25       9.8685       1.1153       0.8812       1.1412       1.0136       0.9326     246.3179\n",
            "          26       9.8672       1.1130       0.8794       1.1408       1.0115       0.9325     256.1851\n",
            "          27       9.8657       1.1113       0.8806       1.1425       1.0101       0.9322     266.0508\n",
            "          28       9.8844       1.1060       0.8839       1.1428       1.0078       0.9352     275.9352\n",
            "          29       9.8736       1.1043       0.8865       1.1407       1.0070       0.9330     285.8088\n",
            "          30       9.8677       1.1013       0.8855       1.1412       1.0064       0.9334     295.6766\n",
            "          31       9.8668       1.0969       0.8906       1.1392       1.0045       0.9357     305.5434\n",
            "          32       9.8692       1.0947       0.8905       1.1412       1.0050       0.9354     315.4125\n",
            "          33       9.8670       1.0934       0.8916       1.1394       1.0024       0.9368     325.2795\n",
            "          34       9.8855       1.0894       0.8935       1.1412       1.0009       0.9371     335.1651\n",
            "          35       9.8744       1.0883       0.8936       1.1413       1.0026       0.9354     345.0394\n",
            "          36       9.8669       1.0857       0.8952       1.1417       1.0018       0.9358     354.9064\n",
            "          37       9.8670       1.0803       0.8980       1.1417       0.9978       0.9369     364.7734\n",
            "          38       9.8636       1.0785       0.8993       1.1410       0.9961       0.9376     374.6370\n",
            "          39       9.8637       1.0777       0.9002       1.1412       0.9978       0.9380     384.5007\n",
            "          40       9.8750       1.0725       0.9024       1.1403       0.9961       0.9384     394.3757\n",
            "          41       9.8744       1.0717       0.9012       1.1418       0.9957       0.9385     404.2501\n",
            "          42       9.8656       1.0664       0.9048       1.1411       0.9936       0.9391     414.1157\n",
            "          43       9.8649       1.0646       0.9073       1.1411       0.9947       0.9398     423.9806\n",
            "          44       9.8644       1.0634       0.9069       1.1414       0.9903       0.9438     433.8449\n",
            "          45       9.8687       1.0603       0.9075       1.1420       0.9893       0.9415     443.7136\n",
            "          46       9.8740       1.0569       0.9100       1.1428       0.9883       0.9438     453.5876\n",
            "          47       9.8683       1.0525       0.9134       1.1404       0.9889       0.9437     463.4559\n",
            "          48       9.8676       1.0525       0.9114       1.1414       0.9865       0.9439     473.3235\n",
            "          49       9.8712       1.0457       0.9167       1.1410       0.9840       0.9457     483.1947\n",
            "          50       9.8629       1.0459       0.9167       1.1408       0.9839       0.9447     493.0576\n",
            "          51       9.8683       1.0410       0.9191       1.1427       0.9821       0.9459     502.9259\n",
            "          52       9.8718       1.0398       0.9191       1.1378       0.9819       0.9453     512.7977\n",
            "          53       9.8680       1.0354       0.9220       1.1423       0.9788       0.9458     522.6657\n",
            "          54       9.8701       1.0281       0.9267       1.1414       0.9792       0.9459     532.5358\n",
            "          55       9.8646       1.0298       0.9250       1.1418       0.9768       0.9468     542.4004\n",
            "          56       9.8663       1.0238       0.9284       1.1423       0.9750       0.9464     552.2667\n",
            "          57       9.8704       1.0197       0.9299       1.1393       0.9758       0.9486     562.1371\n",
            "          58       9.8661       1.0161       0.9331       1.1387       0.9744       0.9488     572.0031\n",
            "          59       9.8871       1.0139       0.9335       1.1374       0.9748       0.9485     581.8902\n",
            "          60       9.8521       1.0142       0.9328       1.1385       0.9724       0.9505     591.7423\n",
            "          61       9.8547       1.0051       0.9376       1.1398       0.9714       0.9505     601.5970\n",
            "          62       9.8655       1.0019       0.9404       1.1366       0.9709       0.9491     611.4626\n",
            "          63       9.8507       0.9981       0.9417       1.1414       0.9718       0.9499     621.3133\n",
            "          64       9.8635       0.9939       0.9437       1.1392       0.9694       0.9502     631.1768\n",
            "          65       9.8897       0.9908       0.9460       1.1410       0.9683       0.9506     641.0665\n",
            "          66       9.8563       0.9848       0.9499       1.1389       0.9668       0.9521     650.9228\n",
            "          67       9.8661       0.9833       0.9501       1.1395       0.9666       0.9523     660.7889\n",
            "          68       9.8666       0.9785       0.9522       1.1420       0.9660       0.9528     670.6555\n",
            "          69       9.8662       0.9716       0.9563       1.1411       0.9637       0.9529     680.5218\n",
            "          70       9.8660       0.9683       0.9578       1.1390       0.9639       0.9524     690.3878\n",
            "          71       9.8636       0.9603       0.9624       1.1419       0.9627       0.9539     700.2514\n",
            "          72       9.8684       0.9565       0.9642       1.1383       0.9605       0.9550     710.1198\n",
            "          73       9.8697       0.9524       0.9663       1.1401       0.9603       0.9545     719.9895\n",
            "          74       9.8700       0.9471       0.9690       1.1410       0.9594       0.9545     729.8596\n",
            "          75       9.8659       0.9427       0.9713       1.1411       0.9584       0.9538     739.7255\n",
            "          76       9.8687       0.9339       0.9759       1.1392       0.9565       0.9550     749.5942\n",
            "          77       9.8514       0.9282       0.9785       1.1398       0.9560       0.9554     759.4456\n",
            "          78       9.8654       0.9232       0.9812       1.1367       0.9556       0.9558     769.3110\n",
            "          79       9.8513       0.9163       0.9850       1.1392       0.9550       0.9565     779.1623\n",
            "          80       9.8708       0.9084       0.9883       1.1385       0.9540       0.9569     789.0330\n",
            "          81       9.8942       0.9034       0.9905       1.1423       0.9533       0.9570     798.9273\n",
            "          82       9.8645       0.9016       0.9916       1.1390       0.9519       0.9584     808.7918\n",
            "          83       9.8700       0.9011       0.9919       1.1394       0.9503       0.9590     818.6618\n",
            "          84       9.8560       0.9032       0.9912       1.1410       0.9508       0.9588     828.5178\n",
            "          85       9.8649       0.9047       0.9902       1.1411       0.9506       0.9593     838.3827\n",
            "          86       9.8691       0.9050       0.9901       1.1383       0.9506       0.9574     848.2518\n",
            "          87       9.8491       0.9081       0.9885       1.1394       0.9508       0.9585     858.1009\n",
            "          88       9.8690       0.9081       0.9886       1.1362       0.9518       0.9588     867.9699\n",
            "          89       9.8598       0.9099       0.9880       1.1389       0.9521       0.9568     877.8297\n",
            "          90       9.8532       0.9107       0.9873       1.1406       0.9518       0.9584     887.6829\n",
            "          91       9.8500       0.9112       0.9872       1.1362       0.9519       0.9576     897.5329\n",
            "          92       9.8529       0.9118       0.9870       1.1400       0.9526       0.9569     907.3858\n",
            "          93       9.8693       0.9103       0.9870       1.1394       0.9518       0.9566     917.2551\n",
            "          94       9.8680       0.9106       0.9871       1.1393       0.9523       0.9570     927.1231\n",
            "          95       9.8772       0.9124       0.9859       1.1437       0.9533       0.9568     937.0003\n",
            "          96       9.8702       0.9151       0.9844       1.1426       0.9530       0.9572     946.8705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-MGu4CO9Yvj",
        "outputId": "38fbb868-846f-465b-97d7-5620b83183fc"
      },
      "source": [
        "seed(123)\n",
        "\n",
        "model_4 = main(96,16,train_transforms=[Crop(32, 32)])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.06 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0023 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1      10.2020       2.0641       0.3142       1.1815       2.2938       0.1085      10.2644\n",
            "           2      10.2099       1.5039       0.6589       1.1701       2.1032       0.2676      20.4744\n",
            "           3      10.0172       1.2873       0.7920       1.1510       1.4996       0.6993      30.4916\n",
            "           4       9.8414       1.1930       0.8455       1.1352       1.1838       0.8572      40.3329\n",
            "           5       9.8301       1.1407       0.8737       1.1312       1.1078       0.8913      50.1631\n",
            "           6       9.8266       1.1067       0.8888       1.1323       1.0782       0.9052      59.9897\n",
            "           7       9.8322       1.0825       0.9024       1.1367       1.0540       0.9144      69.8219\n",
            "           8       9.8398       1.0714       0.9075       1.1430       1.0407       0.9197      79.6617\n",
            "           9       9.8476       1.0594       0.9109       1.1474       1.0303       0.9207      89.5093\n",
            "          10       9.8600       1.0590       0.9124       1.1481       1.0227       0.9256      99.3693\n",
            "          11       9.8550       1.0580       0.9120       1.1474       1.0187       0.9269     109.2243\n",
            "          12       9.8357       1.0553       0.9140       1.1411       1.0145       0.9289     119.0600\n",
            "          13       9.8485       1.0620       0.9092       1.1387       1.0123       0.9305     128.9085\n",
            "          14       9.8458       1.0626       0.9096       1.1371       1.0150       0.9288     138.7543\n",
            "          15       9.8266       1.0695       0.9068       1.1407       1.0142       0.9316     148.5808\n",
            "          16       9.8312       1.0752       0.9030       1.1408       1.0156       0.9295     158.4120\n",
            "          17       9.8438       1.0784       0.9004       1.1436       1.0143       0.9286     168.2558\n",
            "          18       9.8367       1.0821       0.8993       1.1440       1.0154       0.9273     178.0925\n",
            "          19       9.8412       1.0945       0.8918       1.1422       1.0151       0.9300     187.9337\n",
            "          20       9.8517       1.0965       0.8910       1.1443       1.0186       0.9278     197.7854\n",
            "          21       9.8512       1.0876       0.8948       1.1409       1.0163       0.9282     207.6366\n",
            "          22       9.8459       1.0862       0.8975       1.1432       1.0156       0.9300     217.4826\n",
            "          23       9.8404       1.0850       0.8981       1.1409       1.0151       0.9301     227.3230\n",
            "          24       9.8500       1.0835       0.8979       1.1402       1.0150       0.9306     237.1729\n",
            "          25       9.8258       1.0782       0.9012       1.1402       1.0097       0.9305     246.9987\n",
            "          26       9.8657       1.0687       0.9065       1.1408       1.0095       0.9310     256.8644\n",
            "          27       9.8248       1.0701       0.9059       1.1397       1.0103       0.9295     266.6892\n",
            "          28       9.8344       1.0671       0.9076       1.1399       1.0069       0.9322     276.5236\n",
            "          29       9.8481       1.0649       0.9074       1.1396       1.0075       0.9319     286.3717\n",
            "          30       9.8237       1.0621       0.9106       1.1402       1.0046       0.9342     296.1955\n",
            "          31       9.8484       1.0556       0.9138       1.1402       1.0018       0.9342     306.0439\n",
            "          32       9.8472       1.0569       0.9103       1.1373       1.0021       0.9343     315.8911\n",
            "          33       9.8459       1.0527       0.9152       1.1390       1.0019       0.9352     325.7370\n",
            "          34       9.8430       1.0499       0.9166       1.1400       0.9994       0.9370     335.5800\n",
            "          35       9.8440       1.0463       0.9184       1.1427       1.0006       0.9362     345.4240\n",
            "          36       9.8486       1.0408       0.9215       1.1403       0.9967       0.9383     355.2726\n",
            "          37       9.8408       1.0371       0.9238       1.1396       0.9977       0.9371     365.1134\n",
            "          38       9.8500       1.0384       0.9230       1.1409       0.9960       0.9370     374.9633\n",
            "          39       9.8468       1.0322       0.9253       1.1383       0.9941       0.9401     384.8101\n",
            "          40       9.8447       1.0260       0.9281       1.1417       0.9926       0.9389     394.6548\n",
            "          41       9.8468       1.0285       0.9269       1.1405       0.9934       0.9387     404.5016\n",
            "          42       9.8427       1.0212       0.9321       1.1410       0.9914       0.9397     414.3443\n",
            "          43       9.8461       1.0208       0.9319       1.1402       0.9906       0.9399     424.1904\n",
            "          44       9.8634       1.0147       0.9355       1.1420       0.9885       0.9411     434.0538\n",
            "          45       9.8531       1.0140       0.9357       1.1410       0.9890       0.9402     443.9070\n",
            "          46       9.8460       1.0098       0.9379       1.1396       0.9876       0.9407     453.7530\n",
            "          47       9.8400       1.0048       0.9400       1.1410       0.9868       0.9423     463.5930\n",
            "          48       9.8486       1.0043       0.9411       1.1418       0.9838       0.9435     473.4416\n",
            "          49       9.8411       0.9984       0.9430       1.1384       0.9852       0.9427     483.2827\n",
            "          50       9.8266       0.9961       0.9457       1.1388       0.9826       0.9437     493.1093\n",
            "          51       9.8422       0.9915       0.9469       1.1397       0.9825       0.9434     502.9514\n",
            "          52       9.8282       0.9895       0.9486       1.1419       0.9823       0.9437     512.7796\n",
            "          53       9.8453       0.9839       0.9520       1.1389       0.9819       0.9430     522.6250\n",
            "          54       9.8452       0.9802       0.9539       1.1419       0.9799       0.9442     532.4702\n",
            "          55       9.8446       0.9786       0.9547       1.1412       0.9807       0.9446     542.3148\n",
            "          56       9.8457       0.9747       0.9569       1.1413       0.9789       0.9442     552.1605\n",
            "          57       9.8523       0.9724       0.9570       1.1408       0.9788       0.9434     562.0128\n",
            "          58       9.8465       0.9674       0.9606       1.1413       0.9777       0.9457     571.8593\n",
            "          59       9.8397       0.9623       0.9635       1.1404       0.9773       0.9451     581.6990\n",
            "          60       9.8469       0.9578       0.9657       1.1400       0.9763       0.9466     591.5458\n",
            "          61       9.8517       0.9541       0.9676       1.1406       0.9777       0.9430     601.3975\n",
            "          62       9.8454       0.9503       0.9696       1.1410       0.9768       0.9447     611.2429\n",
            "          63       9.8504       0.9460       0.9713       1.1418       0.9756       0.9459     621.0934\n",
            "          64       9.8466       0.9399       0.9740       1.1387       0.9742       0.9462     630.9400\n",
            "          65       9.8468       0.9378       0.9761       1.1417       0.9752       0.9451     640.7867\n",
            "          66       9.8418       0.9342       0.9780       1.1404       0.9742       0.9454     650.6285\n",
            "          67       9.8430       0.9327       0.9775       1.1404       0.9739       0.9449     660.4715\n",
            "          68       9.8475       0.9260       0.9807       1.1396       0.9728       0.9463     670.3191\n",
            "          69       9.8637       0.9214       0.9834       1.1395       0.9728       0.9470     680.1828\n",
            "          70       9.8397       0.9180       0.9848       1.1398       0.9727       0.9467     690.0224\n",
            "          71       9.8442       0.9138       0.9874       1.1399       0.9730       0.9461     699.8666\n",
            "          72       9.8387       0.9081       0.9904       1.1391       0.9722       0.9485     709.7053\n",
            "          73       9.8330       0.9040       0.9911       1.1375       0.9717       0.9463     719.5383\n",
            "          74       9.8342       0.9013       0.9928       1.1414       0.9713       0.9464     729.3726\n",
            "          75       9.8525       0.8967       0.9942       1.1408       0.9706       0.9478     739.2251\n",
            "          76       9.8317       0.8936       0.9955       1.1405       0.9697       0.9481     749.0568\n",
            "          77       9.8436       0.8881       0.9973       1.1391       0.9688       0.9486     758.9003\n",
            "          78       9.8267       0.8850       0.9977       1.1393       0.9686       0.9489     768.7271\n",
            "          79       9.8459       0.8806       0.9990       1.1416       0.9689       0.9487     778.5730\n",
            "          80       9.8404       0.8780       0.9995       1.1396       0.9699       0.9500     788.4134\n",
            "          81       9.8484       0.8758       0.9997       1.1392       0.9693       0.9505     798.2618\n",
            "          82       9.8442       0.8757       0.9997       1.1401       0.9691       0.9493     808.1061\n",
            "          83       9.8445       0.8751       0.9998       1.1398       0.9694       0.9489     817.9506\n",
            "          84       9.8457       0.8750       0.9998       1.1388       0.9692       0.9518     827.7963\n",
            "          85       9.8272       0.8757       0.9998       1.1405       0.9684       0.9519     837.6235\n",
            "          86       9.8497       0.8765       0.9998       1.1401       0.9680       0.9515     847.4732\n",
            "          87       9.8268       0.8778       0.9997       1.1400       0.9680       0.9509     857.3000\n",
            "          88       9.8672       0.8840       0.9981       1.1417       0.9668       0.9504     867.1672\n",
            "          89       9.8429       0.9025       0.9913       1.1406       0.9665       0.9508     877.0101\n",
            "          90       9.8462       0.9140       0.9862       1.1422       0.9667       0.9498     886.8563\n",
            "          91       9.8436       0.9043       0.9910       1.1407       0.9662       0.9523     896.6999\n",
            "          92       9.8456       0.8951       0.9945       1.1409       0.9663       0.9502     906.5454\n",
            "          93       9.8444       0.8888       0.9967       1.1403       0.9660       0.9504     916.3898\n",
            "          94       9.8473       0.8858       0.9973       1.1397       0.9661       0.9497     926.2371\n",
            "          95       9.8286       0.8840       0.9980       1.1404       0.9653       0.9516     936.0656\n",
            "          96       9.8437       0.8829       0.9982       1.1395       0.9656       0.9505     945.9093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h26coriGCK1Q"
      },
      "source": [
        "#### VISUALISATIONS OF THE BUILT MODELS' ACC AND LOSS #######\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MPUREqJUF4D4",
        "outputId": "70684577-5080-42a2-9e76-c2c78a30ede5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('logscutout44.tsv', sep = '\\t')\n",
        "df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>hours</th>\n",
              "      <th>top1Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.002702</td>\n",
              "      <td>11.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.005402</td>\n",
              "      <td>26.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.008132</td>\n",
              "      <td>72.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.010873</td>\n",
              "      <td>85.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>88.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>92</td>\n",
              "      <td>0.252052</td>\n",
              "      <td>95.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>93</td>\n",
              "      <td>0.254793</td>\n",
              "      <td>95.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>94</td>\n",
              "      <td>0.257534</td>\n",
              "      <td>95.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>95</td>\n",
              "      <td>0.260278</td>\n",
              "      <td>95.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>0.263020</td>\n",
              "      <td>95.72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    epoch     hours  top1Accuracy\n",
              "0       1  0.002702         11.67\n",
              "1       2  0.005402         26.79\n",
              "2       3  0.008132         72.73\n",
              "3       4  0.010873         85.95\n",
              "4       5  0.013613         88.86\n",
              "..    ...       ...           ...\n",
              "91     92  0.252052         95.69\n",
              "92     93  0.254793         95.66\n",
              "93     94  0.257534         95.70\n",
              "94     95  0.260278         95.68\n",
              "95     96  0.263020         95.72\n",
              "\n",
              "[96 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaO482gEHhFZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}