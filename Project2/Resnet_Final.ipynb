{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Resnet_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEb09GE9RplR"
      },
      "source": [
        "from inspect import signature\n",
        "import copy\n",
        "from collections import namedtuple, defaultdict\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import singledispatch\n",
        "\n",
        "#####################\n",
        "# utils\n",
        "#####################\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self, synch=None):\n",
        "        self.synch = synch or (lambda: None)\n",
        "        self.synch()\n",
        "        self.times = [time.perf_counter()]\n",
        "        self.total_time = 0.0\n",
        "\n",
        "    def __call__(self, include_in_total=True):\n",
        "        self.synch()\n",
        "        self.times.append(time.perf_counter())\n",
        "        delta_t = self.times[-1] - self.times[-2]\n",
        "        if include_in_total:\n",
        "            self.total_time += delta_t\n",
        "        return delta_t\n",
        "    \n",
        "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
        "\n",
        "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
        "\n",
        "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
        "    formats = formats or default_table_formats\n",
        "    type_ = lambda val: float if isinstance(val, (float, np.float)) else type(val)\n",
        "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
        "\n",
        "def every(n, col): \n",
        "    return lambda data: data[col] % n == 0\n",
        "\n",
        "class Table():\n",
        "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
        "        self.keys, self.report, self.formatter = keys, report, formatter\n",
        "        self.log = []\n",
        "        \n",
        "    def append(self, data):\n",
        "        self.log.append(data)\n",
        "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
        "        self.keys = self.keys or data.keys()\n",
        "        if len(self.log) is 1:\n",
        "            print(*(self.formatter(k, True) for k in self.keys))\n",
        "        if self.report(data):\n",
        "            print(*(self.formatter(data[k]) for k in self.keys))\n",
        "            \n",
        "    def df(self):\n",
        "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])     \n",
        "\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "def preprocess(dataset, transforms):\n",
        "    dataset = copy.copy(dataset) #shallow copy\n",
        "    for transform in transforms:\n",
        "        dataset['data'] = transform(dataset['data'])\n",
        "    return dataset\n",
        "\n",
        "@singledispatch\n",
        "def normalise(x, mean, std):\n",
        "    return (x - mean) / std\n",
        "\n",
        "@normalise.register(np.ndarray) \n",
        "def _(x, mean, std): \n",
        "    #faster inplace for numpy arrays\n",
        "    x = np.array(x, np.float32)\n",
        "    x -= mean\n",
        "    x *= 1.0/std\n",
        "    return x\n",
        "\n",
        "unnormalise = lambda x, mean, std: x*std + mean\n",
        "\n",
        "@singledispatch\n",
        "def pad(x, border):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@pad.register(np.ndarray)\n",
        "def _(x, border): \n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "@singledispatch\n",
        "def transpose(x, source, target):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@transpose.register(np.ndarray)\n",
        "def _(x, source, target):\n",
        "    return x.transpose([source.index(d) for d in target]) \n",
        "\n",
        "#####################\n",
        "## data augmentation\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        return x[..., y0:y0+self.h, x0:x0+self.w]\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
        "    \n",
        "    def output_shape(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return (*_, self.h, self.w)\n",
        "\n",
        "@singledispatch\n",
        "def flip_lr(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@flip_lr.register(np.ndarray)\n",
        "def _(x): \n",
        "    return x[..., ::-1].copy()\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def __call__(self, x, choice):\n",
        "        return flip_lr(x) if choice else x \n",
        "        \n",
        "    def options(self, shape):\n",
        "        return [{'choice': b} for b in [True, False]]\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]    \n",
        "    \n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, dataset, transforms):\n",
        "        self.dataset, self.transforms = dataset, transforms\n",
        "        self.choices = None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "           \n",
        "    def __getitem__(self, index):\n",
        "        data, labels = self.dataset[index]\n",
        "        data = data.copy()\n",
        "        for choices, f in zip(self.choices, self.transforms):\n",
        "            data = f(data, **choices[index])\n",
        "        return data, labels\n",
        "    \n",
        "    def set_random_choices(self):\n",
        "        self.choices = []\n",
        "        x_shape = self.dataset[0][0].shape\n",
        "        N = len(self)\n",
        "        for t in self.transforms:\n",
        "            self.choices.append(np.random.choice(t.options(x_shape), N))\n",
        "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
        "\n",
        "\n",
        "#####################\n",
        "## dict utils\n",
        "#####################\n",
        "\n",
        "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
        "\n",
        "def path_iter(nested_dict, pfx=()):\n",
        "    for name, val in nested_dict.items():\n",
        "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
        "        else: yield ((*pfx, name), val)  \n",
        "\n",
        "def map_nested(func, nested_dict):\n",
        "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
        "\n",
        "def group_by_key(items):\n",
        "    res = defaultdict(list)\n",
        "    for k, v in items: \n",
        "        res[k].append(v) \n",
        "    return res\n",
        "\n",
        "#####################\n",
        "## graph building\n",
        "#####################\n",
        "sep = '/'\n",
        "\n",
        "def split(path):\n",
        "    i = path.rfind(sep) + 1\n",
        "    return path[:i].rstrip(sep), path[i:]\n",
        "\n",
        "def normpath(path):\n",
        "    #simplified os.path.normpath\n",
        "    parts = []\n",
        "    for p in path.split(sep):\n",
        "        if p == '..': parts.pop()\n",
        "        elif p.startswith(sep): parts = [p]\n",
        "        else: parts.append(p)\n",
        "    return sep.join(parts)\n",
        "\n",
        "has_inputs = lambda node: type(node) is tuple\n",
        "\n",
        "def pipeline(net):\n",
        "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
        "\n",
        "def build_graph(net):\n",
        "    flattened = pipeline(net)\n",
        "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
        "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}    \n",
        "\n",
        "#####################\n",
        "## training utils\n",
        "#####################\n",
        "\n",
        "@singledispatch\n",
        "def cat(*xs):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "@singledispatch\n",
        "def to_numpy(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        " \n",
        "class Const(namedtuple('Const', ['val'])):\n",
        "    def __call__(self, x):\n",
        "        return self.val\n",
        "\n",
        "#####################\n",
        "## network visualisation (requires pydot)\n",
        "#####################\n",
        "class ColorMap(dict):\n",
        "    palette = ['#'+x for x in (\n",
        "        'bebada,ffffb3,fb8072,8dd3c7,80b1d3,fdb462,b3de69,fccde5,bc80bd,ccebc5,ffed6f,1f78b4,33a02c,e31a1c,ff7f00,'\n",
        "        '4dddf8,e66493,b07b87,4e90e3,dea05e,d0c281,f0e189,e9e8b1,e0eb71,bbd2a4,6ed641,57eb9c,3ca4d4,92d5e7,b15928'\n",
        "    ).split(',')]\n",
        "\n",
        "    def __missing__(self, key):\n",
        "        self[key] = self.palette[len(self) % len(self.palette)]\n",
        "        return self[key]\n",
        "\n",
        "    def _repr_html_(self):\n",
        "        css = (\n",
        "        '.pill {'\n",
        "            'margin:2px; border-width:1px; border-radius:9px; border-style:solid;'\n",
        "            'display:inline-block; width:100px; height:15px; line-height:15px;'\n",
        "        '}'\n",
        "        '.pill_text {'\n",
        "            'width:90%; margin:auto; font-size:9px; text-align:center; overflow:hidden;'\n",
        "        '}'\n",
        "        )\n",
        "        s = '<div class=pill style=\"background-color:{}\"><div class=pill_text>{}</div></div>'\n",
        "        return '<style>'+css+'</style>'+''.join((s.format(color, text) for text, color in self.items()))\n",
        "\n",
        "def make_dot_graph(nodes, edges, direction='LR', **kwargs):\n",
        "    from pydot import Dot, Cluster, Node, Edge\n",
        "    class Subgraphs(dict):\n",
        "        def __missing__(self, path):\n",
        "            parent, label = split(path)\n",
        "            subgraph = Cluster(path, label=label, style='rounded, filled', fillcolor='#77777744')\n",
        "            self[parent].add_subgraph(subgraph)\n",
        "            return subgraph\n",
        "    g = Dot(rankdir=direction, directed=True, **kwargs)\n",
        "    g.set_node_defaults(\n",
        "        shape='box', style='rounded, filled', fillcolor='#ffffff')\n",
        "    subgraphs = Subgraphs({'': g})\n",
        "    for path, attr in nodes:\n",
        "        parent, label = split(path)\n",
        "        subgraphs[parent].add_node(\n",
        "            Node(name=path, label=label, **attr))\n",
        "    for src, dst, attr in edges:\n",
        "        g.add_edge(Edge(src, dst, **attr))\n",
        "    return g\n",
        "\n",
        "class DotGraph():\n",
        "    def __init__(self, graph, size=15, direction='LR'):\n",
        "        self.nodes = [(k, v) for k, (v,_) in graph.items()]\n",
        "        self.edges = [(src, dst, {}) for dst, (_, inputs) in graph.items() for src in inputs]\n",
        "        self.size, self.direction = size, direction\n",
        "\n",
        "    def dot_graph(self, **kwargs):\n",
        "        return make_dot_graph(self.nodes, self.edges, size=self.size, direction=self.direction,  **kwargs)\n",
        "\n",
        "    def svg(self, **kwargs):\n",
        "        return self.dot_graph(**kwargs).create(format='svg').decode('utf-8')\n",
        "    try:\n",
        "        import pydot\n",
        "        _repr_svg_ = svg\n",
        "    except ImportError:\n",
        "        def __repr__(self): return 'pydot is needed for network visualisation'\n",
        "\n",
        "walk = lambda dct, key: walk(dct, dct[key]) if key in dct else key\n",
        "   \n",
        "def remove_by_type(net, node_type):  \n",
        "    #remove identity nodes for more compact visualisations\n",
        "    graph = build_graph(net)\n",
        "    remap = {k: i[0] for k,(v,i) in graph.items() if isinstance(v, node_type)}\n",
        "    return {k: (v, [walk(remap, x) for x in i]) for k, (v,i) in graph.items() if not isinstance(v, node_type)}"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX5m4uZvs-1H"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import namedtuple \n",
        "from itertools import count\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device(\"cpu\")\n",
        "\n",
        "@cat.register(torch.Tensor)\n",
        "def _(*xs):\n",
        "    return torch.cat(xs)\n",
        "\n",
        "@to_numpy.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return x.detach().cpu().numpy()  \n",
        "\n",
        "@pad.register(torch.Tensor)\n",
        "def _(x, border):\n",
        "    return nn.ReflectionPad2d(border)(x)\n",
        "\n",
        "@transpose.register(torch.Tensor)\n",
        "def _(x, source, target):\n",
        "    return x.permute([source.index(d) for d in target]) \n",
        "\n",
        "def to(*args, **kwargs): \n",
        "    return lambda x: x.to(*args, **kwargs)\n",
        "\n",
        "@flip_lr.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return torch.flip(x, [-1])\n",
        "\n",
        "\n",
        "#####################\n",
        "## dataset\n",
        "#####################\n",
        "from functools import lru_cache as cache\n",
        "\n",
        "@cache(None)\n",
        "def cifar10(root='./data'):\n",
        "    try: \n",
        "        import torchvision\n",
        "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
        "    except ImportError:\n",
        "        from tensorflow.keras import datasets\n",
        "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
        "        return {\n",
        "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
        "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
        "        }\n",
        "             \n",
        "cifar10_mean, cifar10_std = [\n",
        "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
        "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
        "]\n",
        "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')\n",
        "\n",
        "\n",
        "#####################\n",
        "## data loading\n",
        "#####################\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "    \n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices() \n",
        "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.dataloader)\n",
        "\n",
        "#GPU dataloading\n",
        "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
        "\n",
        "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
        "\n",
        "def shuffled(xs, inplace=False):\n",
        "    xs = xs if inplace else copy.copy(xs) \n",
        "    np.random.shuffle(xs)\n",
        "    return xs\n",
        "\n",
        "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
        "    i = torch.randperm(len(data), device=device)\n",
        "    data = data[i]\n",
        "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
        "    data = torch.cat([transform(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
        "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
        "\n",
        "class GPUBatches():\n",
        "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
        "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
        "        N = len(dataset['data'])\n",
        "        self.splits = list(range(0, N+1, batch_size))\n",
        "        if not drop_last and self.splits[-1] != N:\n",
        "            self.splits.append(N)\n",
        "     \n",
        "    def __iter__(self):\n",
        "        data, targets = self.dataset['data'], self.dataset['targets']\n",
        "        for transform in self.transforms:\n",
        "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
        "        if self.shuffle:\n",
        "            i = torch.randperm(len(data), device=device)\n",
        "            data, targets = data[i], targets[i]\n",
        "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.splits) - 1\n",
        "\n",
        "#####################\n",
        "## Layers\n",
        "#####################\n",
        "\n",
        "#Network\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, net):\n",
        "        super().__init__()\n",
        "        self.graph = build_graph(net)\n",
        "        for path, (val, _) in self.graph.items(): \n",
        "            setattr(self, path.replace('/', '_'), val)\n",
        "    \n",
        "    def nodes(self):\n",
        "        return (node for node, _ in self.graph.values())\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        outputs = dict(inputs)\n",
        "        for k, (node, ins) in self.graph.items():\n",
        "            #only compute nodes that are not supplied as inputs.\n",
        "            if k not in outputs: \n",
        "                outputs[k] = node(*[outputs[x] for x in ins])\n",
        "        return outputs\n",
        "    \n",
        "    def half(self):\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
        "                node.half()\n",
        "        return self\n",
        "\n",
        "class Identity(namedtuple('Identity', [])):\n",
        "    def __call__(self, x): return x\n",
        "\n",
        "class Add(namedtuple('Add', [])):\n",
        "    def __call__(self, x, y): return x + y \n",
        "    \n",
        "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
        "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
        "\n",
        "class Mul(nn.Module):\n",
        "    def __init__(self, weight):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "    def __call__(self, x): \n",
        "        return x*self.weight\n",
        "    \n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def forward(self, *xs): return torch.cat(xs, 1)\n",
        "\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
        "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
        "        self.weight.requires_grad = not weight_freeze\n",
        "        self.bias.requires_grad = not bias_freeze\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False): #lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "        return super().train(mode)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return nn.functional.batch_norm(\n",
        "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W) \n",
        "        else:\n",
        "            return nn.functional.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "\n",
        "# Losses\n",
        "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
        "    def __call__(self, log_probs, target):\n",
        "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
        "    \n",
        "class KLLoss(namedtuple('KLLoss', [])):        \n",
        "    def __call__(self, log_probs):\n",
        "        return -log_probs.mean(dim=1)\n",
        "\n",
        "class Correct(namedtuple('Correct', [])):\n",
        "    def __call__(self, classifier, target):\n",
        "        return classifier.max(dim = 1)[1] == target\n",
        "\n",
        "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
        "    def __call__(self, x):\n",
        "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
        "\n",
        "x_ent_loss = Network({\n",
        "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
        "  'acc': (Correct(), ['logits', 'target'])\n",
        "})\n",
        "\n",
        "label_smoothing_loss = lambda alpha: Network({\n",
        "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
        "        'KL':  (KLLoss(), ['logprobs']),\n",
        "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
        "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
        "        'acc': (Correct(), ['logits', 'target']),\n",
        "    })\n",
        "\n",
        "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "#####################\n",
        "## Optimisers\n",
        "##################### \n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    dw.add_(weight_decay, w).mul_(-lr)\n",
        "    v.mul_(momentum).add_(dw)\n",
        "    w.add_(dw.add_(momentum, v))\n",
        "\n",
        "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
        "\n",
        "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
        "\n",
        "def zeros_like(weights):\n",
        "    return [torch.zeros_like(w) for w in weights]\n",
        "\n",
        "def optimiser(weights, param_schedule, update, state_init):\n",
        "    weights = list(weights)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
        "\n",
        "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
        "    step_number += 1\n",
        "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
        "    for w, v in zip(weights, opt_state):\n",
        "        if w.requires_grad:\n",
        "            update(w.data, w.grad.data, v, **param_values)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
        "\n",
        "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
        "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)\n",
        "  \n",
        "#####################\n",
        "## training\n",
        "#####################\n",
        "from itertools import chain\n",
        "\n",
        "def reduce(batches, state, steps):\n",
        "    #state: is a dictionary\n",
        "    #steps: are functions that take (batch, state)\n",
        "    #and return a dictionary of updates to the state (or None)\n",
        "    \n",
        "    for batch in chain(batches, [None]): \n",
        "    #we send an extra batch=None at the end for steps that \n",
        "    #need to do some tidying-up (e.g. log_activations)\n",
        "        for step in steps:\n",
        "            updates = step(batch, state)\n",
        "            if updates:\n",
        "                for k,v in updates.items():\n",
        "                    state[k] = v                  \n",
        "    return state\n",
        "  \n",
        "#define keys in the state dict as constants\n",
        "MODEL = 'model'\n",
        "LOSS = 'loss'\n",
        "VALID_MODEL = 'valid_model'\n",
        "OUTPUT = 'output'\n",
        "OPTS = 'optimisers'\n",
        "ACT_LOG = 'activation_log'\n",
        "WEIGHT_LOG = 'weight_log'\n",
        "\n",
        "#step definitions\n",
        "def forward(training_mode):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training != training_mode: #without the guard it's slow!\n",
        "            model.train(training_mode)\n",
        "        return {OUTPUT: state[LOSS](model(batch))}\n",
        "    return step\n",
        "\n",
        "def forward_tta(tta_transforms):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training:\n",
        "            model.train(False)\n",
        "        logits = torch.mean(torch.stack([model({'input': transform(batch['input'].clone())})['logits'].detach() for transform in tta_transforms], dim=0), dim=0)\n",
        "        return {OUTPUT: state[LOSS](dict(batch, logits=logits))}\n",
        "    return step\n",
        "\n",
        "def backward(dtype=None):\n",
        "    def step(batch, state):\n",
        "        state[MODEL].zero_grad()\n",
        "        if not batch: return\n",
        "        loss = state[OUTPUT][LOSS]\n",
        "        if dtype is not None:\n",
        "            loss = loss.to(dtype)\n",
        "        loss.sum().backward()\n",
        "    return step\n",
        "\n",
        "def opt_steps(batch, state):\n",
        "    if not batch: return\n",
        "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
        "\n",
        "def log_activations(node_names=('loss', 'acc')):\n",
        "    def step(batch, state):\n",
        "        if '_tmp_logs_' not in state: \n",
        "            state['_tmp_logs_'] = []\n",
        "        if batch:\n",
        "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
        "        else:\n",
        "            res = {k: to_numpy(torch.cat(xs)).astype(np.float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
        "            del state['_tmp_logs_']\n",
        "            return {ACT_LOG: res}\n",
        "    return step\n",
        "\n",
        "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
        "\n",
        "def update_ema(momentum, update_freq=1):\n",
        "    n = iter(count())\n",
        "    rho = momentum**update_freq\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        if (next(n) % update_freq) != 0: return\n",
        "        for v, ema_v in zip(state[MODEL].state_dict().values(), state[VALID_MODEL].state_dict().values()):\n",
        "            if not v.dtype.is_floating_point: continue #skip things like num_batches_tracked.\n",
        "            ema_v *= rho\n",
        "            ema_v += (1-rho)*v\n",
        "    return step\n",
        "\n",
        "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
        "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
        "\n",
        "\n",
        "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
        "                on_epoch_end=(lambda state: state)):\n",
        "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
        "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
        "    return {\n",
        "        'train': union({'time': train_time}, train_summary), \n",
        "        'valid': union({'time': valid_time}, valid_summary), \n",
        "        'total time': timer.total_time\n",
        "    }\n",
        "\n",
        "#on_epoch_end\n",
        "def log_weights(state, weights):\n",
        "    state[WEIGHT_LOG] = state.get(WEIGHT_LOG, [])\n",
        "    state[WEIGHT_LOG].append({k: to_numpy(v.data) for k,v in weights.items()})\n",
        "    return state\n",
        "\n",
        "def fine_tune_bn_stats(state, batches, model_key=VALID_MODEL):\n",
        "    reduce(batches, {MODEL: state[model_key]}, [forward(True)])\n",
        "    return state\n",
        "\n",
        "#misc\n",
        "def warmup_cudnn(model, loss, batch):\n",
        "    #run forward and backward pass of the model\n",
        "    #to allow benchmarking of cudnn kernels \n",
        "    reduce([batch], {MODEL: model, LOSS: loss}, [forward(True), backward()])\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "#####################\n",
        "## input whitening\n",
        "#####################\n",
        "\n",
        "def cov(X):\n",
        "    X = X/np.sqrt(X.size(0) - 1)\n",
        "    return X.t() @ X\n",
        "\n",
        "def patches(data, patch_size=(3, 3), dtype=torch.float32):\n",
        "    h, w = patch_size\n",
        "    c = data.size(1)\n",
        "    return data.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1, c, h, w).to(dtype)\n",
        "\n",
        "def eigens(patches):\n",
        "    n,c,h,w = patches.shape\n",
        "    Σ = cov(patches.reshape(n, c*h*w))\n",
        "    Λ, V = torch.symeig(Σ, eigenvectors=True)\n",
        "    return Λ.flip(0), V.t().reshape(c*h*w, c, h, w).flip(0)\n",
        "\n",
        "def whitening_filter(Λ, V, eps=1e-2):\n",
        "    filt = nn.Conv2d(3, 27, kernel_size=(3,3), padding=(1,1), bias=False)\n",
        "    filt.weight.data = (V/torch.sqrt(Λ+eps)[:,None,None,None])\n",
        "    filt.weight.requires_grad = False \n",
        "    return filt"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQpXMeu5to2A"
      },
      "source": [
        "#Network definition\n",
        "def conv_bn_default(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': BatchNorm(c_out), \n",
        "        'relu': nn.ReLU(True)\n",
        "    }\n",
        "    if pool: block['pool'] = pool\n",
        "    return block\n",
        "\n",
        "def residual(c, conv_bn, **kw):\n",
        "    return {\n",
        "        'in': Identity(),\n",
        "        'res1': conv_bn(c, c, **kw),\n",
        "        'res2': conv_bn(c, c, **kw),\n",
        "        'add': (Add(), ['in', 'res2/relu']),\n",
        "    }\n",
        "\n",
        "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), conv_bn=conv_bn_default, prep=conv_bn_default):\n",
        "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
        "    n = {\n",
        "        'input': (None, []),\n",
        "        'prep': prep(3, channels['prep']),\n",
        "        'layer1': conv_bn(channels['prep'], channels['layer1'], pool=pool),\n",
        "        'layer2': conv_bn(channels['layer1'], channels['layer2'], pool=pool),\n",
        "        'layer3': conv_bn(channels['layer2'], channels['layer3'], pool=pool),\n",
        "        'pool': nn.MaxPool2d(4),\n",
        "        'flatten': Flatten(),\n",
        "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
        "        'logits': Mul(weight),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        n[layer]['residual'] = residual(channels[layer], conv_bn)\n",
        "    for layer in extra_layers:\n",
        "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
        "    return n\n",
        "\n",
        "def tsv(logs):\n",
        "    data = [(output['epoch'], output['total time']/3600, output['valid']['acc']*100) for output in logs]\n",
        "    return '\\n'.join(['epoch\\thours\\ttop1Accuracy']+[f'{epoch}\\t{hours:.8f}\\t{acc:.2f}' for (epoch, hours, acc) in data])\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iouYy2hotsK3"
      },
      "source": [
        "import argparse\n",
        "import os.path\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--data_dir', type=str, default='./data')\n",
        "parser.add_argument('--log_dir', type=str, default='.')\n",
        "parser.add_argument('-f')\n",
        "batch_norm = partial(GhostBatchNorm, num_splits=16, weight_freeze=True)\n",
        "relu = partial(nn.CELU, alpha=0.3)\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCRPSJ9vtx86"
      },
      "source": [
        "def conv_bn(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': batch_norm(c_out), \n",
        "        'relu': relu(),\n",
        "    }\n",
        "    if pool: block = {'conv': block['conv'], 'pool': pool, 'bn': block['bn'], 'relu': block['relu']}\n",
        "    return block\n",
        "\n",
        "def whitening_block(c_in, c_out, Λ=None, V=None, eps=1e-2):\n",
        "    return {\n",
        "        'whiten': whitening_filter(Λ, V, eps),\n",
        "        'conv': nn.Conv2d(27, c_out, kernel_size=(1, 1), bias=False),\n",
        "        'norm': batch_norm(c_out), \n",
        "        'act':  relu(),\n",
        "    }\n",
        "\n",
        "def main(epochs, ema_epochs, train_transforms):  \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    print('Downloading datasets')\n",
        "    dataset = map_nested(torch.tensor, cifar10(args.data_dir))\n",
        "\n",
        "    lr_schedule = PiecewiseLinear([0, epochs/5, epochs-ema_epochs], [0, 1.0, 0.1])\n",
        "    batch_size = 512\n",
        "    \n",
        "\n",
        "    print('Warming up torch')\n",
        "    random_data = torch.tensor(np.random.randn(1000,3,32,32).astype(np.float16), device=device)\n",
        "    Λ, V = eigens(patches(random_data))\n",
        "\n",
        "    loss = label_smoothing_loss(0.2)\n",
        "    random_batch = lambda batch_size:  {\n",
        "        'input': torch.Tensor(np.random.rand(batch_size,3,32,32)).cuda().half(), \n",
        "        'target': torch.LongTensor(np.random.randint(0,10,batch_size)).cuda()\n",
        "    }\n",
        "    print('Warming up cudnn on random inputs')\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Λ=Λ, V=V))).to(device).half()\n",
        "    for size in [batch_size, len(dataset['valid']['targets']) % batch_size]:\n",
        "        warmup_cudnn(model, loss, random_batch(size))\n",
        "    \n",
        "    print('Starting timer')\n",
        "    timer = Timer(synch=torch.cuda.synchronize)\n",
        "    \n",
        "    print('Preprocessing training data')\n",
        "    dataset = map_nested(to(device), dataset)\n",
        "    T = lambda x: torch.tensor(x, dtype=torch.float16, device=device)\n",
        "    transforms = [\n",
        "        to(dtype=torch.float16),\n",
        "        partial(normalise, mean=T(cifar10_mean), std=T(cifar10_std)),\n",
        "        partial(transpose, source='NHWC', target='NCHW'), \n",
        "    ]\n",
        "    train_set = preprocess(dataset['train'], transforms + [partial(pad, border=4)])\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "    print('Preprocessing test data')\n",
        "    valid_set = preprocess(dataset['valid'], transforms)\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "\n",
        "    Λ, V = eigens(patches(train_set['data'][:10000,:,4:-4,4:-4])) #center crop to remove padding\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Λ=Λ, V=V))).to(device).half()    \n",
        "   \n",
        "    train_batches = GPUBatches(batch_size=batch_size, transforms=train_transforms, dataset=train_set, shuffle=True,  drop_last=True, max_options=200)\n",
        "    valid_batches = GPUBatches(batch_size=batch_size, dataset=valid_set, shuffle=False, drop_last=False)\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    opts = [\n",
        "        SGD(is_bias[False], {'lr': (lambda step: lr_schedule(step/len(train_batches))/batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}),\n",
        "        SGD(is_bias[True], {'lr': (lambda step: lr_schedule(step/len(train_batches))*(64/batch_size)), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)})\n",
        "    ]\n",
        "    logs, state = Table(), {MODEL: model, VALID_MODEL: copy.deepcopy(model), LOSS: loss, OPTS: opts}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches, valid_batches,\n",
        "                                                          train_steps=(*default_train_steps, update_ema(momentum=0.99, update_freq=5)),\n",
        "                                                          valid_steps=(forward_tta([(lambda x: x), flip_lr]), log_activations(('loss', 'acc'))))))\n",
        "\n",
        "    with open(os.path.join(os.path.expanduser(args.log_dir), 'logs.tsv'), 'w') as f:\n",
        "        f.write(tsv(logs.log))\n",
        "\n",
        "    df=logs.df()\n",
        "    return model, df        \n",
        "        "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWz_wzjRvWQo",
        "outputId": "c45de3aa-3451-4f79-e9a0-1a4bcdea9b02"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(123)\n",
        "\n",
        "model_, dfcut8= main(epochs=96, ema_epochs = 8, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.064 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0024 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       9.4528       2.0910       0.2874       1.0834       2.2944       0.1017       9.5189\n",
            "           2       9.4468       1.5644       0.6234       1.0928       2.0854       0.2956      18.9656\n",
            "           3       9.5429       1.3511       0.7523       1.1004       1.4945       0.7006      28.5086\n",
            "           4       9.6721       1.2620       0.8029       1.1182       1.2055       0.8408      38.1806\n",
            "           5       9.8669       1.2166       0.8281       1.1362       1.1269       0.8818      48.0475\n",
            "           6       9.8776       1.1875       0.8418       1.1543       1.0951       0.8981      57.9251\n",
            "           7      10.0940       1.1649       0.8537       1.1771       1.0726       0.9042      68.0192\n",
            "           8      10.2883       1.1523       0.8588       1.2041       1.0582       0.9107      78.3075\n",
            "           9      10.2821       1.1436       0.8645       1.1779       1.0454       0.9161      88.5896\n",
            "          10      10.1180       1.1333       0.8690       1.1649       1.0358       0.9211      98.7075\n",
            "          11       9.9866       1.1365       0.8672       1.1551       1.0311       0.9247     108.6942\n",
            "          12       9.9211       1.1372       0.8670       1.1544       1.0263       0.9259     118.6152\n",
            "          13      10.0116       1.1395       0.8651       1.1607       1.0244       0.9251     128.6268\n",
            "          14      10.1137       1.1396       0.8651       1.1789       1.0251       0.9233     138.7405\n",
            "          15      10.2796       1.1435       0.8645       1.2174       1.0255       0.9249     149.0201\n",
            "          16      10.4398       1.1471       0.8610       1.2074       1.0265       0.9243     159.4598\n",
            "          17      10.2950       1.1554       0.8569       1.1878       1.0279       0.9241     169.7549\n",
            "          18      10.2161       1.1566       0.8591       1.1828       1.0285       0.9268     179.9710\n",
            "          19      10.1741       1.1596       0.8553       1.1759       1.0297       0.9252     190.1452\n",
            "          20      10.1807       1.1622       0.8549       1.1834       1.0288       0.9256     200.3259\n",
            "          21      10.2175       1.1597       0.8540       1.1856       1.0267       0.9263     210.5433\n",
            "          22      10.2391       1.1596       0.8546       1.1877       1.0258       0.9248     220.7824\n",
            "          23      10.2419       1.1546       0.8591       1.1920       1.0245       0.9285     231.0243\n",
            "          24      10.2471       1.1506       0.8609       1.1927       1.0229       0.9287     241.2713\n",
            "          25      10.2573       1.1478       0.8617       1.1904       1.0248       0.9271     251.5287\n",
            "          26      10.2521       1.1439       0.8630       1.1928       1.0233       0.9269     261.7808\n",
            "          27      10.2513       1.1409       0.8658       1.1850       1.0199       0.9285     272.0321\n",
            "          28      10.2548       1.1428       0.8639       1.1933       1.0191       0.9298     282.2869\n",
            "          29      10.2494       1.1391       0.8666       1.1855       1.0185       0.9297     292.5363\n",
            "          30      10.2415       1.1363       0.8683       1.1865       1.0165       0.9328     302.7777\n",
            "          31      10.2343       1.1306       0.8695       1.1833       1.0166       0.9328     313.0120\n",
            "          32      10.2256       1.1308       0.8692       1.1863       1.0167       0.9300     323.2376\n",
            "          33      10.2198       1.1266       0.8731       1.1839       1.0138       0.9326     333.4574\n",
            "          34      10.2064       1.1231       0.8751       1.1832       1.0133       0.9332     343.6638\n",
            "          35      10.2027       1.1212       0.8764       1.1817       1.0114       0.9331     353.8665\n",
            "          36      10.1939       1.1210       0.8766       1.1811       1.0114       0.9332     364.0605\n",
            "          37      10.1933       1.1174       0.8779       1.1867       1.0129       0.9349     374.2538\n",
            "          38      10.2025       1.1172       0.8782       1.1859       1.0119       0.9342     384.4563\n",
            "          39      10.2173       1.1136       0.8796       1.1880       1.0102       0.9349     394.6736\n",
            "          40      10.2267       1.1092       0.8827       1.1876       1.0064       0.9357     404.9003\n",
            "          41      10.2336       1.1083       0.8819       1.1894       1.0038       0.9361     415.1339\n",
            "          42      10.2384       1.1023       0.8864       1.1876       0.9995       0.9399     425.3723\n",
            "          43      10.2464       1.1011       0.8875       1.1890       1.0000       0.9382     435.6188\n",
            "          44      10.2385       1.0997       0.8877       1.1864       1.0016       0.9373     445.8573\n",
            "          45      10.2327       1.0957       0.8906       1.1876       0.9976       0.9391     456.0899\n",
            "          46      10.2382       1.0951       0.8901       1.1912       0.9973       0.9382     466.3281\n",
            "          47      10.2538       1.0909       0.8928       1.1894       0.9949       0.9415     476.5819\n",
            "          48      10.2361       1.0899       0.8956       1.1862       0.9938       0.9427     486.8179\n",
            "          49      10.2310       1.0885       0.8931       1.1819       0.9933       0.9413     497.0489\n",
            "          50      10.2234       1.0856       0.8936       1.1870       0.9948       0.9400     507.2723\n",
            "          51      10.2093       1.0798       0.8986       1.1822       0.9936       0.9415     517.4816\n",
            "          52      10.2152       1.0803       0.8974       1.1846       0.9921       0.9414     527.6968\n",
            "          53      10.2214       1.0757       0.9004       1.1868       0.9917       0.9420     537.9182\n",
            "          54      10.2225       1.0729       0.9019       1.1850       0.9888       0.9431     548.1407\n",
            "          55      10.2131       1.0689       0.9043       1.1821       0.9871       0.9441     558.3538\n",
            "          56      10.2270       1.0680       0.9040       1.1872       0.9889       0.9434     568.5808\n",
            "          57      10.2216       1.0644       0.9076       1.1846       0.9854       0.9431     578.8024\n",
            "          58      10.2163       1.0629       0.9082       1.1873       0.9845       0.9456     589.0186\n",
            "          59      10.2339       1.0597       0.9090       1.1861       0.9848       0.9446     599.2525\n",
            "          60      10.2323       1.0563       0.9104       1.1897       0.9838       0.9463     609.4848\n",
            "          61      10.2430       1.0511       0.9113       1.1888       0.9811       0.9470     619.7279\n",
            "          62      10.2365       1.0551       0.9112       1.1891       0.9814       0.9460     629.9643\n",
            "          63      10.2384       1.0460       0.9163       1.1848       0.9779       0.9476     640.2027\n",
            "          64      10.2105       1.0465       0.9160       1.1820       0.9777       0.9481     650.4131\n",
            "          65      10.1777       1.0384       0.9198       1.1818       0.9766       0.9469     660.5908\n",
            "          66      10.1981       1.0345       0.9231       1.1903       0.9758       0.9483     670.7890\n",
            "          67      10.2258       1.0363       0.9211       1.1849       0.9747       0.9490     681.0147\n",
            "          68      10.2396       1.0292       0.9254       1.1844       0.9738       0.9490     691.2543\n",
            "          69      10.2374       1.0236       0.9286       1.1919       0.9717       0.9503     701.4917\n",
            "          70      10.2374       1.0218       0.9291       1.1875       0.9715       0.9505     711.7291\n",
            "          71      10.2294       1.0203       0.9303       1.1877       0.9702       0.9498     721.9585\n",
            "          72      10.2114       1.0148       0.9331       1.1819       0.9672       0.9515     732.1699\n",
            "          73      10.1903       1.0091       0.9361       1.1791       0.9675       0.9512     742.3602\n",
            "          74      10.1985       1.0071       0.9367       1.1812       0.9660       0.9531     752.5587\n",
            "          75      10.1896       1.0020       0.9390       1.1800       0.9657       0.9521     762.7483\n",
            "          76      10.1830       0.9976       0.9428       1.1824       0.9640       0.9546     772.9313\n",
            "          77      10.1866       0.9938       0.9443       1.1828       0.9637       0.9532     783.1179\n",
            "          78      10.2006       0.9880       0.9466       1.1851       0.9624       0.9539     793.3185\n",
            "          79      10.2096       0.9820       0.9504       1.1824       0.9615       0.9552     803.5282\n",
            "          80      10.2076       0.9754       0.9543       1.1797       0.9602       0.9558     813.7357\n",
            "          81      10.2037       0.9728       0.9564       1.1832       0.9600       0.9558     823.9394\n",
            "          82      10.2048       0.9689       0.9584       1.1883       0.9574       0.9567     834.1442\n",
            "          83      10.1910       0.9628       0.9614       1.1783       0.9561       0.9583     844.3352\n",
            "          84      10.1954       0.9561       0.9648       1.1789       0.9557       0.9575     854.5306\n",
            "          85      10.1923       0.9532       0.9660       1.1791       0.9554       0.9579     864.7229\n",
            "          86      10.1816       0.9460       0.9701       1.1797       0.9546       0.9568     874.9045\n",
            "          87      10.1902       0.9380       0.9745       1.1807       0.9540       0.9578     885.0947\n",
            "          88      10.2045       0.9314       0.9767       1.1835       0.9527       0.9584     895.2992\n",
            "          89      10.2217       0.9272       0.9800       1.1849       0.9520       0.9572     905.5209\n",
            "          90      10.2344       0.9260       0.9804       1.1866       0.9512       0.9587     915.7553\n",
            "          91      10.2394       0.9242       0.9808       1.1863       0.9507       0.9582     925.9947\n",
            "          92      10.2439       0.9242       0.9813       1.1847       0.9493       0.9596     936.2387\n",
            "          93      10.2546       0.9271       0.9790       1.1855       0.9493       0.9610     946.4932\n",
            "          94      10.2424       0.9288       0.9781       1.1875       0.9500       0.9595     956.7356\n",
            "          95      10.2423       0.9312       0.9771       1.1851       0.9496       0.9595     966.9779\n",
            "          96      10.2358       0.9306       0.9776       1.1908       0.9492       0.9594     977.2137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UOhtkuMwYGR"
      },
      "source": [
        "PATH = \"entire_model_3layers_96epochs_ema8_cutout88.pt\"\n",
        "torch.save(model_, PATH)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjkqUe3XJDOG"
      },
      "source": [
        "import pandas as pd\n",
        "dfcut8.to_csv('dfcut8')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h26coriGCK1Q"
      },
      "source": [
        "#### VISUALISATIONS OF THE BUILT MODELS' ACC AND LOSS #######\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Yeb8QybxKy",
        "outputId": "a3885713-91de-47f6-d8a6-20d79d270377"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaO482gEHhFZ"
      },
      "source": [
        "img_all=np.load(\"drive/MyDrive/test_tensor8.npy\")\n",
        "img_all=img_all.astype(\"int16\")\n",
        "img_all=np.mod(img_all,256)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlQNmMARa0Jv"
      },
      "source": [
        "data = {'data': img_all}\n",
        "dataset = map_nested(torch.tensor, data)\n",
        "dataset = map_nested(to(device), dataset)\n",
        "T = lambda x: torch.tensor(x, dtype=torch.float16, device=device)\n",
        "transforms = [to(dtype=torch.float16),partial(normalise, mean=T(cifar10_mean), std=T(cifar10_std)), partial(transpose, source='NHWC', target='NCHW'), ]\n",
        "test_set = preprocess(dataset, transforms)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmXZcKKdIyf"
      },
      "source": [
        "tensor_test = torch.cuda.HalfTensor(test_set['data'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks-lRFEkdT8t"
      },
      "source": [
        "data_test=torch.utils.data.DataLoader(tensor_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8-NAD-RL5s9",
        "outputId": "dc97dbc3-cd36-4f0b-f65e-de1cf56c18ad"
      },
      "source": [
        "model_.eval()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (prep_whiten): Conv2d(3, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (prep_conv): Conv2d(27, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (prep_norm): GhostBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (prep_act): CELU(alpha=0.3)\n",
              "  (layer1_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res1_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res2_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res2_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (layer2_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer2_bn): GhostBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer2_relu): CELU(alpha=0.3)\n",
              "  (layer3_conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer3_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res1_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res1_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res2_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res2_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten()\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=False)\n",
              "  (logits): Mul()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVB9neaEUI3h",
        "outputId": "42455519-266b-4831-b4fe-3ac6585d2bb6"
      },
      "source": [
        "all_res=np.zeros(300000)\n",
        "for i in np.arange(0,300000,1000):\n",
        "  print(i)\n",
        "  torch.cuda.empty_cache()\n",
        "  random_batch =  {\n",
        "      'input': test_set['data'][i:i+1000], \n",
        "      'target': torch.LongTensor(np.zeros(1000))\n",
        "  }\n",
        "  results=model_.forward(random_batch)\n",
        "  results=results[\"logits\"]\n",
        "  result=torch.argmax(results,axis=1).cpu().detach().numpy()\n",
        "  all_res[i:i+1000]=result\n",
        "\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "100000\n",
            "101000\n",
            "102000\n",
            "103000\n",
            "104000\n",
            "105000\n",
            "106000\n",
            "107000\n",
            "108000\n",
            "109000\n",
            "110000\n",
            "111000\n",
            "112000\n",
            "113000\n",
            "114000\n",
            "115000\n",
            "116000\n",
            "117000\n",
            "118000\n",
            "119000\n",
            "120000\n",
            "121000\n",
            "122000\n",
            "123000\n",
            "124000\n",
            "125000\n",
            "126000\n",
            "127000\n",
            "128000\n",
            "129000\n",
            "130000\n",
            "131000\n",
            "132000\n",
            "133000\n",
            "134000\n",
            "135000\n",
            "136000\n",
            "137000\n",
            "138000\n",
            "139000\n",
            "140000\n",
            "141000\n",
            "142000\n",
            "143000\n",
            "144000\n",
            "145000\n",
            "146000\n",
            "147000\n",
            "148000\n",
            "149000\n",
            "150000\n",
            "151000\n",
            "152000\n",
            "153000\n",
            "154000\n",
            "155000\n",
            "156000\n",
            "157000\n",
            "158000\n",
            "159000\n",
            "160000\n",
            "161000\n",
            "162000\n",
            "163000\n",
            "164000\n",
            "165000\n",
            "166000\n",
            "167000\n",
            "168000\n",
            "169000\n",
            "170000\n",
            "171000\n",
            "172000\n",
            "173000\n",
            "174000\n",
            "175000\n",
            "176000\n",
            "177000\n",
            "178000\n",
            "179000\n",
            "180000\n",
            "181000\n",
            "182000\n",
            "183000\n",
            "184000\n",
            "185000\n",
            "186000\n",
            "187000\n",
            "188000\n",
            "189000\n",
            "190000\n",
            "191000\n",
            "192000\n",
            "193000\n",
            "194000\n",
            "195000\n",
            "196000\n",
            "197000\n",
            "198000\n",
            "199000\n",
            "200000\n",
            "201000\n",
            "202000\n",
            "203000\n",
            "204000\n",
            "205000\n",
            "206000\n",
            "207000\n",
            "208000\n",
            "209000\n",
            "210000\n",
            "211000\n",
            "212000\n",
            "213000\n",
            "214000\n",
            "215000\n",
            "216000\n",
            "217000\n",
            "218000\n",
            "219000\n",
            "220000\n",
            "221000\n",
            "222000\n",
            "223000\n",
            "224000\n",
            "225000\n",
            "226000\n",
            "227000\n",
            "228000\n",
            "229000\n",
            "230000\n",
            "231000\n",
            "232000\n",
            "233000\n",
            "234000\n",
            "235000\n",
            "236000\n",
            "237000\n",
            "238000\n",
            "239000\n",
            "240000\n",
            "241000\n",
            "242000\n",
            "243000\n",
            "244000\n",
            "245000\n",
            "246000\n",
            "247000\n",
            "248000\n",
            "249000\n",
            "250000\n",
            "251000\n",
            "252000\n",
            "253000\n",
            "254000\n",
            "255000\n",
            "256000\n",
            "257000\n",
            "258000\n",
            "259000\n",
            "260000\n",
            "261000\n",
            "262000\n",
            "263000\n",
            "264000\n",
            "265000\n",
            "266000\n",
            "267000\n",
            "268000\n",
            "269000\n",
            "270000\n",
            "271000\n",
            "272000\n",
            "273000\n",
            "274000\n",
            "275000\n",
            "276000\n",
            "277000\n",
            "278000\n",
            "279000\n",
            "280000\n",
            "281000\n",
            "282000\n",
            "283000\n",
            "284000\n",
            "285000\n",
            "286000\n",
            "287000\n",
            "288000\n",
            "289000\n",
            "290000\n",
            "291000\n",
            "292000\n",
            "293000\n",
            "294000\n",
            "295000\n",
            "296000\n",
            "297000\n",
            "298000\n",
            "299000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZwHaRqYVOfM"
      },
      "source": [
        "classnames = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "\n",
        "predictions=classnames[all_res.astype(\"int\")]\n",
        "\n",
        "image_id=np.load(\"image_id.npy\")\n",
        "\n",
        "solution=pd.DataFrame({\"id\" : image_id,\n",
        "\"label\" : predictions})\n",
        "\n",
        "solution.to_csv(\"submission.csv\",index=False)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJtuEguiWcde"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}