{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_fast_Cifar10Kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "065ef231d8b34e0d974a446f91624416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_362dd7066d9548b384bd348b1ca6ae03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8036ba5179bd44dbb62724211a87c2a0",
              "IPY_MODEL_61ee9e1d8f8341299a3f3cdf70455f99"
            ]
          }
        },
        "362dd7066d9548b384bd348b1ca6ae03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8036ba5179bd44dbb62724211a87c2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81b52d10e1ae4d349ecfe963a1f9ab80",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fd91beeb7814effa543249a2e711113"
          }
        },
        "61ee9e1d8f8341299a3f3cdf70455f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0871f05ebf504b73a7679a90b39750bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [16:02&lt;00:00, 177180.85it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3e126e86eb24d27a8430da2ef39437b"
          }
        },
        "81b52d10e1ae4d349ecfe963a1f9ab80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fd91beeb7814effa543249a2e711113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0871f05ebf504b73a7679a90b39750bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3e126e86eb24d27a8430da2ef39437b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijaXjLTgOe17"
      },
      "source": [
        "from inspect import signature\n",
        "import copy\n",
        "from collections import namedtuple, defaultdict\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import singledispatch\n",
        "\n",
        "#####################\n",
        "# utils\n",
        "#####################\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self, synch=None):\n",
        "        self.synch = synch or (lambda: None)\n",
        "        self.synch()\n",
        "        self.times = [time.perf_counter()]\n",
        "        self.total_time = 0.0\n",
        "\n",
        "    def __call__(self, include_in_total=True):\n",
        "        self.synch()\n",
        "        self.times.append(time.perf_counter())\n",
        "        delta_t = self.times[-1] - self.times[-2]\n",
        "        if include_in_total:\n",
        "            self.total_time += delta_t\n",
        "        return delta_t\n",
        "    \n",
        "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
        "\n",
        "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
        "\n",
        "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
        "    formats = formats or default_table_formats\n",
        "    type_ = lambda val: float if isinstance(val, (float, np.float)) else type(val)\n",
        "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
        "\n",
        "def every(n, col): \n",
        "    return lambda data: data[col] % n == 0\n",
        "\n",
        "class Table():\n",
        "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
        "        self.keys, self.report, self.formatter = keys, report, formatter\n",
        "        self.log = []\n",
        "        \n",
        "    def append(self, data):\n",
        "        self.log.append(data)\n",
        "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
        "        self.keys = self.keys or data.keys()\n",
        "        if len(self.log) is 1:\n",
        "            print(*(self.formatter(k, True) for k in self.keys))\n",
        "        if self.report(data):\n",
        "            print(*(self.formatter(data[k]) for k in self.keys))\n",
        "            \n",
        "    def df(self):\n",
        "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])     \n",
        "\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "def preprocess(dataset, transforms):\n",
        "    dataset = copy.copy(dataset) #shallow copy\n",
        "    for transform in transforms:\n",
        "        dataset['data'] = transform(dataset['data'])\n",
        "    return dataset\n",
        "\n",
        "@singledispatch\n",
        "def normalise(x, mean, std):\n",
        "    return (x - mean) / std\n",
        "\n",
        "@normalise.register(np.ndarray) \n",
        "def _(x, mean, std): \n",
        "    #faster inplace for numpy arrays\n",
        "    x = np.array(x, np.float32)\n",
        "    x -= mean\n",
        "    x *= 1.0/std\n",
        "    return x\n",
        "\n",
        "unnormalise = lambda x, mean, std: x*std + mean\n",
        "\n",
        "@singledispatch\n",
        "def pad(x, border):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@pad.register(np.ndarray)\n",
        "def _(x, border): \n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "@singledispatch\n",
        "def transpose(x, source, target):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@transpose.register(np.ndarray)\n",
        "def _(x, source, target):\n",
        "    return x.transpose([source.index(d) for d in target]) \n",
        "\n",
        "#####################\n",
        "## data augmentation\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        return x[..., y0:y0+self.h, x0:x0+self.w]\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
        "    \n",
        "    def output_shape(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return (*_, self.h, self.w)\n",
        "\n",
        "@singledispatch\n",
        "def flip_lr(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "@flip_lr.register(np.ndarray)\n",
        "def _(x): \n",
        "    return x[..., ::-1].copy()\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def __call__(self, x, choice):\n",
        "        return flip_lr(x) if choice else x \n",
        "        \n",
        "    def options(self, shape):\n",
        "        return [{'choice': b} for b in [True, False]]\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]    \n",
        "    \n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, dataset, transforms):\n",
        "        self.dataset, self.transforms = dataset, transforms\n",
        "        self.choices = None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "           \n",
        "    def __getitem__(self, index):\n",
        "        data, labels = self.dataset[index]\n",
        "        data = data.copy()\n",
        "        for choices, f in zip(self.choices, self.transforms):\n",
        "            data = f(data, **choices[index])\n",
        "        return data, labels\n",
        "    \n",
        "    def set_random_choices(self):\n",
        "        self.choices = []\n",
        "        x_shape = self.dataset[0][0].shape\n",
        "        N = len(self)\n",
        "        for t in self.transforms:\n",
        "            self.choices.append(np.random.choice(t.options(x_shape), N))\n",
        "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
        "\n",
        "\n",
        "#####################\n",
        "## dict utils\n",
        "#####################\n",
        "\n",
        "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
        "\n",
        "def path_iter(nested_dict, pfx=()):\n",
        "    for name, val in nested_dict.items():\n",
        "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
        "        else: yield ((*pfx, name), val)  \n",
        "\n",
        "def map_nested(func, nested_dict):\n",
        "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
        "\n",
        "def group_by_key(items):\n",
        "    res = defaultdict(list)\n",
        "    for k, v in items: \n",
        "        res[k].append(v) \n",
        "    return res\n",
        "\n",
        "#####################\n",
        "## graph building\n",
        "#####################\n",
        "sep = '/'\n",
        "\n",
        "def split(path):\n",
        "    i = path.rfind(sep) + 1\n",
        "    return path[:i].rstrip(sep), path[i:]\n",
        "\n",
        "def normpath(path):\n",
        "    #simplified os.path.normpath\n",
        "    parts = []\n",
        "    for p in path.split(sep):\n",
        "        if p == '..': parts.pop()\n",
        "        elif p.startswith(sep): parts = [p]\n",
        "        else: parts.append(p)\n",
        "    return sep.join(parts)\n",
        "\n",
        "has_inputs = lambda node: type(node) is tuple\n",
        "\n",
        "def pipeline(net):\n",
        "    return [(sep.join(path), (node if has_inputs(node) else (node, [-1]))) for (path, node) in path_iter(net)]\n",
        "\n",
        "def build_graph(net):\n",
        "    flattened = pipeline(net)\n",
        "    resolve_input = lambda rel_path, path, idx: normpath(sep.join((path, '..', rel_path))) if isinstance(rel_path, str) else flattened[idx+rel_path][0]\n",
        "    return {path: (node[0], [resolve_input(rel_path, path, idx) for rel_path in node[1]]) for idx, (path, node) in enumerate(flattened)}    \n",
        "\n",
        "#####################\n",
        "## training utils\n",
        "#####################\n",
        "\n",
        "@singledispatch\n",
        "def cat(*xs):\n",
        "    raise NotImplementedError\n",
        "    \n",
        "@singledispatch\n",
        "def to_numpy(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        " \n",
        "class Const(namedtuple('Const', ['val'])):\n",
        "    def __call__(self, x):\n",
        "        return self.val\n",
        "\n",
        "#####################\n",
        "## network visualisation (requires pydot)\n",
        "#####################\n",
        "class ColorMap(dict):\n",
        "    palette = ['#'+x for x in (\n",
        "        'bebada,ffffb3,fb8072,8dd3c7,80b1d3,fdb462,b3de69,fccde5,bc80bd,ccebc5,ffed6f,1f78b4,33a02c,e31a1c,ff7f00,'\n",
        "        '4dddf8,e66493,b07b87,4e90e3,dea05e,d0c281,f0e189,e9e8b1,e0eb71,bbd2a4,6ed641,57eb9c,3ca4d4,92d5e7,b15928'\n",
        "    ).split(',')]\n",
        "\n",
        "    def __missing__(self, key):\n",
        "        self[key] = self.palette[len(self) % len(self.palette)]\n",
        "        return self[key]\n",
        "\n",
        "    def _repr_html_(self):\n",
        "        css = (\n",
        "        '.pill {'\n",
        "            'margin:2px; border-width:1px; border-radius:9px; border-style:solid;'\n",
        "            'display:inline-block; width:100px; height:15px; line-height:15px;'\n",
        "        '}'\n",
        "        '.pill_text {'\n",
        "            'width:90%; margin:auto; font-size:9px; text-align:center; overflow:hidden;'\n",
        "        '}'\n",
        "        )\n",
        "        s = '<div class=pill style=\"background-color:{}\"><div class=pill_text>{}</div></div>'\n",
        "        return '<style>'+css+'</style>'+''.join((s.format(color, text) for text, color in self.items()))\n",
        "\n",
        "def make_dot_graph(nodes, edges, direction='LR', **kwargs):\n",
        "    from pydot import Dot, Cluster, Node, Edge\n",
        "    class Subgraphs(dict):\n",
        "        def __missing__(self, path):\n",
        "            parent, label = split(path)\n",
        "            subgraph = Cluster(path, label=label, style='rounded, filled', fillcolor='#77777744')\n",
        "            self[parent].add_subgraph(subgraph)\n",
        "            return subgraph\n",
        "    g = Dot(rankdir=direction, directed=True, **kwargs)\n",
        "    g.set_node_defaults(\n",
        "        shape='box', style='rounded, filled', fillcolor='#ffffff')\n",
        "    subgraphs = Subgraphs({'': g})\n",
        "    for path, attr in nodes:\n",
        "        parent, label = split(path)\n",
        "        subgraphs[parent].add_node(\n",
        "            Node(name=path, label=label, **attr))\n",
        "    for src, dst, attr in edges:\n",
        "        g.add_edge(Edge(src, dst, **attr))\n",
        "    return g\n",
        "\n",
        "class DotGraph():\n",
        "    def __init__(self, graph, size=15, direction='LR'):\n",
        "        self.nodes = [(k, v) for k, (v,_) in graph.items()]\n",
        "        self.edges = [(src, dst, {}) for dst, (_, inputs) in graph.items() for src in inputs]\n",
        "        self.size, self.direction = size, direction\n",
        "\n",
        "    def dot_graph(self, **kwargs):\n",
        "        return make_dot_graph(self.nodes, self.edges, size=self.size, direction=self.direction,  **kwargs)\n",
        "\n",
        "    def svg(self, **kwargs):\n",
        "        return self.dot_graph(**kwargs).create(format='svg').decode('utf-8')\n",
        "    try:\n",
        "        import pydot\n",
        "        _repr_svg_ = svg\n",
        "    except ImportError:\n",
        "        def __repr__(self): return 'pydot is needed for network visualisation'\n",
        "\n",
        "walk = lambda dct, key: walk(dct, dct[key]) if key in dct else key\n",
        "   \n",
        "def remove_by_type(net, node_type):  \n",
        "    #remove identity nodes for more compact visualisations\n",
        "    graph = build_graph(net)\n",
        "    remap = {k: i[0] for k,(v,i) in graph.items() if isinstance(v, node_type)}\n",
        "    return {k: (v, [walk(remap, x) for x in i]) for k, (v,i) in graph.items() if not isinstance(v, node_type)}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAWAraJyotR5",
        "outputId": "f6dff6a0-3a31-4bcb-e212-ba6e2ff97c82"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqHRBjI-WQ94"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import namedtuple \n",
        "from itertools import count\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device(\"cpu\")\n",
        "\n",
        "@cat.register(torch.Tensor)\n",
        "def _(*xs):\n",
        "    return torch.cat(xs)\n",
        "\n",
        "@to_numpy.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return x.detach().cpu().numpy()  \n",
        "\n",
        "@pad.register(torch.Tensor)\n",
        "def _(x, border):\n",
        "    return nn.ReflectionPad2d(border)(x)\n",
        "\n",
        "@transpose.register(torch.Tensor)\n",
        "def _(x, source, target):\n",
        "    return x.permute([source.index(d) for d in target]) \n",
        "\n",
        "def to(*args, **kwargs): \n",
        "    return lambda x: x.to(*args, **kwargs)\n",
        "\n",
        "@flip_lr.register(torch.Tensor)\n",
        "def _(x):\n",
        "    return torch.flip(x, [-1])\n",
        "\n",
        "\n",
        "#####################\n",
        "## dataset\n",
        "#####################\n",
        "from functools import lru_cache as cache\n",
        "\n",
        "@cache(None)\n",
        "def cifar10(root='./data'):\n",
        "    try: \n",
        "        import torchvision\n",
        "        download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "        return {k: {'data': v.data, 'targets': v.targets} for k,v in [('train', download(train=True)), ('valid', download(train=False))]}\n",
        "    except ImportError:\n",
        "        from tensorflow.keras import datasets\n",
        "        (train_images, train_labels), (valid_images, valid_labels) = datasets.cifar10.load_data()\n",
        "        return {\n",
        "            'train': {'data': train_images, 'targets': train_labels.squeeze()},\n",
        "            'valid': {'data': valid_images, 'targets': valid_labels.squeeze()}\n",
        "        }\n",
        "             \n",
        "cifar10_mean, cifar10_std = [\n",
        "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
        "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
        "]\n",
        "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')\n",
        "\n",
        "\n",
        "#####################\n",
        "## data loading\n",
        "#####################\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "    \n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices() \n",
        "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.dataloader)\n",
        "\n",
        "#GPU dataloading\n",
        "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
        "\n",
        "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
        "\n",
        "def shuffled(xs, inplace=False):\n",
        "    xs = xs if inplace else copy.copy(xs) \n",
        "    np.random.shuffle(xs)\n",
        "    return xs\n",
        "\n",
        "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
        "    i = torch.randperm(len(data), device=device)\n",
        "    data = data[i]\n",
        "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
        "    data = torch.cat([transform(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
        "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
        "\n",
        "class GPUBatches():\n",
        "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
        "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
        "        N = len(dataset['data'])\n",
        "        self.splits = list(range(0, N+1, batch_size))\n",
        "        if not drop_last and self.splits[-1] != N:\n",
        "            self.splits.append(N)\n",
        "     \n",
        "    def __iter__(self):\n",
        "        data, targets = self.dataset['data'], self.dataset['targets']\n",
        "        for transform in self.transforms:\n",
        "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
        "        if self.shuffle:\n",
        "            i = torch.randperm(len(data), device=device)\n",
        "            data, targets = data[i], targets[i]\n",
        "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.splits) - 1\n",
        "\n",
        "#####################\n",
        "## Layers\n",
        "#####################\n",
        "\n",
        "#Network\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, net):\n",
        "        super().__init__()\n",
        "        self.graph = build_graph(net)\n",
        "        for path, (val, _) in self.graph.items(): \n",
        "            setattr(self, path.replace('/', '_'), val)\n",
        "    \n",
        "    def nodes(self):\n",
        "        return (node for node, _ in self.graph.values())\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        outputs = dict(inputs)\n",
        "        for k, (node, ins) in self.graph.items():\n",
        "            #only compute nodes that are not supplied as inputs.\n",
        "            if k not in outputs: \n",
        "                outputs[k] = node(*[outputs[x] for x in ins])\n",
        "        return outputs\n",
        "    \n",
        "    def half(self):\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
        "                node.half()\n",
        "        return self\n",
        "\n",
        "class Identity(namedtuple('Identity', [])):\n",
        "    def __call__(self, x): return x\n",
        "\n",
        "class Add(namedtuple('Add', [])):\n",
        "    def __call__(self, x, y): return x + y \n",
        "    \n",
        "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
        "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
        "\n",
        "class Mul(nn.Module):\n",
        "    def __init__(self, weight):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "    def __call__(self, x): \n",
        "        return x*self.weight\n",
        "    \n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def forward(self, *xs): return torch.cat(xs, 1)\n",
        "\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight_freeze=False, bias_freeze=False, weight_init=1.0, bias_init=0.0):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        if weight_init is not None: self.weight.data.fill_(weight_init)\n",
        "        if bias_init is not None: self.bias.data.fill_(bias_init)\n",
        "        self.weight.requires_grad = not weight_freeze\n",
        "        self.bias.requires_grad = not bias_freeze\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False): #lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "        return super().train(mode)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return nn.functional.batch_norm(\n",
        "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W) \n",
        "        else:\n",
        "            return nn.functional.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "\n",
        "# Losses\n",
        "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
        "    def __call__(self, log_probs, target):\n",
        "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
        "    \n",
        "class KLLoss(namedtuple('KLLoss', [])):        \n",
        "    def __call__(self, log_probs):\n",
        "        return -log_probs.mean(dim=1)\n",
        "\n",
        "class Correct(namedtuple('Correct', [])):\n",
        "    def __call__(self, classifier, target):\n",
        "        return classifier.max(dim = 1)[1] == target\n",
        "\n",
        "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
        "    def __call__(self, x):\n",
        "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
        "\n",
        "x_ent_loss = Network({\n",
        "  'loss':  (nn.CrossEntropyLoss(reduction='none'), ['logits', 'target']),\n",
        "  'acc': (Correct(), ['logits', 'target'])\n",
        "})\n",
        "\n",
        "label_smoothing_loss = lambda alpha: Network({\n",
        "        'logprobs': (LogSoftmax(dim=1), ['logits']),\n",
        "        'KL':  (KLLoss(), ['logprobs']),\n",
        "        'xent':  (CrossEntropyLoss(), ['logprobs', 'target']),\n",
        "        'loss': (AddWeighted(wx=1-alpha, wy=alpha), ['xent', 'KL']),\n",
        "        'acc': (Correct(), ['logits', 'target']),\n",
        "    })\n",
        "\n",
        "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "#####################\n",
        "## Optimisers\n",
        "##################### \n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    dw.add_(weight_decay, w).mul_(-lr)\n",
        "    v.mul_(momentum).add_(dw)\n",
        "    w.add_(dw.add_(momentum, v))\n",
        "\n",
        "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
        "\n",
        "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
        "\n",
        "def zeros_like(weights):\n",
        "    return [torch.zeros_like(w) for w in weights]\n",
        "\n",
        "def optimiser(weights, param_schedule, update, state_init):\n",
        "    weights = list(weights)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
        "\n",
        "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
        "    step_number += 1\n",
        "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
        "    for w, v in zip(weights, opt_state):\n",
        "        if w.requires_grad:\n",
        "            update(w.data, w.grad.data, v, **param_values)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
        "\n",
        "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
        "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)\n",
        "  \n",
        "#####################\n",
        "## training\n",
        "#####################\n",
        "from itertools import chain\n",
        "\n",
        "def reduce(batches, state, steps):\n",
        "    #state: is a dictionary\n",
        "    #steps: are functions that take (batch, state)\n",
        "    #and return a dictionary of updates to the state (or None)\n",
        "    \n",
        "    for batch in chain(batches, [None]): \n",
        "    #we send an extra batch=None at the end for steps that \n",
        "    #need to do some tidying-up (e.g. log_activations)\n",
        "        for step in steps:\n",
        "            updates = step(batch, state)\n",
        "            if updates:\n",
        "                for k,v in updates.items():\n",
        "                    state[k] = v                  \n",
        "    return state\n",
        "  \n",
        "#define keys in the state dict as constants\n",
        "MODEL = 'model'\n",
        "LOSS = 'loss'\n",
        "VALID_MODEL = 'valid_model'\n",
        "OUTPUT = 'output'\n",
        "OPTS = 'optimisers'\n",
        "ACT_LOG = 'activation_log'\n",
        "WEIGHT_LOG = 'weight_log'\n",
        "\n",
        "#step definitions\n",
        "def forward(training_mode):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training != training_mode: #without the guard it's slow!\n",
        "            model.train(training_mode)\n",
        "        return {OUTPUT: state[LOSS](model(batch))}\n",
        "    return step\n",
        "\n",
        "def forward_tta(tta_transforms):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training:\n",
        "            model.train(False)\n",
        "        logits = torch.mean(torch.stack([model({'input': transform(batch['input'].clone())})['logits'].detach() for transform in tta_transforms], dim=0), dim=0)\n",
        "        return {OUTPUT: state[LOSS](dict(batch, logits=logits))}\n",
        "    return step\n",
        "\n",
        "def backward(dtype=None):\n",
        "    def step(batch, state):\n",
        "        state[MODEL].zero_grad()\n",
        "        if not batch: return\n",
        "        loss = state[OUTPUT][LOSS]\n",
        "        if dtype is not None:\n",
        "            loss = loss.to(dtype)\n",
        "        loss.sum().backward()\n",
        "    return step\n",
        "\n",
        "def opt_steps(batch, state):\n",
        "    if not batch: return\n",
        "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
        "\n",
        "def log_activations(node_names=('loss', 'acc')):\n",
        "    def step(batch, state):\n",
        "        if '_tmp_logs_' not in state: \n",
        "            state['_tmp_logs_'] = []\n",
        "        if batch:\n",
        "            state['_tmp_logs_'].extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
        "        else:\n",
        "            res = {k: to_numpy(torch.cat(xs)).astype(np.float) for k, xs in group_by_key(state['_tmp_logs_']).items()}\n",
        "            del state['_tmp_logs_']\n",
        "            return {ACT_LOG: res}\n",
        "    return step\n",
        "\n",
        "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
        "\n",
        "def update_ema(momentum, update_freq=1):\n",
        "    n = iter(count())\n",
        "    rho = momentum**update_freq\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        if (next(n) % update_freq) != 0: return\n",
        "        for v, ema_v in zip(state[MODEL].state_dict().values(), state[VALID_MODEL].state_dict().values()):\n",
        "            if not v.dtype.is_floating_point: continue #skip things like num_batches_tracked.\n",
        "            ema_v *= rho\n",
        "            ema_v += (1-rho)*v\n",
        "    return step\n",
        "\n",
        "default_train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
        "default_valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
        "\n",
        "\n",
        "def train_epoch(state, timer, train_batches, valid_batches, train_steps=default_train_steps, valid_steps=default_valid_steps, \n",
        "                on_epoch_end=(lambda state: state)):\n",
        "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
        "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(include_in_total=False) #DAWNBench rules\n",
        "    return {\n",
        "        'train': union({'time': train_time}, train_summary), \n",
        "        'valid': union({'time': valid_time}, valid_summary), \n",
        "        'total time': timer.total_time\n",
        "    }\n",
        "\n",
        "#on_epoch_end\n",
        "def log_weights(state, weights):\n",
        "    state[WEIGHT_LOG] = state.get(WEIGHT_LOG, [])\n",
        "    state[WEIGHT_LOG].append({k: to_numpy(v.data) for k,v in weights.items()})\n",
        "    return state\n",
        "\n",
        "def fine_tune_bn_stats(state, batches, model_key=VALID_MODEL):\n",
        "    reduce(batches, {MODEL: state[model_key]}, [forward(True)])\n",
        "    return state\n",
        "\n",
        "#misc\n",
        "def warmup_cudnn(model, loss, batch):\n",
        "    #run forward and backward pass of the model\n",
        "    #to allow benchmarking of cudnn kernels \n",
        "    reduce([batch], {MODEL: model, LOSS: loss}, [forward(True), backward()])\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "#####################\n",
        "## input whitening\n",
        "#####################\n",
        "\n",
        "def cov(X):\n",
        "    X = X/np.sqrt(X.size(0) - 1)\n",
        "    return X.t() @ X\n",
        "\n",
        "def patches(data, patch_size=(3, 3), dtype=torch.float32):\n",
        "    h, w = patch_size\n",
        "    c = data.size(1)\n",
        "    return data.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1, c, h, w).to(dtype)\n",
        "\n",
        "def eigens(patches):\n",
        "    n,c,h,w = patches.shape\n",
        "    Î£ = cov(patches.reshape(n, c*h*w))\n",
        "    Î›, V = torch.symeig(Î£, eigenvectors=True)\n",
        "    return Î›.flip(0), V.t().reshape(c*h*w, c, h, w).flip(0)\n",
        "\n",
        "def whitening_filter(Î›, V, eps=1e-2):\n",
        "    filt = nn.Conv2d(3, 27, kernel_size=(3,3), padding=(1,1), bias=False)\n",
        "    filt.weight.data = (V/torch.sqrt(Î›+eps)[:,None,None,None])\n",
        "    filt.weight.requires_grad = False \n",
        "    return filt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r1zVzM2WXRB"
      },
      "source": [
        "########## 3-layer Network definition #############\n",
        "def conv_bn_default(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': BatchNorm(c_out), \n",
        "        'relu': nn.ReLU(True)\n",
        "    }\n",
        "    if pool: block['pool'] = pool\n",
        "    return block\n",
        "\n",
        "def residual(c, conv_bn, **kw):\n",
        "    return {\n",
        "        'in': Identity(),\n",
        "        'res1': conv_bn(c, c, **kw),\n",
        "        'res2': conv_bn(c, c, **kw),\n",
        "        'add': (Add(), ['in', 'res2/relu']),\n",
        "    }\n",
        "\n",
        "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), conv_bn=conv_bn_default, prep=conv_bn_default):\n",
        "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
        "    n = {\n",
        "        'input': (None, []),\n",
        "        'prep': prep(3, channels['prep']),\n",
        "        'layer1': conv_bn(channels['prep'], channels['layer1'], pool=pool),\n",
        "        'layer2': conv_bn(channels['layer1'], channels['layer2'], pool=pool),\n",
        "        'layer3': conv_bn(channels['layer2'], channels['layer3'], pool=pool),\n",
        "        'pool': nn.MaxPool2d(4),\n",
        "        'flatten': Flatten(),\n",
        "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
        "        'logits': Mul(weight),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        n[layer]['residual'] = residual(channels[layer], conv_bn)\n",
        "    for layer in extra_layers:\n",
        "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
        "    return n\n",
        "\n",
        "def tsv(logs):\n",
        "    data = [(output['epoch'], output['total time']/3600, output['valid']['acc']*100) for output in logs]\n",
        "    return '\\n'.join(['epoch\\thours\\ttop1Accuracy']+[f'{epoch}\\t{hours:.8f}\\t{acc:.2f}' for (epoch, hours, acc) in data])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ATW-KUYtLn"
      },
      "source": [
        "import argparse\n",
        "import os.path\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--data_dir', type=str, default='./data')\n",
        "parser.add_argument('--log_dir', type=str, default='.')\n",
        "parser.add_argument('-f')\n",
        "batch_norm = partial(GhostBatchNorm, num_splits=16, weight_freeze=True)\n",
        "relu = partial(nn.CELU, alpha=0.3)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YhPuVkKZNIT"
      },
      "source": [
        "def conv_bn(c_in, c_out, pool=None):\n",
        "    block = {\n",
        "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
        "        'bn': batch_norm(c_out), \n",
        "        'relu': relu(),\n",
        "    }\n",
        "    if pool: block = {'conv': block['conv'], 'pool': pool, 'bn': block['bn'], 'relu': block['relu']}\n",
        "    return block\n",
        "\n",
        "def whitening_block(c_in, c_out, Î›=None, V=None, eps=1e-2):\n",
        "    return {\n",
        "        'whiten': whitening_filter(Î›, V, eps),\n",
        "        'conv': nn.Conv2d(27, c_out, kernel_size=(1, 1), bias=False),\n",
        "        'norm': batch_norm(c_out), \n",
        "        'act':  relu(),\n",
        "    }\n",
        "\n",
        "def main(epochs, ema_epochs, train_transforms):  \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    print('Downloading datasets')\n",
        "    dataset = map_nested(torch.tensor, cifar10(args.data_dir))\n",
        "\n",
        "    lr_schedule = PiecewiseLinear([0, epochs/5, epochs-ema_epochs], [0, 1.0, 0.1])\n",
        "    batch_size = 512\n",
        "    \n",
        "\n",
        "    print('Warming up torch')\n",
        "    random_data = torch.tensor(np.random.randn(1000,3,32,32).astype(np.float16), device=device)\n",
        "    Î›, V = eigens(patches(random_data))\n",
        "\n",
        "    loss = label_smoothing_loss(0.2)\n",
        "    random_batch = lambda batch_size:  {\n",
        "        'input': torch.Tensor(np.random.rand(batch_size,3,32,32)).cuda().half(), \n",
        "        'target': torch.LongTensor(np.random.randint(0,10,batch_size)).cuda()\n",
        "    }\n",
        "    print('Warming up cudnn on random inputs')\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Î›=Î›, V=V))).to(device).half()\n",
        "    for size in [batch_size, len(dataset['valid']['targets']) % batch_size]:\n",
        "        warmup_cudnn(model, loss, random_batch(size))\n",
        "    \n",
        "    print('Starting timer')\n",
        "    timer = Timer(synch=torch.cuda.synchronize)\n",
        "    \n",
        "    print('Preprocessing training data')\n",
        "    dataset = map_nested(to(device), dataset)\n",
        "    T = lambda x: torch.tensor(x, dtype=torch.float16, device=device)\n",
        "    transforms = [\n",
        "        to(dtype=torch.float16),\n",
        "        partial(normalise, mean=T(cifar10_mean), std=T(cifar10_std)),\n",
        "        partial(transpose, source='NHWC', target='NCHW'), \n",
        "    ]\n",
        "    train_set = preprocess(dataset['train'], transforms + [partial(pad, border=4)])\n",
        "    return train_set\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "    print('Preprocessing test data')\n",
        "    valid_set = preprocess(dataset['valid'], transforms)\n",
        "    print(f'Finished in {timer():.2} seconds')\n",
        "\n",
        "    Î›, V = eigens(patches(train_set['data'][:10000,:,4:-4,4:-4])) #center crop to remove padding\n",
        "    model = Network(net(weight=1/16, conv_bn=conv_bn, prep=partial(whitening_block, Î›=Î›, V=V))).to(device).half()    \n",
        "   \n",
        "    train_batches = GPUBatches(batch_size=batch_size, transforms=train_transforms, dataset=train_set, shuffle=True,  drop_last=True, max_options=200)\n",
        "    valid_batches = GPUBatches(batch_size=batch_size, dataset=valid_set, shuffle=False, drop_last=False)\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    opts = [\n",
        "        SGD(is_bias[False], {'lr': (lambda step: lr_schedule(step/len(train_batches))/batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}),\n",
        "        SGD(is_bias[True], {'lr': (lambda step: lr_schedule(step/len(train_batches))*(64/batch_size)), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)})\n",
        "    ]\n",
        "    logs, state = Table(), {MODEL: model, VALID_MODEL: copy.deepcopy(model), LOSS: loss, OPTS: opts}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches, valid_batches,\n",
        "                                                          train_steps=(*default_train_steps, update_ema(momentum=0.99, update_freq=5)),\n",
        "                                                          valid_steps=(forward_tta([(lambda x: x), flip_lr]), log_activations(('loss', 'acc'))))))\n",
        "\n",
        "    with open(os.path.join(os.path.expanduser(args.log_dir), 'logs.tsv'), 'w') as f:\n",
        "        f.write(tsv(logs.log))\n",
        "    \n",
        "    data = logs.df()\n",
        "\n",
        "    return model, data"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "065ef231d8b34e0d974a446f91624416",
            "362dd7066d9548b384bd348b1ca6ae03",
            "8036ba5179bd44dbb62724211a87c2a0",
            "61ee9e1d8f8341299a3f3cdf70455f99",
            "81b52d10e1ae4d349ecfe963a1f9ab80",
            "2fd91beeb7814effa543249a2e711113",
            "0871f05ebf504b73a7679a90b39750bb",
            "c3e126e86eb24d27a8430da2ef39437b"
          ]
        },
        "id": "1UtZwsSqapGx",
        "outputId": "27953a17-4851-45ee-977d-a91039e23af2"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(123)\n",
        "\n",
        "model3l, logs3l = main(epochs=96, ema_epochs = 8, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "065ef231d8b34e0d974a446f91624416",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.14 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0032 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:242: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       8.8617       2.0782       0.3023       1.0724       2.2902       0.1149       9.0028\n",
            "           2       8.7185       1.5638       0.6222       1.0722       2.0682       0.3353      17.7212\n",
            "           3       8.7187       1.3545       0.7498       1.0717       1.4723       0.7201      26.4399\n",
            "           4       8.7178       1.2649       0.8027       1.0719       1.2009       0.8451      35.1577\n",
            "           5       8.7162       1.2206       0.8252       1.0729       1.1337       0.8771      43.8740\n",
            "           6       8.7178       1.1900       0.8430       1.0719       1.0986       0.8948      52.5917\n",
            "           7       8.7194       1.1659       0.8541       1.0713       1.0742       0.9028      61.3111\n",
            "           8       8.7162       1.1499       0.8620       1.0714       1.0563       0.9123      70.0273\n",
            "           9       8.7174       1.1479       0.8615       1.0709       1.0454       0.9157      78.7447\n",
            "          10       8.7168       1.1369       0.8680       1.0717       1.0371       0.9190      87.4615\n",
            "          11       8.7147       1.1382       0.8667       1.0717       1.0319       0.9218      96.1762\n",
            "          12       8.7183       1.1379       0.8668       1.0712       1.0286       0.9239     104.8944\n",
            "          13       8.7154       1.1374       0.8672       1.0717       1.0265       0.9237     113.6098\n",
            "          14       8.7138       1.1424       0.8636       1.0714       1.0261       0.9238     122.3237\n",
            "          15       8.7120       1.1440       0.8635       1.0707       1.0257       0.9256     131.0357\n",
            "          16       8.7144       1.1488       0.8605       1.0705       1.0305       0.9245     139.7501\n",
            "          17       8.7150       1.1528       0.8591       1.0703       1.0311       0.9235     148.4651\n",
            "          18       8.7141       1.1610       0.8537       1.0702       1.0323       0.9232     157.1791\n",
            "          19       8.7137       1.1620       0.8541       1.0711       1.0309       0.9238     165.8928\n",
            "          20       8.7145       1.1668       0.8515       1.0706       1.0306       0.9236     174.6073\n",
            "          21       8.7132       1.1640       0.8543       1.0702       1.0312       0.9250     183.3205\n",
            "          22       8.7130       1.1584       0.8551       1.0703       1.0342       0.9231     192.0335\n",
            "          23       8.7127       1.1565       0.8572       1.0705       1.0355       0.9229     200.7462\n",
            "          24       8.7127       1.1523       0.8594       1.0701       1.0297       0.9260     209.4589\n",
            "          25       8.7130       1.1501       0.8617       1.0709       1.0265       0.9255     218.1719\n",
            "          26       8.7110       1.1457       0.8625       1.0704       1.0262       0.9284     226.8829\n",
            "          27       8.7107       1.1421       0.8636       1.0699       1.0285       0.9247     235.5936\n",
            "          28       8.7131       1.1408       0.8655       1.0716       1.0249       0.9277     244.3067\n",
            "          29       8.7135       1.1359       0.8686       1.0707       1.0204       0.9303     253.0202\n",
            "          30       8.7135       1.1367       0.8678       1.0707       1.0247       0.9291     261.7337\n",
            "          31       8.7116       1.1292       0.8712       1.0705       1.0168       0.9321     270.4453\n",
            "          32       8.7144       1.1309       0.8699       1.0703       1.0168       0.9327     279.1597\n",
            "          33       8.7122       1.1299       0.8710       1.0701       1.0153       0.9343     287.8719\n",
            "          34       8.7102       1.1220       0.8760       1.0700       1.0157       0.9337     296.5821\n",
            "          35       8.7113       1.1215       0.8763       1.0704       1.0140       0.9344     305.2933\n",
            "          36       8.7118       1.1197       0.8768       1.0707       1.0092       0.9367     314.0052\n",
            "          37       8.7133       1.1213       0.8773       1.0704       1.0112       0.9313     322.7184\n",
            "          38       8.7124       1.1169       0.8783       1.0708       1.0073       0.9348     331.4308\n",
            "          39       8.7117       1.1119       0.8820       1.0709       1.0066       0.9369     340.1425\n",
            "          40       8.7093       1.1073       0.8835       1.0705       1.0038       0.9375     348.8518\n",
            "          41       8.7113       1.1067       0.8831       1.0705       1.0029       0.9364     357.5630\n",
            "          42       8.7121       1.1042       0.8850       1.0704       1.0027       0.9384     366.2751\n",
            "          43       8.7145       1.0973       0.8890       1.0706       1.0033       0.9373     374.9896\n",
            "          44       8.7113       1.1030       0.8856       1.0699       1.0001       0.9392     383.7010\n",
            "          45       8.7126       1.0953       0.8901       1.0707       0.9990       0.9410     392.4136\n",
            "          46       8.7132       1.0945       0.8896       1.0706       0.9982       0.9384     401.1267\n",
            "          47       8.7116       1.0929       0.8907       1.0701       0.9970       0.9400     409.8383\n",
            "          48       8.7107       1.0893       0.8927       1.0701       0.9983       0.9387     418.5490\n",
            "          49       8.7122       1.0887       0.8935       1.0699       0.9955       0.9399     427.2612\n",
            "          50       8.7102       1.0862       0.8955       1.0699       0.9950       0.9410     435.9714\n",
            "          51       8.7133       1.0811       0.8978       1.0703       0.9937       0.9417     444.6847\n",
            "          52       8.7111       1.0799       0.8987       1.0703       0.9913       0.9425     453.3959\n",
            "          53       8.7124       1.0766       0.8994       1.0701       0.9912       0.9417     462.1082\n",
            "          54       8.7089       1.0703       0.9024       1.0699       0.9892       0.9440     470.8171\n",
            "          55       8.7131       1.0709       0.9020       1.0703       0.9889       0.9427     479.5302\n",
            "          56       8.7115       1.0668       0.9046       1.0709       0.9868       0.9437     488.2417\n",
            "          57       8.7127       1.0655       0.9063       1.0707       0.9849       0.9436     496.9544\n",
            "          58       8.7144       1.0640       0.9070       1.0699       0.9826       0.9463     505.6688\n",
            "          59       8.7135       1.0577       0.9097       1.0704       0.9831       0.9482     514.3823\n",
            "          60       8.7109       1.0545       0.9140       1.0703       0.9823       0.9468     523.0932\n",
            "          61       8.7120       1.0516       0.9141       1.0702       0.9824       0.9460     531.8052\n",
            "          62       8.7120       1.0489       0.9144       1.0700       0.9803       0.9482     540.5172\n",
            "          63       8.7131       1.0483       0.9145       1.0701       0.9805       0.9469     549.2303\n",
            "          64       8.7122       1.0422       0.9181       1.0705       0.9788       0.9475     557.9425\n",
            "          65       8.7104       1.0387       0.9199       1.0705       0.9784       0.9480     566.6529\n",
            "          66       8.7129       1.0377       0.9218       1.0704       0.9781       0.9472     575.3659\n",
            "          67       8.7111       1.0363       0.9217       1.0709       0.9760       0.9485     584.0770\n",
            "          68       8.7115       1.0263       0.9275       1.0698       0.9726       0.9511     592.7885\n",
            "          69       8.7117       1.0249       0.9284       1.0700       0.9723       0.9510     601.5002\n",
            "          70       8.7135       1.0203       0.9290       1.0706       0.9701       0.9522     610.2137\n",
            "          71       8.7107       1.0206       0.9289       1.0706       0.9693       0.9520     618.9244\n",
            "          72       8.7131       1.0170       0.9317       1.0700       0.9705       0.9518     627.6375\n",
            "          73       8.7100       1.0074       0.9373       1.0703       0.9696       0.9516     636.3475\n",
            "          74       8.7131       1.0078       0.9378       1.0698       0.9696       0.9514     645.0606\n",
            "          75       8.7154       1.0008       0.9404       1.0703       0.9675       0.9538     653.7760\n",
            "          76       8.7118       0.9986       0.9417       1.0704       0.9667       0.9527     662.4878\n",
            "          77       8.7118       0.9942       0.9439       1.0702       0.9654       0.9532     671.1997\n",
            "          78       8.7125       0.9856       0.9498       1.0704       0.9639       0.9540     679.9122\n",
            "          79       8.7134       0.9810       0.9515       1.0706       0.9628       0.9535     688.6256\n",
            "          80       8.7129       0.9776       0.9530       1.0703       0.9615       0.9551     697.3385\n",
            "          81       8.7134       0.9746       0.9549       1.0708       0.9604       0.9549     706.0519\n",
            "          82       8.7119       0.9698       0.9568       1.0704       0.9601       0.9558     714.7638\n",
            "          83       8.7111       0.9589       0.9632       1.0701       0.9586       0.9564     723.4749\n",
            "          84       8.7143       0.9583       0.9634       1.0701       0.9564       0.9572     732.1892\n",
            "          85       8.7138       0.9499       0.9684       1.0706       0.9558       0.9575     740.9029\n",
            "          86       8.7138       0.9442       0.9707       1.0703       0.9549       0.9573     749.6167\n",
            "          87       8.7130       0.9374       0.9742       1.0707       0.9546       0.9576     758.3297\n",
            "          88       8.7140       0.9318       0.9765       1.0708       0.9540       0.9579     767.0436\n",
            "          89       8.7134       0.9274       0.9799       1.0708       0.9528       0.9579     775.7570\n",
            "          90       8.7143       0.9244       0.9808       1.0705       0.9512       0.9579     784.4714\n",
            "          91       8.7122       0.9256       0.9800       1.0709       0.9524       0.9578     793.1836\n",
            "          92       8.7127       0.9244       0.9812       1.0707       0.9527       0.9585     801.8963\n",
            "          93       8.7138       0.9263       0.9800       1.0702       0.9531       0.9572     810.6101\n",
            "          94       8.7146       0.9264       0.9804       1.0711       0.9523       0.9580     819.3247\n",
            "          95       8.7133       0.9290       0.9786       1.0703       0.9521       0.9584     828.0380\n",
            "          96       8.7138       0.9294       0.9784       1.0700       0.9515       0.9584     836.7519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS3N6wrSpOzA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rrMUz4ycJv9"
      },
      "source": [
        "# save model and logs #\n",
        "\n",
        "PATH = \"model_3layers_final.pt\"\n",
        "torch.save(model3l, PATH)\n",
        "\n",
        "import pandas as pd\n",
        "logs3l.to_csv('logs3l.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqB9NrERdKVn"
      },
      "source": [
        "## 4-layer net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ-K5NHzdPzN"
      },
      "source": [
        "# Network 4 layers definition #\n",
        "\n",
        "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), conv_bn=conv_bn_default, prep=conv_bn_default):\n",
        "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512, 'layer4':1024}\n",
        "    n = {\n",
        "        'input': (None, []),\n",
        "        'prep': prep(3, channels['prep']),\n",
        "        'layer1': conv_bn(channels['prep'], channels['layer1'], pool=pool),\n",
        "        'layer2': conv_bn(channels['layer1'], channels['layer2'], pool=pool),\n",
        "        'layer3': conv_bn(channels['layer2'], channels['layer3'], pool=pool),\n",
        "        'layer4': conv_bn(channels['layer3'], channels['layer4'], pool=pool),\n",
        "        'pool': nn.MaxPool2d(2),\n",
        "        'flatten': Flatten(),\n",
        "        'linear': nn.Linear(channels['layer4'], 10, bias=False),\n",
        "        'logits': Mul(weight),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        n[layer]['residual'] = residual(channels[layer], conv_bn)\n",
        "    for layer in extra_layers:\n",
        "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uhox8rPdqmt",
        "outputId": "556ddbaf-3c56-414a-c9db-d078d52fac85"
      },
      "source": [
        "seed(123)\n",
        "\n",
        "model4l, logs4l = main(epochs=96, ema_epochs = 8, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.067 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0032 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1      10.3312       2.1068       0.2947       1.2658       2.3003       0.1003      10.4018\n",
            "           2      10.1720       1.5699       0.6159       1.2642       2.1689       0.2112      20.5738\n",
            "           3      10.1702       1.3448       0.7472       1.2657       1.4780       0.7104      30.7440\n",
            "           4      10.1709       1.2582       0.7953       1.2651       1.1730       0.8535      40.9149\n",
            "           5      10.1714       1.2128       0.8206       1.2653       1.1025       0.8810      51.0863\n",
            "           6      10.1716       1.1822       0.8357       1.2648       1.0720       0.8952      61.2580\n",
            "           7      10.1726       1.1654       0.8459       1.2657       1.0560       0.9005      71.4305\n",
            "           8      10.1727       1.1525       0.8520       1.2651       1.0380       0.9109      81.6032\n",
            "           9      10.1680       1.1471       0.8542       1.2648       1.0277       0.9144      91.7712\n",
            "          10      10.1710       1.1424       0.8565       1.2646       1.0229       0.9191     101.9423\n",
            "          11      10.1679       1.1404       0.8579       1.2648       1.0192       0.9202     112.1102\n",
            "          12      10.1682       1.1412       0.8571       1.2643       1.0165       0.9235     122.2784\n",
            "          13      10.1689       1.1438       0.8554       1.2643       1.0160       0.9232     132.4474\n",
            "          14      10.1689       1.1452       0.8543       1.2638       1.0162       0.9236     142.6162\n",
            "          15      10.1685       1.1489       0.8529       1.2639       1.0128       0.9252     152.7847\n",
            "          16      10.1675       1.1547       0.8500       1.2636       1.0167       0.9234     162.9522\n",
            "          17      10.1680       1.1570       0.8493       1.2645       1.0195       0.9218     173.1201\n",
            "          18      10.1660       1.1638       0.8448       1.2637       1.0269       0.9167     183.2861\n",
            "          19      10.1660       1.1675       0.8433       1.2638       1.0231       0.9176     193.4522\n",
            "          20      10.1633       1.1701       0.8409       1.2635       1.0300       0.9137     203.6155\n",
            "          21      10.1660       1.1681       0.8435       1.2634       1.0238       0.9181     213.7815\n",
            "          22      10.1646       1.1649       0.8444       1.2635       1.0200       0.9195     223.9460\n",
            "          23      10.1635       1.1661       0.8452       1.2633       1.0185       0.9203     234.1096\n",
            "          24      10.1662       1.1586       0.8482       1.2636       1.0180       0.9219     244.2758\n",
            "          25      10.1650       1.1570       0.8493       1.2634       1.0182       0.9222     254.4408\n",
            "          26      10.1655       1.1537       0.8509       1.2636       1.0157       0.9221     264.6063\n",
            "          27      10.1644       1.1512       0.8528       1.2633       1.0148       0.9247     274.7707\n",
            "          28      10.1658       1.1485       0.8542       1.2634       1.0152       0.9233     284.9365\n",
            "          29      10.1650       1.1489       0.8532       1.2640       1.0129       0.9247     295.1015\n",
            "          30      10.1647       1.1401       0.8580       1.2637       1.0116       0.9236     305.2662\n",
            "          31      10.1673       1.1395       0.8584       1.2632       1.0085       0.9267     315.4335\n",
            "          32      10.1656       1.1367       0.8599       1.2634       1.0058       0.9296     325.5991\n",
            "          33      10.1636       1.1359       0.8605       1.2634       1.0058       0.9261     335.7627\n",
            "          34      10.1661       1.1308       0.8637       1.2632       1.0069       0.9262     345.9288\n",
            "          35      10.1656       1.1289       0.8644       1.2632       1.0018       0.9278     356.0943\n",
            "          36      10.1662       1.1287       0.8648       1.2634       1.0008       0.9296     366.2605\n",
            "          37      10.1645       1.1262       0.8656       1.2637       1.0032       0.9283     376.4250\n",
            "          38      10.1646       1.1228       0.8683       1.2637       0.9992       0.9294     386.5897\n",
            "          39      10.1650       1.1208       0.8690       1.2639       1.0005       0.9297     396.7547\n",
            "          40      10.1643       1.1162       0.8719       1.2636       1.0002       0.9303     406.9190\n",
            "          41      10.1642       1.1155       0.8701       1.2629       0.9977       0.9303     417.0832\n",
            "          42      10.1629       1.1104       0.8747       1.2631       0.9948       0.9327     427.2461\n",
            "          43      10.1650       1.1082       0.8746       1.2632       0.9965       0.9330     437.4111\n",
            "          44      10.1642       1.1046       0.8770       1.2631       0.9929       0.9331     447.5754\n",
            "          45      10.1652       1.1054       0.8780       1.2633       0.9952       0.9339     457.7406\n",
            "          46      10.1647       1.1033       0.8773       1.2632       0.9903       0.9351     467.9053\n",
            "          47      10.1644       1.0981       0.8819       1.2634       0.9897       0.9358     478.0697\n",
            "          48      10.1659       1.0975       0.8813       1.2635       0.9886       0.9366     488.2357\n",
            "          49      10.1645       1.0912       0.8852       1.2632       0.9884       0.9357     498.4002\n",
            "          50      10.1639       1.0929       0.8835       1.2636       0.9864       0.9382     508.5641\n",
            "          51      10.1651       1.0880       0.8876       1.2631       0.9858       0.9378     518.7292\n",
            "          52      10.1631       1.0864       0.8863       1.2632       0.9851       0.9374     528.8923\n",
            "          53      10.1648       1.0843       0.8866       1.2633       0.9829       0.9392     539.0571\n",
            "          54      10.1662       1.0807       0.8914       1.2636       0.9825       0.9399     549.2233\n",
            "          55      10.1662       1.0744       0.8931       1.2631       0.9811       0.9394     559.3894\n",
            "          56      10.1653       1.0737       0.8939       1.2633       0.9815       0.9415     569.5547\n",
            "          57      10.1629       1.0738       0.8936       1.2632       0.9799       0.9406     579.7176\n",
            "          58      10.1649       1.0718       0.8948       1.2630       0.9800       0.9409     589.8825\n",
            "          59      10.1617       1.0638       0.8999       1.2632       0.9775       0.9439     600.0443\n",
            "          60      10.1647       1.0630       0.9000       1.2628       0.9765       0.9428     610.2090\n",
            "          61      10.1637       1.0575       0.9020       1.2631       0.9738       0.9449     620.3726\n",
            "          62      10.1651       1.0589       0.9002       1.2631       0.9729       0.9438     630.5378\n",
            "          63      10.1645       1.0512       0.9062       1.2631       0.9702       0.9481     640.7023\n",
            "          64      10.1644       1.0488       0.9078       1.2637       0.9712       0.9468     650.8666\n",
            "          65      10.1665       1.0465       0.9101       1.2633       0.9705       0.9464     661.0332\n",
            "          66      10.1636       1.0401       0.9127       1.2630       0.9710       0.9467     671.1968\n",
            "          67      10.1638       1.0394       0.9119       1.2636       0.9674       0.9474     681.3606\n",
            "          68      10.1651       1.0365       0.9143       1.2640       0.9673       0.9475     691.5257\n",
            "          69      10.1655       1.0315       0.9163       1.2634       0.9659       0.9482     701.6912\n",
            "          70      10.1650       1.0263       0.9184       1.2633       0.9644       0.9499     711.8562\n",
            "          71      10.1649       1.0245       0.9214       1.2638       0.9637       0.9499     722.0211\n",
            "          72      10.1668       1.0175       0.9248       1.2632       0.9619       0.9514     732.1879\n",
            "          73      10.1663       1.0153       0.9262       1.2636       0.9611       0.9520     742.3542\n",
            "          74      10.1664       1.0119       0.9275       1.2631       0.9613       0.9513     752.5205\n",
            "          75      10.1660       1.0047       0.9312       1.2636       0.9592       0.9539     762.6866\n",
            "          76      10.1661       1.0018       0.9332       1.2641       0.9593       0.9535     772.8527\n",
            "          77      10.1681       0.9953       0.9359       1.2632       0.9591       0.9518     783.0207\n",
            "          78      10.1682       0.9880       0.9410       1.2636       0.9591       0.9525     793.1889\n",
            "          79      10.1686       0.9837       0.9429       1.2632       0.9586       0.9527     803.3575\n",
            "          80      10.1661       0.9778       0.9465       1.2633       0.9571       0.9534     813.5237\n",
            "          81      10.1678       0.9748       0.9483       1.2637       0.9567       0.9533     823.6915\n",
            "          82      10.1671       0.9723       0.9491       1.2639       0.9553       0.9540     833.8586\n",
            "          83      10.1671       0.9645       0.9536       1.2638       0.9548       0.9530     844.0257\n",
            "          84      10.1678       0.9573       0.9579       1.2643       0.9541       0.9549     854.1935\n",
            "          85      10.1683       0.9518       0.9601       1.2640       0.9528       0.9547     864.3618\n",
            "          86      10.1669       0.9476       0.9641       1.2633       0.9522       0.9563     874.5287\n",
            "          87      10.1664       0.9379       0.9674       1.2638       0.9536       0.9543     884.6951\n",
            "          88      10.1663       0.9308       0.9724       1.2637       0.9528       0.9551     894.8613\n",
            "          89      10.2070       0.9271       0.9744       1.2631       0.9501       0.9560     905.0683\n",
            "          90      10.1667       0.9241       0.9755       1.2636       0.9491       0.9565     915.2350\n",
            "          91      10.1671       0.9243       0.9754       1.2638       0.9495       0.9557     925.4021\n",
            "          92      10.1682       0.9249       0.9755       1.2642       0.9483       0.9570     935.5703\n",
            "          93      10.1645       0.9269       0.9741       1.2640       0.9477       0.9565     945.7347\n",
            "          94      10.1668       0.9282       0.9737       1.2640       0.9476       0.9584     955.9015\n",
            "          95      10.1661       0.9290       0.9734       1.2635       0.9473       0.9596     966.0676\n",
            "          96      10.1643       0.9264       0.9743       1.2640       0.9467       0.9584     976.2320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAUGTyMcdyaW"
      },
      "source": [
        "# save model and logs #\n",
        "\n",
        "PATH2 = \"model_4layers_final.pt\"\n",
        "torch.save(model4l, PATH2)\n",
        "\n",
        "import pandas as pd\n",
        "logs4l.to_csv('logs4l.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cMFxQs_d8Ue"
      },
      "source": [
        "## 5 - layer net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qCKxI1Md7MM"
      },
      "source": [
        "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3', 'layer5'), conv_bn=conv_bn_default, prep=conv_bn_default):\n",
        "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512, 'layer4':1024, 'layer5': 2048}\n",
        "    n = {\n",
        "        'input': (None, []),\n",
        "        'prep': prep(3, channels['prep']),\n",
        "        'layer1': conv_bn(channels['prep'], channels['layer1'], pool=pool),\n",
        "        'layer2': conv_bn(channels['layer1'], channels['layer2'], pool=pool),\n",
        "        'layer3': conv_bn(channels['layer2'], channels['layer3'], pool=pool),\n",
        "        'layer4': conv_bn(channels['layer3'], channels['layer4'], pool=pool),\n",
        "        'layer5': conv_bn(channels['layer4'], channels['layer5'], pool=pool),\n",
        "        'pool': nn.MaxPool2d(1),\n",
        "        'flatten': Flatten(),\n",
        "        'linear': nn.Linear(channels['layer5'], 10, bias=False),\n",
        "        'logits': Mul(weight),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        n[layer]['residual'] = residual(channels[layer], conv_bn)\n",
        "    for layer in extra_layers:\n",
        "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer])       \n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu94a7mOeN_r",
        "outputId": "914f852b-bd78-4796-cded-dc9e0e16597f"
      },
      "source": [
        "\n",
        "seed(123)\n",
        "\n",
        "model5l, logs5l = main(epochs=96, ema_epochs = 8, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 0.21 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.0028 seconds\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1      21.5490       1.9691       0.3635       2.6069       2.3025       0.1105      21.7624\n",
            "           2      20.7445       1.4561       0.6729       2.6055       2.2063       0.2104      42.5069\n",
            "           3      20.7399       1.3141       0.7580       2.6055       1.4216       0.7661      63.2468\n",
            "           4      20.7434       1.2496       0.7920       2.6059       1.1419       0.8600      83.9902\n",
            "           5      20.7418       1.2113       0.8143       2.6057       1.0817       0.8846     104.7319\n",
            "           6      20.7400       1.1881       0.8275       2.6065       1.0590       0.8967     125.4720\n",
            "           7      20.7431       1.1723       0.8367       2.6060       1.0447       0.9052     146.2150\n",
            "           8      20.7391       1.1666       0.8405       2.6055       1.0357       0.9098     166.9541\n",
            "           9      20.7396       1.1587       0.8442       2.6059       1.0311       0.9126     187.6937\n",
            "          10      20.7393       1.1561       0.8463       2.6058       1.0267       0.9157     208.4331\n",
            "          11      20.7377       1.1611       0.8421       2.6052       1.0253       0.9161     229.1708\n",
            "          12      20.7380       1.1596       0.8429       2.6058       1.0224       0.9170     249.9088\n",
            "          13      20.7391       1.1632       0.8407       2.6058       1.0182       0.9219     270.6479\n",
            "          14      20.7391       1.1645       0.8436       2.6051       1.0196       0.9205     291.3871\n",
            "          15      20.7387       1.1711       0.8384       2.6055       1.0230       0.9184     312.1257\n",
            "          16      20.7388       1.1717       0.8375       2.6055       1.0220       0.9189     332.8645\n",
            "          17      20.7368       1.1766       0.8355       2.6047       1.0233       0.9206     353.6013\n",
            "          18      20.7380       1.1866       0.8313       2.6052       1.0255       0.9171     374.3392\n",
            "          19      20.7361       1.1891       0.8304       2.6047       1.0311       0.9151     395.0754\n",
            "          20      20.7359       1.1882       0.8300       2.6049       1.0333       0.9128     415.8112\n",
            "          21      20.7385       1.1855       0.8329       2.6045       1.0318       0.9146     436.5498\n",
            "          22      20.7352       1.1852       0.8303       2.6046       1.0284       0.9168     457.2849\n",
            "          23      20.7368       1.1843       0.8316       2.6051       1.0321       0.9152     478.0217\n",
            "          24      20.7363       1.1777       0.8347       2.6041       1.0261       0.9166     498.7581\n",
            "          25      20.7345       1.1790       0.8351       2.6050       1.0273       0.9195     519.4926\n",
            "          26      20.7355       1.1729       0.8382       2.6048       1.0258       0.9196     540.2281\n",
            "          27      20.7354       1.1719       0.8380       2.6045       1.0217       0.9220     560.9636\n",
            "          28      20.7359       1.1722       0.8383       2.6044       1.0212       0.9203     581.6995\n",
            "          29      20.7393       1.1644       0.8422       2.6049       1.0218       0.9230     602.4388\n",
            "          30      20.7330       1.1661       0.8418       2.6049       1.0181       0.9200     623.1718\n",
            "          31      20.7341       1.1587       0.8467       2.6047       1.0180       0.9214     643.9059\n",
            "          32      20.7359       1.1595       0.8459       2.6044       1.0167       0.9219     664.6417\n",
            "          33      20.7347       1.1565       0.8476       2.6036       1.0170       0.9222     685.3764\n",
            "          34      20.7363       1.1563       0.8462       2.6045       1.0177       0.9226     706.1127\n",
            "          35      20.7365       1.1510       0.8487       2.6043       1.0156       0.9253     726.8492\n",
            "          36      20.7346       1.1525       0.8478       2.6047       1.0148       0.9242     747.5838\n",
            "          37      20.7367       1.1430       0.8551       2.6042       1.0139       0.9243     768.3205\n",
            "          38      20.7361       1.1413       0.8553       2.6050       1.0097       0.9244     789.0566\n",
            "          39      20.7359       1.1401       0.8557       2.6048       1.0103       0.9261     809.7925\n",
            "          40      20.7344       1.1377       0.8565       2.6042       1.0114       0.9258     830.5269\n",
            "          41      20.7361       1.1371       0.8574       2.6038       1.0094       0.9251     851.2630\n",
            "          42      20.7359       1.1312       0.8604       2.6047       1.0061       0.9278     871.9989\n",
            "          43      20.7348       1.1261       0.8634       2.6047       1.0037       0.9289     892.7337\n",
            "          44      20.7338       1.1283       0.8636       2.6042       1.0017       0.9313     913.4675\n",
            "          45      20.7349       1.1234       0.8659       2.6042       1.0010       0.9297     934.2024\n",
            "          46      20.7330       1.1246       0.8632       2.6047       1.0027       0.9283     954.9353\n",
            "          47      20.7356       1.1197       0.8667       2.6038       1.0024       0.9281     975.6710\n",
            "          48      20.7357       1.1138       0.8687       2.6045       0.9992       0.9321     996.4066\n",
            "          49      20.7322       1.1130       0.8697       2.6040       0.9957       0.9343    1017.1388\n",
            "          50      20.7361       1.1086       0.8732       2.6043       0.9975       0.9325    1037.8749\n",
            "          51      20.7353       1.1082       0.8732       2.6048       0.9945       0.9335    1058.6102\n",
            "          52      20.7358       1.1056       0.8751       2.6046       0.9945       0.9332    1079.3460\n",
            "          53      20.7345       1.1025       0.8760       2.6049       0.9919       0.9346    1100.0804\n",
            "          54      20.7364       1.0970       0.8797       2.6045       0.9880       0.9369    1120.8168\n",
            "          55      20.7339       1.0928       0.8821       2.6054       0.9884       0.9384    1141.5508\n",
            "          56      20.7348       1.0962       0.8796       2.6042       0.9891       0.9376    1162.2855\n",
            "          57      20.7331       1.0896       0.8830       2.6046       0.9873       0.9363    1183.0186\n",
            "          58      20.7371       1.0871       0.8831       2.6050       0.9862       0.9386    1203.7557\n",
            "          59      20.7332       1.0816       0.8878       2.6046       0.9837       0.9382    1224.4889\n",
            "          60      20.7348       1.0775       0.8889       2.6049       0.9835       0.9391    1245.2237\n",
            "          61      20.7332       1.0752       0.8915       2.6044       0.9797       0.9412    1265.9569\n",
            "          62      20.7342       1.0722       0.8921       2.6046       0.9793       0.9423    1286.6912\n",
            "          63      20.7357       1.0723       0.8915       2.6046       0.9803       0.9405    1307.4269\n",
            "          64      20.7353       1.0678       0.8944       2.6051       0.9779       0.9431    1328.1622\n",
            "          65      20.7343       1.0602       0.8990       2.6047       0.9763       0.9440    1348.8966\n",
            "          66      20.7351       1.0568       0.9005       2.6052       0.9748       0.9442    1369.6316\n",
            "          67      20.7363       1.0532       0.9024       2.6048       0.9749       0.9441    1390.3679\n",
            "          68      20.7353       1.0483       0.9039       2.6047       0.9756       0.9430    1411.1032\n",
            "          69      20.7379       1.0470       0.9062       2.6048       0.9752       0.9444    1431.8411\n",
            "          70      20.7356       1.0398       0.9098       2.6048       0.9734       0.9443    1452.5767\n",
            "          71      20.7357       1.0409       0.9095       2.6049       0.9721       0.9440    1473.3124\n",
            "          72      20.7357       1.0320       0.9137       2.6040       0.9698       0.9457    1494.0481\n",
            "          73      20.7362       1.0317       0.9138       2.6048       0.9691       0.9479    1514.7843\n",
            "          74      20.7354       1.0252       0.9181       2.6043       0.9679       0.9471    1535.5197\n",
            "          75      20.7357       1.0169       0.9218       2.6046       0.9664       0.9480    1556.2555\n",
            "          76      20.7375       1.0181       0.9226       2.6049       0.9663       0.9470    1576.9929\n",
            "          77      20.7365       1.0104       0.9253       2.6047       0.9642       0.9470    1597.7295\n",
            "          78      20.7370       1.0051       0.9291       2.6053       0.9631       0.9481    1618.4665\n",
            "          79      20.7347       0.9982       0.9322       2.6050       0.9621       0.9498    1639.2012\n",
            "          80      20.7377       0.9933       0.9347       2.6050       0.9615       0.9511    1659.9388\n",
            "          81      20.7394       0.9861       0.9391       2.6046       0.9613       0.9498    1680.6783\n",
            "          82      20.7371       0.9827       0.9411       2.6053       0.9607       0.9507    1701.4154\n",
            "          83      20.7382       0.9737       0.9460       2.6047       0.9589       0.9516    1722.1536\n",
            "          84      20.7327       0.9695       0.9484       2.6050       0.9577       0.9513    1742.8863\n",
            "          85      20.7346       0.9624       0.9520       2.6043       0.9573       0.9515    1763.6209\n",
            "          86      20.7373       0.9576       0.9542       2.6052       0.9565       0.9525    1784.3582\n",
            "          87      20.7370       0.9487       0.9585       2.6049       0.9563       0.9534    1805.0952\n",
            "          88      20.7336       0.9400       0.9645       2.6042       0.9532       0.9552    1825.8288\n",
            "          89      20.7352       0.9372       0.9650       2.6052       0.9540       0.9546    1846.5639\n",
            "          90      20.7367       0.9336       0.9676       2.6057       0.9541       0.9525    1867.3007\n",
            "          91      20.7358       0.9354       0.9663       2.6049       0.9532       0.9547    1888.0365\n",
            "          92      20.7386       0.9349       0.9668       2.6049       0.9528       0.9554    1908.7750\n",
            "          93      20.7357       0.9347       0.9670       2.6049       0.9538       0.9531    1929.5108\n",
            "          94      20.7375       0.9361       0.9663       2.6053       0.9543       0.9524    1950.2483\n",
            "          95      20.7364       0.9383       0.9646       2.6050       0.9552       0.9522    1970.9847\n",
            "          96      20.7356       0.9369       0.9654       2.6053       0.9536       0.9531    1991.7203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0j6g4z9eUeu"
      },
      "source": [
        "# save model and logs #\n",
        "\n",
        "PATH3 = \"model_5layers_final.pt\"\n",
        "torch.save(model5l, PATH3)\n",
        "\n",
        "import pandas as pd\n",
        "logs5l.to_csv('logs5l.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyqBwCSSt6Vf"
      },
      "source": [
        "# Download the test set directly from kaggle #\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpbgv6qiuHE7"
      },
      "source": [
        "# install kaggle\n",
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "uxnpOZX0uNrC",
        "outputId": "6daca8fc-bad0-42e2-d926-e5e7b7dd4c81"
      },
      "source": [
        "from google.colab import files \n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26632066-02bd-447b-95eb-28f1da29cb78\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26632066-02bd-447b-95eb-28f1da29cb78\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ubialonczyk\",\"key\":\"2d69e330ccc25265ef376b6352b59782\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADtXlKGWulT5"
      },
      "source": [
        "# create a kaggle folder\n",
        "\n",
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7AF9WNYvGTM"
      },
      "source": [
        "# copy the kaggle.json to the created folder\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKTta3KlvTNo"
      },
      "source": [
        "# Permission for the json to act \n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8M6rsmXvtcf",
        "outputId": "5c9edf37-6a5f-46d3-ed86-6b5fff0b6751"
      },
      "source": [
        "# which datasets are available\n",
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              223KB  2021-04-19 17:31:00           3123  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-02-17 16:10:44           1833  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   1GB  2021-02-18 10:08:27           1236  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52            354  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19            629  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04            162  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23            268  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39             66  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37            392  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54             93  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00            425  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51             55  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            173  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26             32  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40             43  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54             43  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         136125  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         133641  \n",
            "rtatman/188-million-us-wildfires                            1.88 Million US Wildfires                         168MB  2020-05-12 21:03:49          14769  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          16792  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5gGeK5ev5Ed",
        "outputId": "116d0b37-416c-4ba0-ff58-a48c27b4e323"
      },
      "source": [
        "! kaggle competitions download -c cifar-10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading sampleSubmission.csv.zip to /content\n",
            "  0% 0.00/655k [00:00<?, ?B/s]\n",
            "100% 655k/655k [00:00<00:00, 91.0MB/s]\n",
            "Downloading test.7z to /content\n",
            " 99% 604M/610M [00:06<00:00, 118MB/s]\n",
            "100% 610M/610M [00:06<00:00, 97.4MB/s]\n",
            "Downloading train.7z to /content\n",
            " 75% 78.0M/105M [00:00<00:00, 61.0MB/s]\n",
            "100% 105M/105M [00:01<00:00, 108MB/s]  \n",
            "Downloading trainLabels.csv to /content\n",
            "  0% 0.00/575k [00:00<?, ?B/s]\n",
            "100% 575k/575k [00:00<00:00, 192MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "wuQxmttNdUjC",
        "outputId": "f7af2dd4-273d-46e2-e087-954b86d0e6af"
      },
      "source": [
        "test_set = np.load('test_tensor8.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-5957e664f8d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_tensor8.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 40894336 into shape (300000,32,32,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im-QPiUupQs8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju33nwJ1pRE0"
      },
      "source": [
        "## PREDYKCJE ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuE3JUgQpTNP"
      },
      "source": [
        "img_all=np.load(\"drive/MyDrive/Kopia test_tensor8.npy\")\n",
        "img_all=img_all.astype(\"int16\")\n",
        "img_all=np.mod(img_all,256)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6UHF_W3pgTL",
        "outputId": "79a454ae-9c19-4c69-b0ce-984951334aed"
      },
      "source": [
        "data = {'data': img_all, 'targets': torch.LongTensor(np.random.randint(0,300000)).cuda()}\n",
        "dataset = map_nested(torch.tensor, data)\n",
        "dataset = map_nested(to(device), dataset)\n",
        "T = lambda x: torch.tensor(x, dtype=torch.float16, device=device)\n",
        "transforms = [to(dtype=torch.float16),partial(normalise, mean=T(cifar10_mean), std=T(cifar10_std)), partial(transpose, source='NHWC', target='NCHW'), ]\n",
        "print('Preprocessing test data')\n",
        "test_set = preprocess(dataset, transforms)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQewGi_MwfBx",
        "outputId": "44a53ce3-9c33-489c-d874-deb1e15db5b2"
      },
      "source": [
        "test_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': tensor([[[[-1.1006e+00, -6.8481e-02, -4.9591e-03,  ..., -1.0684e+00,\n",
              "            -1.2109e+00, -1.2744e+00],\n",
              "           [-9.4141e-01,  1.0910e-02,  7.4402e-02,  ...,  1.6968e-01,\n",
              "             1.5381e-01,  1.3794e-01],\n",
              "           [-8.6230e-01,  1.0910e-02,  1.2201e-01,  ...,  2.9663e-01,\n",
              "             2.0142e-01,  2.6779e-02],\n",
              "           ...,\n",
              "           [-4.4946e-01, -4.3359e-01, -4.0186e-01,  ...,  1.8047e+00,\n",
              "             1.7891e+00, -2.7490e-01],\n",
              "           [-3.7012e-01, -3.2251e-01, -3.3838e-01,  ...,  1.7891e+00,\n",
              "             1.8047e+00, -4.4946e-01],\n",
              "           [-1.0205e+00, -1.0205e+00, -1.0205e+00,  ...,  1.8203e+00,\n",
              "             1.8359e+00, -5.9229e-01]],\n",
              " \n",
              "          [[-1.1104e+00, -7.9529e-02, -1.5099e-02,  ..., -1.1426e+00,\n",
              "            -1.2549e+00, -1.2871e+00],\n",
              "           [-9.9756e-01, -3.1204e-02,  3.3203e-02,  ...,  9.7656e-02,\n",
              "             8.1543e-02,  8.1543e-02],\n",
              "           [-9.0088e-01, -1.5099e-02,  9.7656e-02,  ...,  1.7810e-01,\n",
              "             8.1543e-02, -6.3416e-02],\n",
              "           ...,\n",
              "           [-4.8218e-01, -4.4995e-01, -4.3384e-01,  ...,  1.8213e+00,\n",
              "             1.8047e+00, -2.8882e-01],\n",
              "           [-3.8550e-01, -3.3716e-01, -3.3716e-01,  ...,  1.8213e+00,\n",
              "             1.8369e+00, -4.4995e-01],\n",
              "           [-1.0459e+00, -1.0293e+00, -1.0459e+00,  ...,  1.8535e+00,\n",
              "             1.8691e+00, -5.7861e-01]],\n",
              " \n",
              "          [[-1.0029e+00, -4.3121e-02,  1.8740e-03,  ..., -1.1230e+00,\n",
              "            -1.2578e+00, -1.3174e+00],\n",
              "           [-8.8281e-01,  1.8740e-03,  6.1859e-02,  ...,  4.6875e-02,\n",
              "            -1.3123e-02,  1.8740e-03],\n",
              "           [-7.9297e-01,  1.6876e-02,  1.0681e-01,  ...,  1.0681e-01,\n",
              "             1.8740e-03, -1.1810e-01],\n",
              "           ...,\n",
              "           [-3.5791e-01, -3.4302e-01, -3.1299e-01,  ...,  1.7861e+00,\n",
              "             1.7715e+00, -1.7810e-01],\n",
              "           [-2.8296e-01, -2.2302e-01, -2.3804e-01,  ...,  1.7715e+00,\n",
              "             1.7861e+00, -3.2812e-01],\n",
              "           [-8.8281e-01, -8.8281e-01, -8.9795e-01,  ...,  1.8018e+00,\n",
              "             1.8311e+00, -4.6289e-01]]],\n",
              " \n",
              " \n",
              "         [[[-1.0371e+00, -1.0840e+00, -1.1318e+00,  ..., -5.2881e-01,\n",
              "            -5.1270e-01, -5.7617e-01],\n",
              "           [-1.0371e+00, -1.0684e+00, -1.1162e+00,  ..., -9.5752e-01,\n",
              "            -9.4141e-01, -9.4141e-01],\n",
              "           [-1.0684e+00, -1.0840e+00, -1.1318e+00,  ..., -1.0527e+00,\n",
              "            -8.9404e-01, -9.7314e-01],\n",
              "           ...,\n",
              "           [ 1.2168e+00,  1.0742e+00,  1.1221e+00,  ..., -3.7012e-01,\n",
              "            -3.2251e-01, -4.3359e-01],\n",
              "           [ 3.9185e-01,  5.9814e-01,  9.7900e-01,  ..., -3.7012e-01,\n",
              "            -3.7012e-01, -4.8120e-01],\n",
              "           [-4.3359e-01, -2.1130e-01,  4.2358e-01,  ..., -4.0186e-01,\n",
              "            -4.1772e-01, -5.2881e-01]],\n",
              " \n",
              "          [[-1.0781e+00, -1.1104e+00, -1.1426e+00,  ..., -4.9829e-01,\n",
              "            -4.8218e-01, -5.4639e-01],\n",
              "           [-1.0781e+00, -1.1104e+00, -1.1260e+00,  ..., -9.3311e-01,\n",
              "            -9.1699e-01, -9.3311e-01],\n",
              "           [-1.0938e+00, -1.1104e+00, -1.1260e+00,  ..., -1.0293e+00,\n",
              "            -8.8477e-01, -9.3311e-01],\n",
              "           ...,\n",
              "           [ 9.6729e-01,  8.2227e-01,  9.3506e-01,  ..., -3.5327e-01,\n",
              "            -3.0493e-01, -4.0161e-01],\n",
              "           [ 2.2644e-01,  4.0356e-01,  7.2559e-01,  ..., -3.5327e-01,\n",
              "            -3.3716e-01, -4.6606e-01],\n",
              "           [-4.4995e-01, -2.7271e-01,  1.7810e-01,  ..., -3.8550e-01,\n",
              "            -4.0161e-01, -5.1416e-01]],\n",
              " \n",
              "          [[-9.1260e-01, -9.5801e-01, -9.8779e-01,  ..., -8.8074e-02,\n",
              "            -1.0309e-01, -1.9312e-01],\n",
              "           [-9.1260e-01, -9.4287e-01, -9.7266e-01,  ..., -6.2793e-01,\n",
              "            -6.2793e-01, -6.5771e-01],\n",
              "           [-9.2773e-01, -9.4287e-01, -9.7266e-01,  ..., -7.7783e-01,\n",
              "            -6.1279e-01, -6.8799e-01],\n",
              "           ...,\n",
              "           [ 4.2163e-01,  2.5684e-01,  4.8169e-01,  ..., -1.3306e-01,\n",
              "            -1.0309e-01, -2.0801e-01],\n",
              "           [ 1.0681e-01,  4.6875e-02,  2.2681e-01,  ..., -1.3306e-01,\n",
              "            -1.3306e-01, -2.6807e-01],\n",
              "           [-1.4807e-01, -1.3306e-01, -2.8122e-02,  ..., -1.9312e-01,\n",
              "            -2.0801e-01, -3.4302e-01]]],\n",
              " \n",
              " \n",
              "         [[[-1.7354e+00, -1.7031e+00, -1.7197e+00,  ..., -1.4814e+00,\n",
              "            -1.4014e+00, -1.9414e+00],\n",
              "           [-2.7490e-01, -6.0791e-01, -9.7314e-01,  ..., -1.2744e+00,\n",
              "            -1.2744e+00, -1.8779e+00],\n",
              "           [-2.7490e-01, -7.1924e-01, -1.0840e+00,  ..., -9.2578e-01,\n",
              "            -9.5752e-01, -1.7832e+00],\n",
              "           ...,\n",
              "           [-1.4336e+00,  1.2201e-01,  9.0271e-02,  ..., -5.7617e-01,\n",
              "            -1.9543e-01, -1.1609e-01],\n",
              "           [-1.4492e+00,  6.1426e-01,  5.1904e-01,  ..., -2.1130e-01,\n",
              "            -1.9543e-01, -1.9543e-01],\n",
              "           [-1.6553e+00,  2.3315e-01,  3.2837e-01,  ..., -1.6084e+00,\n",
              "            -1.7354e+00, -1.8301e+00]],\n",
              " \n",
              "          [[-1.7861e+00, -1.7549e+00, -1.7549e+00,  ..., -1.4004e+00,\n",
              "            -1.3193e+00, -1.9150e+00],\n",
              "           [-6.1084e-01, -8.6865e-01, -1.1426e+00,  ..., -1.2227e+00,\n",
              "            -1.2227e+00, -1.8672e+00],\n",
              "           [-5.7861e-01, -9.1699e-01, -1.1904e+00,  ..., -9.0088e-01,\n",
              "            -9.1699e-01, -1.7549e+00],\n",
              "           ...,\n",
              "           [-1.4482e+00,  4.9316e-02,  1.7105e-02,  ..., -6.2695e-01,\n",
              "            -2.7271e-01, -1.9226e-01],\n",
              "           [-1.4482e+00,  5.6445e-01,  4.5190e-01,  ..., -2.4060e-01,\n",
              "            -2.4060e-01, -2.5659e-01],\n",
              "           [-1.6582e+00,  1.7810e-01,  2.5879e-01,  ..., -1.6094e+00,\n",
              "            -1.7227e+00, -1.8350e+00]],\n",
              " \n",
              "          [[-1.6025e+00, -1.6025e+00, -1.6172e+00,  ..., -1.5430e+00,\n",
              "            -1.4531e+00, -1.6777e+00],\n",
              "           [-1.0029e+00, -1.1826e+00, -1.3623e+00,  ..., -1.4531e+00,\n",
              "            -1.4531e+00, -1.6777e+00],\n",
              "           [-9.8779e-01, -1.1973e+00, -1.3623e+00,  ..., -1.2275e+00,\n",
              "            -1.2725e+00, -1.6172e+00],\n",
              "           ...,\n",
              "           [-1.2578e+00,  3.1860e-02,  1.8740e-03,  ..., -5.2295e-01,\n",
              "            -2.3804e-01, -1.7810e-01],\n",
              "           [-1.2432e+00,  5.1172e-01,  4.0674e-01,  ..., -2.3804e-01,\n",
              "            -2.5293e-01, -2.5293e-01],\n",
              "           [-1.4375e+00,  1.5186e-01,  2.2681e-01,  ..., -1.3926e+00,\n",
              "            -1.4980e+00, -1.5879e+00]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[ 6.4600e-01,  4.2664e-02,  1.0614e-01,  ..., -1.7957e-01,\n",
              "            -6.0791e-01, -5.4443e-01],\n",
              "           [-1.9543e-01,  3.1250e-01,  3.7598e-01,  ..., -6.7139e-01,\n",
              "            -5.2881e-01, -3.2251e-01],\n",
              "           [-1.9543e-01,  3.9185e-01, -5.2582e-02,  ..., -2.1130e-01,\n",
              "            -3.6713e-02, -2.2717e-01],\n",
              "           ...,\n",
              "           [-1.0371e+00, -1.2432e+00, -9.0967e-01,  ..., -9.0967e-01,\n",
              "            -7.1924e-01, -9.5752e-01],\n",
              "           [-4.4946e-01, -1.1318e+00, -1.1162e+00,  ..., -1.0371e+00,\n",
              "            -9.8926e-01, -1.0527e+00],\n",
              "           [-1.1006e+00, -7.5098e-01, -9.2578e-01,  ..., -8.9404e-01,\n",
              "            -1.4336e+00, -8.6230e-01]],\n",
              " \n",
              "          [[-3.1204e-02,  1.1377e-01, -1.5099e-02,  ..., -1.9226e-01,\n",
              "            -2.0837e-01, -2.8882e-01],\n",
              "           [-3.1204e-02, -1.9226e-01,  6.2891e-01,  ..., -7.0752e-01,\n",
              "            -7.7197e-01, -9.5642e-02],\n",
              "           [ 1.4600e-01,  9.7656e-02,  3.3911e-01,  ..., -4.1772e-01,\n",
              "            -5.7861e-01, -6.2695e-01],\n",
              "           ...,\n",
              "           [-1.9226e-01, -1.4392e-01, -3.6938e-01,  ...,  1.1377e-01,\n",
              "             2.7490e-01, -1.4392e-01],\n",
              "           [ 1.2988e-01, -7.9529e-02, -2.0837e-01,  ..., -2.8882e-01,\n",
              "            -1.5099e-02, -4.7302e-02],\n",
              "           [-5.7861e-01, -6.4307e-01, -1.4392e-01,  ...,  1.0061e-03,\n",
              "            -3.6938e-01,  4.9316e-02]],\n",
              " \n",
              "          [[ 3.3179e-01, -1.0309e-01, -8.8074e-02,  ..., -2.0801e-01,\n",
              "            -7.0312e-01, -3.5791e-01],\n",
              "           [-1.3123e-02,  4.5166e-01,  1.0681e-01,  ..., -4.7803e-01,\n",
              "            -1.7810e-01, -7.1777e-01],\n",
              "           [ 3.4668e-01,  1.8740e-03, -2.2302e-01,  ..., -4.1797e-01,\n",
              "            -8.8074e-02, -5.5273e-01],\n",
              "           ...,\n",
              "           [ 7.3682e-01,  8.2666e-01,  1.5762e+00,  ...,  1.1113e+00,\n",
              "             1.2461e+00,  5.8691e-01],\n",
              "           [ 7.6660e-01,  7.2168e-01,  1.0068e+00,  ...,  8.4180e-01,\n",
              "             2.4182e-01,  5.2686e-01],\n",
              "           [ 8.4180e-01,  6.3184e-01,  9.6143e-01,  ...,  9.3164e-01,\n",
              "             9.1650e-01,  1.1270e+00]]],\n",
              " \n",
              " \n",
              "         [[[-5.6055e-01, -5.6055e-01, -7.8271e-01,  ..., -5.9229e-01,\n",
              "            -7.0361e-01, -7.6709e-01],\n",
              "           [-8.6230e-01, -4.0186e-01, -7.9883e-01,  ..., -1.0049e+00,\n",
              "            -4.9707e-01, -4.3359e-01],\n",
              "           [-7.3535e-01, -6.5576e-01, -7.3535e-01,  ..., -4.6533e-01,\n",
              "            -4.4946e-01, -1.6370e-01],\n",
              "           ...,\n",
              "           [-3.0664e-01, -4.3359e-01,  1.8555e-01,  ..., -1.2588e+00,\n",
              "            -9.2578e-01, -1.1797e+00],\n",
              "           [-6.8481e-02,  1.6968e-01, -2.0828e-02,  ..., -7.9883e-01,\n",
              "            -7.6709e-01, -1.0840e+00],\n",
              "           [-4.0186e-01, -3.5425e-01, -6.5576e-01,  ..., -1.1162e+00,\n",
              "            -8.3057e-01, -8.9404e-01]],\n",
              " \n",
              "          [[-4.8218e-01, -4.4995e-01, -4.4995e-01,  ..., -7.2363e-01,\n",
              "            -1.1104e+00, -1.0137e+00],\n",
              "           [-8.5254e-01, -5.6250e-01, -6.5918e-01,  ..., -9.6533e-01,\n",
              "            -9.0088e-01, -9.3311e-01],\n",
              "           [-9.6533e-01, -6.4307e-01, -5.7861e-01,  ..., -9.0088e-01,\n",
              "            -7.0752e-01, -9.1699e-01],\n",
              "           ...,\n",
              "           [-3.8550e-01, -6.7529e-01, -6.4307e-01,  ..., -1.2393e+00,\n",
              "            -6.7529e-01, -1.1104e+00],\n",
              "           [-9.1699e-01, -5.9473e-01, -1.1104e+00,  ..., -1.3516e+00,\n",
              "            -9.3311e-01, -8.8477e-01],\n",
              "           [-6.4307e-01, -9.4922e-01, -1.2393e+00,  ..., -3.3716e-01,\n",
              "            -6.1084e-01, -9.9756e-01]],\n",
              " \n",
              "          [[-4.0308e-01, -2.8122e-02, -2.2302e-01,  ..., -9.2773e-01,\n",
              "            -1.1230e+00, -1.0928e+00],\n",
              "           [-6.7285e-01, -3.8794e-01, -5.6787e-01,  ..., -5.2295e-01,\n",
              "            -5.3809e-01, -2.5293e-01],\n",
              "           [-6.1279e-01, -4.4800e-01, -8.2275e-01,  ..., -6.1279e-01,\n",
              "            -5.2295e-01, -5.2295e-01],\n",
              "           ...,\n",
              "           [-4.9292e-01, -7.4805e-01, -7.0312e-01,  ..., -6.8799e-01,\n",
              "            -9.8779e-01, -9.1260e-01],\n",
              "           [-5.0781e-01, -7.4805e-01, -7.9297e-01,  ..., -1.2432e+00,\n",
              "            -6.2793e-01, -9.8779e-01],\n",
              "           [-3.2812e-01, -6.5771e-01, -6.5771e-01,  ..., -4.7803e-01,\n",
              "            -2.0801e-01, -4.4800e-01]]],\n",
              " \n",
              " \n",
              "         [[[ 5.3467e-01,  5.5078e-01,  5.3467e-01,  ...,  5.3467e-01,\n",
              "             5.0293e-01,  5.0293e-01],\n",
              "           [ 5.1904e-01,  5.1904e-01,  5.1904e-01,  ...,  5.0293e-01,\n",
              "             4.8706e-01,  4.8706e-01],\n",
              "           [ 5.5078e-01,  4.3945e-01,  3.1250e-01,  ...,  4.7119e-01,\n",
              "             4.5532e-01,  4.5532e-01],\n",
              "           ...,\n",
              "           [ 1.8682e+00,  1.8525e+00,  1.8682e+00,  ...,  1.7568e+00,\n",
              "             1.7412e+00,  1.7412e+00],\n",
              "           [ 1.9160e+00,  1.8994e+00,  1.9160e+00,  ...,  1.8359e+00,\n",
              "             1.8203e+00,  1.8203e+00],\n",
              "           [ 1.8359e+00,  1.8203e+00,  1.8525e+00,  ...,  1.8525e+00,\n",
              "             1.8203e+00,  1.8359e+00]],\n",
              " \n",
              "          [[ 6.2891e-01,  6.4502e-01,  6.2891e-01,  ...,  6.2891e-01,\n",
              "             5.9668e-01,  5.9668e-01],\n",
              "           [ 6.1279e-01,  6.1279e-01,  6.1279e-01,  ...,  5.9668e-01,\n",
              "             5.8057e-01,  5.8057e-01],\n",
              "           [ 6.4502e-01,  5.1611e-01,  4.0356e-01,  ...,  5.4834e-01,\n",
              "             5.4834e-01,  5.4834e-01],\n",
              "           ...,\n",
              "           [ 1.9658e+00,  1.9492e+00,  1.9658e+00,  ...,  1.8535e+00,\n",
              "             1.8369e+00,  1.8369e+00],\n",
              "           [ 2.0137e+00,  1.9980e+00,  2.0137e+00,  ...,  1.9336e+00,\n",
              "             1.9170e+00,  1.9170e+00],\n",
              "           [ 1.9336e+00,  1.9170e+00,  1.9492e+00,  ...,  1.9492e+00,\n",
              "             1.9170e+00,  1.9336e+00]],\n",
              " \n",
              "          [[ 8.5645e-01,  8.7158e-01,  8.5645e-01,  ...,  8.5645e-01,\n",
              "             8.2666e-01,  8.2666e-01],\n",
              "           [ 8.4180e-01,  8.4180e-01,  8.4180e-01,  ...,  8.2666e-01,\n",
              "             8.1152e-01,  8.1152e-01],\n",
              "           [ 8.7158e-01,  7.6660e-01,  6.4648e-01,  ...,  7.8174e-01,\n",
              "             7.8174e-01,  7.8174e-01],\n",
              "           ...,\n",
              "           [ 1.9512e+00,  1.9365e+00,  1.9512e+00,  ...,  1.8467e+00,\n",
              "             1.8311e+00,  1.8311e+00],\n",
              "           [ 1.9961e+00,  1.9814e+00,  1.9961e+00,  ...,  1.9209e+00,\n",
              "             1.9062e+00,  1.9062e+00],\n",
              "           [ 1.9209e+00,  1.9062e+00,  1.9365e+00,  ...,  1.9365e+00,\n",
              "             1.9062e+00,  1.9209e+00]]]], device='cuda:0', dtype=torch.float16),\n",
              " 'targets': tensor([-7227954606847027779,  1831150298347181399,  2071822155787768164,\n",
              "          ...,  1471863927130297944,  1281999256319594970,\n",
              "          1197974681144598092], device='cuda:0')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2OqqrnvxXrF",
        "outputId": "4c6845e2-dfcc-4120-9f22-603233439a43"
      },
      "source": [
        "test_set['data']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.1006e+00, -6.8481e-02, -4.9591e-03,  ..., -1.0684e+00,\n",
              "           -1.2109e+00, -1.2744e+00],\n",
              "          [-9.4141e-01,  1.0910e-02,  7.4402e-02,  ...,  1.6968e-01,\n",
              "            1.5381e-01,  1.3794e-01],\n",
              "          [-8.6230e-01,  1.0910e-02,  1.2201e-01,  ...,  2.9663e-01,\n",
              "            2.0142e-01,  2.6779e-02],\n",
              "          ...,\n",
              "          [-4.4946e-01, -4.3359e-01, -4.0186e-01,  ...,  1.8047e+00,\n",
              "            1.7891e+00, -2.7490e-01],\n",
              "          [-3.7012e-01, -3.2251e-01, -3.3838e-01,  ...,  1.7891e+00,\n",
              "            1.8047e+00, -4.4946e-01],\n",
              "          [-1.0205e+00, -1.0205e+00, -1.0205e+00,  ...,  1.8203e+00,\n",
              "            1.8359e+00, -5.9229e-01]],\n",
              "\n",
              "         [[-1.1104e+00, -7.9529e-02, -1.5099e-02,  ..., -1.1426e+00,\n",
              "           -1.2549e+00, -1.2871e+00],\n",
              "          [-9.9756e-01, -3.1204e-02,  3.3203e-02,  ...,  9.7656e-02,\n",
              "            8.1543e-02,  8.1543e-02],\n",
              "          [-9.0088e-01, -1.5099e-02,  9.7656e-02,  ...,  1.7810e-01,\n",
              "            8.1543e-02, -6.3416e-02],\n",
              "          ...,\n",
              "          [-4.8218e-01, -4.4995e-01, -4.3384e-01,  ...,  1.8213e+00,\n",
              "            1.8047e+00, -2.8882e-01],\n",
              "          [-3.8550e-01, -3.3716e-01, -3.3716e-01,  ...,  1.8213e+00,\n",
              "            1.8369e+00, -4.4995e-01],\n",
              "          [-1.0459e+00, -1.0293e+00, -1.0459e+00,  ...,  1.8535e+00,\n",
              "            1.8691e+00, -5.7861e-01]],\n",
              "\n",
              "         [[-1.0029e+00, -4.3121e-02,  1.8740e-03,  ..., -1.1230e+00,\n",
              "           -1.2578e+00, -1.3174e+00],\n",
              "          [-8.8281e-01,  1.8740e-03,  6.1859e-02,  ...,  4.6875e-02,\n",
              "           -1.3123e-02,  1.8740e-03],\n",
              "          [-7.9297e-01,  1.6876e-02,  1.0681e-01,  ...,  1.0681e-01,\n",
              "            1.8740e-03, -1.1810e-01],\n",
              "          ...,\n",
              "          [-3.5791e-01, -3.4302e-01, -3.1299e-01,  ...,  1.7861e+00,\n",
              "            1.7715e+00, -1.7810e-01],\n",
              "          [-2.8296e-01, -2.2302e-01, -2.3804e-01,  ...,  1.7715e+00,\n",
              "            1.7861e+00, -3.2812e-01],\n",
              "          [-8.8281e-01, -8.8281e-01, -8.9795e-01,  ...,  1.8018e+00,\n",
              "            1.8311e+00, -4.6289e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.0371e+00, -1.0840e+00, -1.1318e+00,  ..., -5.2881e-01,\n",
              "           -5.1270e-01, -5.7617e-01],\n",
              "          [-1.0371e+00, -1.0684e+00, -1.1162e+00,  ..., -9.5752e-01,\n",
              "           -9.4141e-01, -9.4141e-01],\n",
              "          [-1.0684e+00, -1.0840e+00, -1.1318e+00,  ..., -1.0527e+00,\n",
              "           -8.9404e-01, -9.7314e-01],\n",
              "          ...,\n",
              "          [ 1.2168e+00,  1.0742e+00,  1.1221e+00,  ..., -3.7012e-01,\n",
              "           -3.2251e-01, -4.3359e-01],\n",
              "          [ 3.9185e-01,  5.9814e-01,  9.7900e-01,  ..., -3.7012e-01,\n",
              "           -3.7012e-01, -4.8120e-01],\n",
              "          [-4.3359e-01, -2.1130e-01,  4.2358e-01,  ..., -4.0186e-01,\n",
              "           -4.1772e-01, -5.2881e-01]],\n",
              "\n",
              "         [[-1.0781e+00, -1.1104e+00, -1.1426e+00,  ..., -4.9829e-01,\n",
              "           -4.8218e-01, -5.4639e-01],\n",
              "          [-1.0781e+00, -1.1104e+00, -1.1260e+00,  ..., -9.3311e-01,\n",
              "           -9.1699e-01, -9.3311e-01],\n",
              "          [-1.0938e+00, -1.1104e+00, -1.1260e+00,  ..., -1.0293e+00,\n",
              "           -8.8477e-01, -9.3311e-01],\n",
              "          ...,\n",
              "          [ 9.6729e-01,  8.2227e-01,  9.3506e-01,  ..., -3.5327e-01,\n",
              "           -3.0493e-01, -4.0161e-01],\n",
              "          [ 2.2644e-01,  4.0356e-01,  7.2559e-01,  ..., -3.5327e-01,\n",
              "           -3.3716e-01, -4.6606e-01],\n",
              "          [-4.4995e-01, -2.7271e-01,  1.7810e-01,  ..., -3.8550e-01,\n",
              "           -4.0161e-01, -5.1416e-01]],\n",
              "\n",
              "         [[-9.1260e-01, -9.5801e-01, -9.8779e-01,  ..., -8.8074e-02,\n",
              "           -1.0309e-01, -1.9312e-01],\n",
              "          [-9.1260e-01, -9.4287e-01, -9.7266e-01,  ..., -6.2793e-01,\n",
              "           -6.2793e-01, -6.5771e-01],\n",
              "          [-9.2773e-01, -9.4287e-01, -9.7266e-01,  ..., -7.7783e-01,\n",
              "           -6.1279e-01, -6.8799e-01],\n",
              "          ...,\n",
              "          [ 4.2163e-01,  2.5684e-01,  4.8169e-01,  ..., -1.3306e-01,\n",
              "           -1.0309e-01, -2.0801e-01],\n",
              "          [ 1.0681e-01,  4.6875e-02,  2.2681e-01,  ..., -1.3306e-01,\n",
              "           -1.3306e-01, -2.6807e-01],\n",
              "          [-1.4807e-01, -1.3306e-01, -2.8122e-02,  ..., -1.9312e-01,\n",
              "           -2.0801e-01, -3.4302e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.7354e+00, -1.7031e+00, -1.7197e+00,  ..., -1.4814e+00,\n",
              "           -1.4014e+00, -1.9414e+00],\n",
              "          [-2.7490e-01, -6.0791e-01, -9.7314e-01,  ..., -1.2744e+00,\n",
              "           -1.2744e+00, -1.8779e+00],\n",
              "          [-2.7490e-01, -7.1924e-01, -1.0840e+00,  ..., -9.2578e-01,\n",
              "           -9.5752e-01, -1.7832e+00],\n",
              "          ...,\n",
              "          [-1.4336e+00,  1.2201e-01,  9.0271e-02,  ..., -5.7617e-01,\n",
              "           -1.9543e-01, -1.1609e-01],\n",
              "          [-1.4492e+00,  6.1426e-01,  5.1904e-01,  ..., -2.1130e-01,\n",
              "           -1.9543e-01, -1.9543e-01],\n",
              "          [-1.6553e+00,  2.3315e-01,  3.2837e-01,  ..., -1.6084e+00,\n",
              "           -1.7354e+00, -1.8301e+00]],\n",
              "\n",
              "         [[-1.7861e+00, -1.7549e+00, -1.7549e+00,  ..., -1.4004e+00,\n",
              "           -1.3193e+00, -1.9150e+00],\n",
              "          [-6.1084e-01, -8.6865e-01, -1.1426e+00,  ..., -1.2227e+00,\n",
              "           -1.2227e+00, -1.8672e+00],\n",
              "          [-5.7861e-01, -9.1699e-01, -1.1904e+00,  ..., -9.0088e-01,\n",
              "           -9.1699e-01, -1.7549e+00],\n",
              "          ...,\n",
              "          [-1.4482e+00,  4.9316e-02,  1.7105e-02,  ..., -6.2695e-01,\n",
              "           -2.7271e-01, -1.9226e-01],\n",
              "          [-1.4482e+00,  5.6445e-01,  4.5190e-01,  ..., -2.4060e-01,\n",
              "           -2.4060e-01, -2.5659e-01],\n",
              "          [-1.6582e+00,  1.7810e-01,  2.5879e-01,  ..., -1.6094e+00,\n",
              "           -1.7227e+00, -1.8350e+00]],\n",
              "\n",
              "         [[-1.6025e+00, -1.6025e+00, -1.6172e+00,  ..., -1.5430e+00,\n",
              "           -1.4531e+00, -1.6777e+00],\n",
              "          [-1.0029e+00, -1.1826e+00, -1.3623e+00,  ..., -1.4531e+00,\n",
              "           -1.4531e+00, -1.6777e+00],\n",
              "          [-9.8779e-01, -1.1973e+00, -1.3623e+00,  ..., -1.2275e+00,\n",
              "           -1.2725e+00, -1.6172e+00],\n",
              "          ...,\n",
              "          [-1.2578e+00,  3.1860e-02,  1.8740e-03,  ..., -5.2295e-01,\n",
              "           -2.3804e-01, -1.7810e-01],\n",
              "          [-1.2432e+00,  5.1172e-01,  4.0674e-01,  ..., -2.3804e-01,\n",
              "           -2.5293e-01, -2.5293e-01],\n",
              "          [-1.4375e+00,  1.5186e-01,  2.2681e-01,  ..., -1.3926e+00,\n",
              "           -1.4980e+00, -1.5879e+00]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 6.4600e-01,  4.2664e-02,  1.0614e-01,  ..., -1.7957e-01,\n",
              "           -6.0791e-01, -5.4443e-01],\n",
              "          [-1.9543e-01,  3.1250e-01,  3.7598e-01,  ..., -6.7139e-01,\n",
              "           -5.2881e-01, -3.2251e-01],\n",
              "          [-1.9543e-01,  3.9185e-01, -5.2582e-02,  ..., -2.1130e-01,\n",
              "           -3.6713e-02, -2.2717e-01],\n",
              "          ...,\n",
              "          [-1.0371e+00, -1.2432e+00, -9.0967e-01,  ..., -9.0967e-01,\n",
              "           -7.1924e-01, -9.5752e-01],\n",
              "          [-4.4946e-01, -1.1318e+00, -1.1162e+00,  ..., -1.0371e+00,\n",
              "           -9.8926e-01, -1.0527e+00],\n",
              "          [-1.1006e+00, -7.5098e-01, -9.2578e-01,  ..., -8.9404e-01,\n",
              "           -1.4336e+00, -8.6230e-01]],\n",
              "\n",
              "         [[-3.1204e-02,  1.1377e-01, -1.5099e-02,  ..., -1.9226e-01,\n",
              "           -2.0837e-01, -2.8882e-01],\n",
              "          [-3.1204e-02, -1.9226e-01,  6.2891e-01,  ..., -7.0752e-01,\n",
              "           -7.7197e-01, -9.5642e-02],\n",
              "          [ 1.4600e-01,  9.7656e-02,  3.3911e-01,  ..., -4.1772e-01,\n",
              "           -5.7861e-01, -6.2695e-01],\n",
              "          ...,\n",
              "          [-1.9226e-01, -1.4392e-01, -3.6938e-01,  ...,  1.1377e-01,\n",
              "            2.7490e-01, -1.4392e-01],\n",
              "          [ 1.2988e-01, -7.9529e-02, -2.0837e-01,  ..., -2.8882e-01,\n",
              "           -1.5099e-02, -4.7302e-02],\n",
              "          [-5.7861e-01, -6.4307e-01, -1.4392e-01,  ...,  1.0061e-03,\n",
              "           -3.6938e-01,  4.9316e-02]],\n",
              "\n",
              "         [[ 3.3179e-01, -1.0309e-01, -8.8074e-02,  ..., -2.0801e-01,\n",
              "           -7.0312e-01, -3.5791e-01],\n",
              "          [-1.3123e-02,  4.5166e-01,  1.0681e-01,  ..., -4.7803e-01,\n",
              "           -1.7810e-01, -7.1777e-01],\n",
              "          [ 3.4668e-01,  1.8740e-03, -2.2302e-01,  ..., -4.1797e-01,\n",
              "           -8.8074e-02, -5.5273e-01],\n",
              "          ...,\n",
              "          [ 7.3682e-01,  8.2666e-01,  1.5762e+00,  ...,  1.1113e+00,\n",
              "            1.2461e+00,  5.8691e-01],\n",
              "          [ 7.6660e-01,  7.2168e-01,  1.0068e+00,  ...,  8.4180e-01,\n",
              "            2.4182e-01,  5.2686e-01],\n",
              "          [ 8.4180e-01,  6.3184e-01,  9.6143e-01,  ...,  9.3164e-01,\n",
              "            9.1650e-01,  1.1270e+00]]],\n",
              "\n",
              "\n",
              "        [[[-5.6055e-01, -5.6055e-01, -7.8271e-01,  ..., -5.9229e-01,\n",
              "           -7.0361e-01, -7.6709e-01],\n",
              "          [-8.6230e-01, -4.0186e-01, -7.9883e-01,  ..., -1.0049e+00,\n",
              "           -4.9707e-01, -4.3359e-01],\n",
              "          [-7.3535e-01, -6.5576e-01, -7.3535e-01,  ..., -4.6533e-01,\n",
              "           -4.4946e-01, -1.6370e-01],\n",
              "          ...,\n",
              "          [-3.0664e-01, -4.3359e-01,  1.8555e-01,  ..., -1.2588e+00,\n",
              "           -9.2578e-01, -1.1797e+00],\n",
              "          [-6.8481e-02,  1.6968e-01, -2.0828e-02,  ..., -7.9883e-01,\n",
              "           -7.6709e-01, -1.0840e+00],\n",
              "          [-4.0186e-01, -3.5425e-01, -6.5576e-01,  ..., -1.1162e+00,\n",
              "           -8.3057e-01, -8.9404e-01]],\n",
              "\n",
              "         [[-4.8218e-01, -4.4995e-01, -4.4995e-01,  ..., -7.2363e-01,\n",
              "           -1.1104e+00, -1.0137e+00],\n",
              "          [-8.5254e-01, -5.6250e-01, -6.5918e-01,  ..., -9.6533e-01,\n",
              "           -9.0088e-01, -9.3311e-01],\n",
              "          [-9.6533e-01, -6.4307e-01, -5.7861e-01,  ..., -9.0088e-01,\n",
              "           -7.0752e-01, -9.1699e-01],\n",
              "          ...,\n",
              "          [-3.8550e-01, -6.7529e-01, -6.4307e-01,  ..., -1.2393e+00,\n",
              "           -6.7529e-01, -1.1104e+00],\n",
              "          [-9.1699e-01, -5.9473e-01, -1.1104e+00,  ..., -1.3516e+00,\n",
              "           -9.3311e-01, -8.8477e-01],\n",
              "          [-6.4307e-01, -9.4922e-01, -1.2393e+00,  ..., -3.3716e-01,\n",
              "           -6.1084e-01, -9.9756e-01]],\n",
              "\n",
              "         [[-4.0308e-01, -2.8122e-02, -2.2302e-01,  ..., -9.2773e-01,\n",
              "           -1.1230e+00, -1.0928e+00],\n",
              "          [-6.7285e-01, -3.8794e-01, -5.6787e-01,  ..., -5.2295e-01,\n",
              "           -5.3809e-01, -2.5293e-01],\n",
              "          [-6.1279e-01, -4.4800e-01, -8.2275e-01,  ..., -6.1279e-01,\n",
              "           -5.2295e-01, -5.2295e-01],\n",
              "          ...,\n",
              "          [-4.9292e-01, -7.4805e-01, -7.0312e-01,  ..., -6.8799e-01,\n",
              "           -9.8779e-01, -9.1260e-01],\n",
              "          [-5.0781e-01, -7.4805e-01, -7.9297e-01,  ..., -1.2432e+00,\n",
              "           -6.2793e-01, -9.8779e-01],\n",
              "          [-3.2812e-01, -6.5771e-01, -6.5771e-01,  ..., -4.7803e-01,\n",
              "           -2.0801e-01, -4.4800e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 5.3467e-01,  5.5078e-01,  5.3467e-01,  ...,  5.3467e-01,\n",
              "            5.0293e-01,  5.0293e-01],\n",
              "          [ 5.1904e-01,  5.1904e-01,  5.1904e-01,  ...,  5.0293e-01,\n",
              "            4.8706e-01,  4.8706e-01],\n",
              "          [ 5.5078e-01,  4.3945e-01,  3.1250e-01,  ...,  4.7119e-01,\n",
              "            4.5532e-01,  4.5532e-01],\n",
              "          ...,\n",
              "          [ 1.8682e+00,  1.8525e+00,  1.8682e+00,  ...,  1.7568e+00,\n",
              "            1.7412e+00,  1.7412e+00],\n",
              "          [ 1.9160e+00,  1.8994e+00,  1.9160e+00,  ...,  1.8359e+00,\n",
              "            1.8203e+00,  1.8203e+00],\n",
              "          [ 1.8359e+00,  1.8203e+00,  1.8525e+00,  ...,  1.8525e+00,\n",
              "            1.8203e+00,  1.8359e+00]],\n",
              "\n",
              "         [[ 6.2891e-01,  6.4502e-01,  6.2891e-01,  ...,  6.2891e-01,\n",
              "            5.9668e-01,  5.9668e-01],\n",
              "          [ 6.1279e-01,  6.1279e-01,  6.1279e-01,  ...,  5.9668e-01,\n",
              "            5.8057e-01,  5.8057e-01],\n",
              "          [ 6.4502e-01,  5.1611e-01,  4.0356e-01,  ...,  5.4834e-01,\n",
              "            5.4834e-01,  5.4834e-01],\n",
              "          ...,\n",
              "          [ 1.9658e+00,  1.9492e+00,  1.9658e+00,  ...,  1.8535e+00,\n",
              "            1.8369e+00,  1.8369e+00],\n",
              "          [ 2.0137e+00,  1.9980e+00,  2.0137e+00,  ...,  1.9336e+00,\n",
              "            1.9170e+00,  1.9170e+00],\n",
              "          [ 1.9336e+00,  1.9170e+00,  1.9492e+00,  ...,  1.9492e+00,\n",
              "            1.9170e+00,  1.9336e+00]],\n",
              "\n",
              "         [[ 8.5645e-01,  8.7158e-01,  8.5645e-01,  ...,  8.5645e-01,\n",
              "            8.2666e-01,  8.2666e-01],\n",
              "          [ 8.4180e-01,  8.4180e-01,  8.4180e-01,  ...,  8.2666e-01,\n",
              "            8.1152e-01,  8.1152e-01],\n",
              "          [ 8.7158e-01,  7.6660e-01,  6.4648e-01,  ...,  7.8174e-01,\n",
              "            7.8174e-01,  7.8174e-01],\n",
              "          ...,\n",
              "          [ 1.9512e+00,  1.9365e+00,  1.9512e+00,  ...,  1.8467e+00,\n",
              "            1.8311e+00,  1.8311e+00],\n",
              "          [ 1.9961e+00,  1.9814e+00,  1.9961e+00,  ...,  1.9209e+00,\n",
              "            1.9062e+00,  1.9062e+00],\n",
              "          [ 1.9209e+00,  1.9062e+00,  1.9365e+00,  ...,  1.9365e+00,\n",
              "            1.9062e+00,  1.9209e+00]]]], device='cuda:0', dtype=torch.float16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emYwjqkzp1zo"
      },
      "source": [
        "test =  {\n",
        "      'input': test_set['data'], \n",
        "      'target': torch.LongTensor(np.random.randint(0,300000)).cuda()\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dxf2fl3p76-",
        "outputId": "4e2fda4a-1d1e-4c1d-9b52-5bc953ac966c"
      },
      "source": [
        "model3l = torch.load(\"drive/MyDrive/Kopia model_3layers_final.pt\")\n",
        "model3l.eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (prep_whiten): Conv2d(3, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (prep_conv): Conv2d(27, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (prep_norm): GhostBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (prep_act): CELU(alpha=0.3)\n",
              "  (layer1_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res1_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res2_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res2_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (layer2_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer2_bn): GhostBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer2_relu): CELU(alpha=0.3)\n",
              "  (layer3_conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer3_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res1_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res1_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res2_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res2_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten()\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=False)\n",
              "  (logits): Mul()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkH0qcCJrxus"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "UbFdjhzlqfJT",
        "outputId": "8c4b53f9-8593-4a3e-d064-8fcd3a390c36"
      },
      "source": [
        "predykcje3l = model3l(test_set['data'])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2c336c863509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredykcje3l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a57e9487eea1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m#only compute nodes that are not supplied as inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72XLxXdoqlwe",
        "outputId": "9451f5d6-5951-4e9e-a78a-d393d94b91c0"
      },
      "source": [
        "test_set['data']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.1006e+00, -6.8481e-02, -4.9591e-03,  ..., -1.0684e+00,\n",
              "           -1.2109e+00, -1.2744e+00],\n",
              "          [-9.4141e-01,  1.0910e-02,  7.4402e-02,  ...,  1.6968e-01,\n",
              "            1.5381e-01,  1.3794e-01],\n",
              "          [-8.6230e-01,  1.0910e-02,  1.2201e-01,  ...,  2.9663e-01,\n",
              "            2.0142e-01,  2.6779e-02],\n",
              "          ...,\n",
              "          [-4.4946e-01, -4.3359e-01, -4.0186e-01,  ...,  1.8047e+00,\n",
              "            1.7891e+00, -2.7490e-01],\n",
              "          [-3.7012e-01, -3.2251e-01, -3.3838e-01,  ...,  1.7891e+00,\n",
              "            1.8047e+00, -4.4946e-01],\n",
              "          [-1.0205e+00, -1.0205e+00, -1.0205e+00,  ...,  1.8203e+00,\n",
              "            1.8359e+00, -5.9229e-01]],\n",
              "\n",
              "         [[-1.1104e+00, -7.9529e-02, -1.5099e-02,  ..., -1.1426e+00,\n",
              "           -1.2549e+00, -1.2871e+00],\n",
              "          [-9.9756e-01, -3.1204e-02,  3.3203e-02,  ...,  9.7656e-02,\n",
              "            8.1543e-02,  8.1543e-02],\n",
              "          [-9.0088e-01, -1.5099e-02,  9.7656e-02,  ...,  1.7810e-01,\n",
              "            8.1543e-02, -6.3416e-02],\n",
              "          ...,\n",
              "          [-4.8218e-01, -4.4995e-01, -4.3384e-01,  ...,  1.8213e+00,\n",
              "            1.8047e+00, -2.8882e-01],\n",
              "          [-3.8550e-01, -3.3716e-01, -3.3716e-01,  ...,  1.8213e+00,\n",
              "            1.8369e+00, -4.4995e-01],\n",
              "          [-1.0459e+00, -1.0293e+00, -1.0459e+00,  ...,  1.8535e+00,\n",
              "            1.8691e+00, -5.7861e-01]],\n",
              "\n",
              "         [[-1.0029e+00, -4.3121e-02,  1.8740e-03,  ..., -1.1230e+00,\n",
              "           -1.2578e+00, -1.3174e+00],\n",
              "          [-8.8281e-01,  1.8740e-03,  6.1859e-02,  ...,  4.6875e-02,\n",
              "           -1.3123e-02,  1.8740e-03],\n",
              "          [-7.9297e-01,  1.6876e-02,  1.0681e-01,  ...,  1.0681e-01,\n",
              "            1.8740e-03, -1.1810e-01],\n",
              "          ...,\n",
              "          [-3.5791e-01, -3.4302e-01, -3.1299e-01,  ...,  1.7861e+00,\n",
              "            1.7715e+00, -1.7810e-01],\n",
              "          [-2.8296e-01, -2.2302e-01, -2.3804e-01,  ...,  1.7715e+00,\n",
              "            1.7861e+00, -3.2812e-01],\n",
              "          [-8.8281e-01, -8.8281e-01, -8.9795e-01,  ...,  1.8018e+00,\n",
              "            1.8311e+00, -4.6289e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.0371e+00, -1.0840e+00, -1.1318e+00,  ..., -5.2881e-01,\n",
              "           -5.1270e-01, -5.7617e-01],\n",
              "          [-1.0371e+00, -1.0684e+00, -1.1162e+00,  ..., -9.5752e-01,\n",
              "           -9.4141e-01, -9.4141e-01],\n",
              "          [-1.0684e+00, -1.0840e+00, -1.1318e+00,  ..., -1.0527e+00,\n",
              "           -8.9404e-01, -9.7314e-01],\n",
              "          ...,\n",
              "          [ 1.2168e+00,  1.0742e+00,  1.1221e+00,  ..., -3.7012e-01,\n",
              "           -3.2251e-01, -4.3359e-01],\n",
              "          [ 3.9185e-01,  5.9814e-01,  9.7900e-01,  ..., -3.7012e-01,\n",
              "           -3.7012e-01, -4.8120e-01],\n",
              "          [-4.3359e-01, -2.1130e-01,  4.2358e-01,  ..., -4.0186e-01,\n",
              "           -4.1772e-01, -5.2881e-01]],\n",
              "\n",
              "         [[-1.0781e+00, -1.1104e+00, -1.1426e+00,  ..., -4.9829e-01,\n",
              "           -4.8218e-01, -5.4639e-01],\n",
              "          [-1.0781e+00, -1.1104e+00, -1.1260e+00,  ..., -9.3311e-01,\n",
              "           -9.1699e-01, -9.3311e-01],\n",
              "          [-1.0938e+00, -1.1104e+00, -1.1260e+00,  ..., -1.0293e+00,\n",
              "           -8.8477e-01, -9.3311e-01],\n",
              "          ...,\n",
              "          [ 9.6729e-01,  8.2227e-01,  9.3506e-01,  ..., -3.5327e-01,\n",
              "           -3.0493e-01, -4.0161e-01],\n",
              "          [ 2.2644e-01,  4.0356e-01,  7.2559e-01,  ..., -3.5327e-01,\n",
              "           -3.3716e-01, -4.6606e-01],\n",
              "          [-4.4995e-01, -2.7271e-01,  1.7810e-01,  ..., -3.8550e-01,\n",
              "           -4.0161e-01, -5.1416e-01]],\n",
              "\n",
              "         [[-9.1260e-01, -9.5801e-01, -9.8779e-01,  ..., -8.8074e-02,\n",
              "           -1.0309e-01, -1.9312e-01],\n",
              "          [-9.1260e-01, -9.4287e-01, -9.7266e-01,  ..., -6.2793e-01,\n",
              "           -6.2793e-01, -6.5771e-01],\n",
              "          [-9.2773e-01, -9.4287e-01, -9.7266e-01,  ..., -7.7783e-01,\n",
              "           -6.1279e-01, -6.8799e-01],\n",
              "          ...,\n",
              "          [ 4.2163e-01,  2.5684e-01,  4.8169e-01,  ..., -1.3306e-01,\n",
              "           -1.0309e-01, -2.0801e-01],\n",
              "          [ 1.0681e-01,  4.6875e-02,  2.2681e-01,  ..., -1.3306e-01,\n",
              "           -1.3306e-01, -2.6807e-01],\n",
              "          [-1.4807e-01, -1.3306e-01, -2.8122e-02,  ..., -1.9312e-01,\n",
              "           -2.0801e-01, -3.4302e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.7354e+00, -1.7031e+00, -1.7197e+00,  ..., -1.4814e+00,\n",
              "           -1.4014e+00, -1.9414e+00],\n",
              "          [-2.7490e-01, -6.0791e-01, -9.7314e-01,  ..., -1.2744e+00,\n",
              "           -1.2744e+00, -1.8779e+00],\n",
              "          [-2.7490e-01, -7.1924e-01, -1.0840e+00,  ..., -9.2578e-01,\n",
              "           -9.5752e-01, -1.7832e+00],\n",
              "          ...,\n",
              "          [-1.4336e+00,  1.2201e-01,  9.0271e-02,  ..., -5.7617e-01,\n",
              "           -1.9543e-01, -1.1609e-01],\n",
              "          [-1.4492e+00,  6.1426e-01,  5.1904e-01,  ..., -2.1130e-01,\n",
              "           -1.9543e-01, -1.9543e-01],\n",
              "          [-1.6553e+00,  2.3315e-01,  3.2837e-01,  ..., -1.6084e+00,\n",
              "           -1.7354e+00, -1.8301e+00]],\n",
              "\n",
              "         [[-1.7861e+00, -1.7549e+00, -1.7549e+00,  ..., -1.4004e+00,\n",
              "           -1.3193e+00, -1.9150e+00],\n",
              "          [-6.1084e-01, -8.6865e-01, -1.1426e+00,  ..., -1.2227e+00,\n",
              "           -1.2227e+00, -1.8672e+00],\n",
              "          [-5.7861e-01, -9.1699e-01, -1.1904e+00,  ..., -9.0088e-01,\n",
              "           -9.1699e-01, -1.7549e+00],\n",
              "          ...,\n",
              "          [-1.4482e+00,  4.9316e-02,  1.7105e-02,  ..., -6.2695e-01,\n",
              "           -2.7271e-01, -1.9226e-01],\n",
              "          [-1.4482e+00,  5.6445e-01,  4.5190e-01,  ..., -2.4060e-01,\n",
              "           -2.4060e-01, -2.5659e-01],\n",
              "          [-1.6582e+00,  1.7810e-01,  2.5879e-01,  ..., -1.6094e+00,\n",
              "           -1.7227e+00, -1.8350e+00]],\n",
              "\n",
              "         [[-1.6025e+00, -1.6025e+00, -1.6172e+00,  ..., -1.5430e+00,\n",
              "           -1.4531e+00, -1.6777e+00],\n",
              "          [-1.0029e+00, -1.1826e+00, -1.3623e+00,  ..., -1.4531e+00,\n",
              "           -1.4531e+00, -1.6777e+00],\n",
              "          [-9.8779e-01, -1.1973e+00, -1.3623e+00,  ..., -1.2275e+00,\n",
              "           -1.2725e+00, -1.6172e+00],\n",
              "          ...,\n",
              "          [-1.2578e+00,  3.1860e-02,  1.8740e-03,  ..., -5.2295e-01,\n",
              "           -2.3804e-01, -1.7810e-01],\n",
              "          [-1.2432e+00,  5.1172e-01,  4.0674e-01,  ..., -2.3804e-01,\n",
              "           -2.5293e-01, -2.5293e-01],\n",
              "          [-1.4375e+00,  1.5186e-01,  2.2681e-01,  ..., -1.3926e+00,\n",
              "           -1.4980e+00, -1.5879e+00]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 6.4600e-01,  4.2664e-02,  1.0614e-01,  ..., -1.7957e-01,\n",
              "           -6.0791e-01, -5.4443e-01],\n",
              "          [-1.9543e-01,  3.1250e-01,  3.7598e-01,  ..., -6.7139e-01,\n",
              "           -5.2881e-01, -3.2251e-01],\n",
              "          [-1.9543e-01,  3.9185e-01, -5.2582e-02,  ..., -2.1130e-01,\n",
              "           -3.6713e-02, -2.2717e-01],\n",
              "          ...,\n",
              "          [-1.0371e+00, -1.2432e+00, -9.0967e-01,  ..., -9.0967e-01,\n",
              "           -7.1924e-01, -9.5752e-01],\n",
              "          [-4.4946e-01, -1.1318e+00, -1.1162e+00,  ..., -1.0371e+00,\n",
              "           -9.8926e-01, -1.0527e+00],\n",
              "          [-1.1006e+00, -7.5098e-01, -9.2578e-01,  ..., -8.9404e-01,\n",
              "           -1.4336e+00, -8.6230e-01]],\n",
              "\n",
              "         [[-3.1204e-02,  1.1377e-01, -1.5099e-02,  ..., -1.9226e-01,\n",
              "           -2.0837e-01, -2.8882e-01],\n",
              "          [-3.1204e-02, -1.9226e-01,  6.2891e-01,  ..., -7.0752e-01,\n",
              "           -7.7197e-01, -9.5642e-02],\n",
              "          [ 1.4600e-01,  9.7656e-02,  3.3911e-01,  ..., -4.1772e-01,\n",
              "           -5.7861e-01, -6.2695e-01],\n",
              "          ...,\n",
              "          [-1.9226e-01, -1.4392e-01, -3.6938e-01,  ...,  1.1377e-01,\n",
              "            2.7490e-01, -1.4392e-01],\n",
              "          [ 1.2988e-01, -7.9529e-02, -2.0837e-01,  ..., -2.8882e-01,\n",
              "           -1.5099e-02, -4.7302e-02],\n",
              "          [-5.7861e-01, -6.4307e-01, -1.4392e-01,  ...,  1.0061e-03,\n",
              "           -3.6938e-01,  4.9316e-02]],\n",
              "\n",
              "         [[ 3.3179e-01, -1.0309e-01, -8.8074e-02,  ..., -2.0801e-01,\n",
              "           -7.0312e-01, -3.5791e-01],\n",
              "          [-1.3123e-02,  4.5166e-01,  1.0681e-01,  ..., -4.7803e-01,\n",
              "           -1.7810e-01, -7.1777e-01],\n",
              "          [ 3.4668e-01,  1.8740e-03, -2.2302e-01,  ..., -4.1797e-01,\n",
              "           -8.8074e-02, -5.5273e-01],\n",
              "          ...,\n",
              "          [ 7.3682e-01,  8.2666e-01,  1.5762e+00,  ...,  1.1113e+00,\n",
              "            1.2461e+00,  5.8691e-01],\n",
              "          [ 7.6660e-01,  7.2168e-01,  1.0068e+00,  ...,  8.4180e-01,\n",
              "            2.4182e-01,  5.2686e-01],\n",
              "          [ 8.4180e-01,  6.3184e-01,  9.6143e-01,  ...,  9.3164e-01,\n",
              "            9.1650e-01,  1.1270e+00]]],\n",
              "\n",
              "\n",
              "        [[[-5.6055e-01, -5.6055e-01, -7.8271e-01,  ..., -5.9229e-01,\n",
              "           -7.0361e-01, -7.6709e-01],\n",
              "          [-8.6230e-01, -4.0186e-01, -7.9883e-01,  ..., -1.0049e+00,\n",
              "           -4.9707e-01, -4.3359e-01],\n",
              "          [-7.3535e-01, -6.5576e-01, -7.3535e-01,  ..., -4.6533e-01,\n",
              "           -4.4946e-01, -1.6370e-01],\n",
              "          ...,\n",
              "          [-3.0664e-01, -4.3359e-01,  1.8555e-01,  ..., -1.2588e+00,\n",
              "           -9.2578e-01, -1.1797e+00],\n",
              "          [-6.8481e-02,  1.6968e-01, -2.0828e-02,  ..., -7.9883e-01,\n",
              "           -7.6709e-01, -1.0840e+00],\n",
              "          [-4.0186e-01, -3.5425e-01, -6.5576e-01,  ..., -1.1162e+00,\n",
              "           -8.3057e-01, -8.9404e-01]],\n",
              "\n",
              "         [[-4.8218e-01, -4.4995e-01, -4.4995e-01,  ..., -7.2363e-01,\n",
              "           -1.1104e+00, -1.0137e+00],\n",
              "          [-8.5254e-01, -5.6250e-01, -6.5918e-01,  ..., -9.6533e-01,\n",
              "           -9.0088e-01, -9.3311e-01],\n",
              "          [-9.6533e-01, -6.4307e-01, -5.7861e-01,  ..., -9.0088e-01,\n",
              "           -7.0752e-01, -9.1699e-01],\n",
              "          ...,\n",
              "          [-3.8550e-01, -6.7529e-01, -6.4307e-01,  ..., -1.2393e+00,\n",
              "           -6.7529e-01, -1.1104e+00],\n",
              "          [-9.1699e-01, -5.9473e-01, -1.1104e+00,  ..., -1.3516e+00,\n",
              "           -9.3311e-01, -8.8477e-01],\n",
              "          [-6.4307e-01, -9.4922e-01, -1.2393e+00,  ..., -3.3716e-01,\n",
              "           -6.1084e-01, -9.9756e-01]],\n",
              "\n",
              "         [[-4.0308e-01, -2.8122e-02, -2.2302e-01,  ..., -9.2773e-01,\n",
              "           -1.1230e+00, -1.0928e+00],\n",
              "          [-6.7285e-01, -3.8794e-01, -5.6787e-01,  ..., -5.2295e-01,\n",
              "           -5.3809e-01, -2.5293e-01],\n",
              "          [-6.1279e-01, -4.4800e-01, -8.2275e-01,  ..., -6.1279e-01,\n",
              "           -5.2295e-01, -5.2295e-01],\n",
              "          ...,\n",
              "          [-4.9292e-01, -7.4805e-01, -7.0312e-01,  ..., -6.8799e-01,\n",
              "           -9.8779e-01, -9.1260e-01],\n",
              "          [-5.0781e-01, -7.4805e-01, -7.9297e-01,  ..., -1.2432e+00,\n",
              "           -6.2793e-01, -9.8779e-01],\n",
              "          [-3.2812e-01, -6.5771e-01, -6.5771e-01,  ..., -4.7803e-01,\n",
              "           -2.0801e-01, -4.4800e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 5.3467e-01,  5.5078e-01,  5.3467e-01,  ...,  5.3467e-01,\n",
              "            5.0293e-01,  5.0293e-01],\n",
              "          [ 5.1904e-01,  5.1904e-01,  5.1904e-01,  ...,  5.0293e-01,\n",
              "            4.8706e-01,  4.8706e-01],\n",
              "          [ 5.5078e-01,  4.3945e-01,  3.1250e-01,  ...,  4.7119e-01,\n",
              "            4.5532e-01,  4.5532e-01],\n",
              "          ...,\n",
              "          [ 1.8682e+00,  1.8525e+00,  1.8682e+00,  ...,  1.7568e+00,\n",
              "            1.7412e+00,  1.7412e+00],\n",
              "          [ 1.9160e+00,  1.8994e+00,  1.9160e+00,  ...,  1.8359e+00,\n",
              "            1.8203e+00,  1.8203e+00],\n",
              "          [ 1.8359e+00,  1.8203e+00,  1.8525e+00,  ...,  1.8525e+00,\n",
              "            1.8203e+00,  1.8359e+00]],\n",
              "\n",
              "         [[ 6.2891e-01,  6.4502e-01,  6.2891e-01,  ...,  6.2891e-01,\n",
              "            5.9668e-01,  5.9668e-01],\n",
              "          [ 6.1279e-01,  6.1279e-01,  6.1279e-01,  ...,  5.9668e-01,\n",
              "            5.8057e-01,  5.8057e-01],\n",
              "          [ 6.4502e-01,  5.1611e-01,  4.0356e-01,  ...,  5.4834e-01,\n",
              "            5.4834e-01,  5.4834e-01],\n",
              "          ...,\n",
              "          [ 1.9658e+00,  1.9492e+00,  1.9658e+00,  ...,  1.8535e+00,\n",
              "            1.8369e+00,  1.8369e+00],\n",
              "          [ 2.0137e+00,  1.9980e+00,  2.0137e+00,  ...,  1.9336e+00,\n",
              "            1.9170e+00,  1.9170e+00],\n",
              "          [ 1.9336e+00,  1.9170e+00,  1.9492e+00,  ...,  1.9492e+00,\n",
              "            1.9170e+00,  1.9336e+00]],\n",
              "\n",
              "         [[ 8.5645e-01,  8.7158e-01,  8.5645e-01,  ...,  8.5645e-01,\n",
              "            8.2666e-01,  8.2666e-01],\n",
              "          [ 8.4180e-01,  8.4180e-01,  8.4180e-01,  ...,  8.2666e-01,\n",
              "            8.1152e-01,  8.1152e-01],\n",
              "          [ 8.7158e-01,  7.6660e-01,  6.4648e-01,  ...,  7.8174e-01,\n",
              "            7.8174e-01,  7.8174e-01],\n",
              "          ...,\n",
              "          [ 1.9512e+00,  1.9365e+00,  1.9512e+00,  ...,  1.8467e+00,\n",
              "            1.8311e+00,  1.8311e+00],\n",
              "          [ 1.9961e+00,  1.9814e+00,  1.9961e+00,  ...,  1.9209e+00,\n",
              "            1.9062e+00,  1.9062e+00],\n",
              "          [ 1.9209e+00,  1.9062e+00,  1.9365e+00,  ...,  1.9365e+00,\n",
              "            1.9062e+00,  1.9209e+00]]]], device='cuda:0', dtype=torch.float16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26VZJdAug--"
      },
      "source": [
        "test_data_nowe = test_set['data'].permute([0, 2, 3, 1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlTsHKZ9wgGq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us1X1td8vdxW",
        "outputId": "fc1e39f9-d07a-4bab-aaaa-9d54d797c44d"
      },
      "source": [
        "model3l"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (prep_whiten): Conv2d(3, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (prep_conv): Conv2d(27, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (prep_norm): GhostBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (prep_act): CELU(alpha=0.3)\n",
              "  (layer1_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res1_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res1_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer1_residual_res2_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1_residual_res2_bn): GhostBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (layer2_conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer2_bn): GhostBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer2_relu): CELU(alpha=0.3)\n",
              "  (layer3_conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (layer3_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res1_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res1_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res1_relu): CELU(alpha=0.3)\n",
              "  (layer3_residual_res2_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer3_residual_res2_bn): GhostBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer3_residual_res2_relu): CELU(alpha=0.3)\n",
              "  (pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten()\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=False)\n",
              "  (logits): Mul()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3O-9ttwofz",
        "outputId": "80d8fc6a-4843-4a0b-c028-abc8b940a6c0"
      },
      "source": [
        "train=main(24,2, train_transforms = [Crop(32, 32), FlipLR(), Cutout(8,8)])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading datasets\n",
            "Warming up torch\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-XbuoCzxkqM",
        "outputId": "da36894d-b9c1-4945-b316-edc25cbb7931"
      },
      "source": [
        "train"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': tensor([[[[-2.0828e-02,  2.6779e-02, -3.7012e-01,  ..., -6.0791e-01,\n",
              "            -3.5425e-01, -1.0022e-01],\n",
              "           [-1.6370e-01, -3.0664e-01, -6.0791e-01,  ..., -4.9591e-03,\n",
              "             7.4402e-02,  1.5381e-01],\n",
              "           [-2.4304e-01, -6.7139e-01, -1.2109e+00,  ..., -1.1609e-01,\n",
              "             7.4402e-02,  2.6489e-01],\n",
              "           ...,\n",
              "           [ 9.1553e-01,  1.0430e+00,  1.1533e+00,  ...,  5.5078e-01,\n",
              "             1.8555e-01, -1.1953e+00],\n",
              "           [ 4.8706e-01,  7.4121e-01,  9.6338e-01,  ...,  7.5684e-01,\n",
              "             9.6338e-01, -1.7957e-01],\n",
              "           [ 9.0271e-02,  4.0771e-01,  6.7773e-01,  ...,  1.2803e+00,\n",
              "             1.2334e+00,  2.8076e-01]],\n",
              " \n",
              "          [[-7.3975e-01, -7.0752e-01, -9.3311e-01,  ..., -1.0615e+00,\n",
              "            -9.0088e-01, -7.0752e-01],\n",
              "           [-8.5254e-01, -9.6533e-01, -1.1104e+00,  ..., -5.7861e-01,\n",
              "            -5.3027e-01, -4.8218e-01],\n",
              "           [-8.2031e-01, -1.1748e+00, -1.5449e+00,  ..., -6.2695e-01,\n",
              "            -4.6606e-01, -2.8882e-01],\n",
              "           ...,\n",
              "           [ 3.7134e-01,  5.4834e-01,  6.1279e-01,  ...,  1.6211e-01,\n",
              "            -2.8882e-01, -1.5938e+00],\n",
              "           [-4.7302e-02,  2.5879e-01,  4.0356e-01,  ...,  3.3911e-01,\n",
              "             5.1611e-01, -5.7861e-01],\n",
              "           [-5.6250e-01, -1.9226e-01, -1.5099e-02,  ...,  8.0615e-01,\n",
              "             7.0947e-01, -2.5659e-01]],\n",
              " \n",
              "          [[-1.1680e+00, -1.1230e+00, -1.1973e+00,  ..., -1.3027e+00,\n",
              "            -1.2275e+00, -1.1074e+00],\n",
              "           [-1.2129e+00, -1.2881e+00, -1.3330e+00,  ..., -9.5801e-01,\n",
              "            -9.5801e-01, -9.4287e-01],\n",
              "           [-1.0928e+00, -1.3623e+00, -1.5879e+00,  ..., -9.5801e-01,\n",
              "            -8.3789e-01, -6.8799e-01],\n",
              "           ...,\n",
              "           [-1.1973e+00, -1.3027e+00, -1.3174e+00,  ..., -6.5771e-01,\n",
              "            -8.0811e-01, -1.5430e+00],\n",
              "           [-1.3174e+00, -1.2881e+00, -1.3477e+00,  ..., -4.0308e-01,\n",
              "            -2.3804e-01, -1.0176e+00],\n",
              "           [-1.1973e+00, -1.1826e+00, -1.4229e+00,  ..., -1.3306e-01,\n",
              "            -1.7810e-01, -8.5303e-01]]],\n",
              " \n",
              " \n",
              "         [[[ 1.4238e+00,  9.6338e-01,  1.6968e-01,  ..., -8.6230e-01,\n",
              "            -8.9404e-01, -4.9707e-01],\n",
              "           [ 4.0771e-01,  1.0614e-01, -5.2582e-02,  ..., -7.8271e-01,\n",
              "            -5.9229e-01, -4.4946e-01],\n",
              "           [ 2.0142e-01,  3.4424e-01, -1.6370e-01,  ..., -7.3535e-01,\n",
              "            -5.6055e-01, -6.7139e-01],\n",
              "           ...,\n",
              "           [ 4.7119e-01,  3.4424e-01,  4.5532e-01,  ..., -1.3223e+00,\n",
              "            -1.6240e+00, -1.4814e+00],\n",
              "           [-1.4783e-01, -1.9543e-01, -1.6370e-01,  ..., -7.6709e-01,\n",
              "            -1.4971e+00, -6.3965e-01],\n",
              "           [-1.0840e+00, -1.3066e+00, -1.2910e+00,  ..., -3.5425e-01,\n",
              "            -1.4492e+00, -1.3066e+00]],\n",
              " \n",
              "          [[ 1.5635e+00,  1.0156e+00,  2.5879e-01,  ..., -8.0420e-01,\n",
              "            -8.0420e-01, -4.1772e-01],\n",
              "           [ 5.4834e-01,  1.6211e-01, -3.1204e-02,  ..., -7.2363e-01,\n",
              "            -5.1416e-01, -3.6938e-01],\n",
              "           [ 4.3579e-01,  4.8413e-01, -1.2781e-01,  ..., -6.5918e-01,\n",
              "            -4.8218e-01, -5.9473e-01],\n",
              "           ...,\n",
              "           [ 4.8413e-01,  4.8413e-01,  5.9668e-01,  ..., -1.4326e+00,\n",
              "            -1.7549e+00, -1.5615e+00],\n",
              "           [-2.4060e-01, -1.6003e-01, -6.3416e-02,  ..., -7.3975e-01,\n",
              "            -1.5449e+00, -7.2363e-01],\n",
              "           [-1.1582e+00, -1.3193e+00, -1.2393e+00,  ..., -2.4060e-01,\n",
              "            -1.4482e+00, -1.4326e+00]],\n",
              " \n",
              "          [[ 1.6963e+00,  1.1416e+00,  4.2163e-01,  ..., -8.0811e-01,\n",
              "            -9.5801e-01, -5.9814e-01],\n",
              "           [ 7.9639e-01,  3.7671e-01,  1.0681e-01,  ..., -7.4805e-01,\n",
              "            -6.5771e-01, -5.0781e-01],\n",
              "           [ 7.2168e-01,  6.7676e-01, -2.8122e-02,  ..., -7.4805e-01,\n",
              "            -5.8301e-01, -6.5771e-01],\n",
              "           ...,\n",
              "           [ 7.6660e-01,  7.3682e-01,  8.4180e-01,  ..., -1.1680e+00,\n",
              "            -1.4980e+00, -1.2725e+00],\n",
              "           [ 4.6875e-02,  1.0681e-01,  2.7173e-01,  ..., -4.4800e-01,\n",
              "            -1.2725e+00, -5.3809e-01],\n",
              "           [-8.5303e-01, -9.8779e-01, -8.6768e-01,  ...,  1.3684e-01,\n",
              "            -1.1523e+00, -1.2129e+00]]],\n",
              " \n",
              " \n",
              "         [[[ 2.0586e+00,  2.0586e+00,  2.0586e+00,  ...,  2.0586e+00,\n",
              "             2.0586e+00,  2.0586e+00],\n",
              "           [ 2.0586e+00,  2.0586e+00,  2.0586e+00,  ...,  2.0586e+00,\n",
              "             2.0586e+00,  2.0586e+00],\n",
              "           [ 2.0430e+00,  2.0430e+00,  2.0430e+00,  ...,  2.0430e+00,\n",
              "             2.0430e+00,  2.0430e+00],\n",
              "           ...,\n",
              "           [-6.5576e-01, -4.8120e-01, -3.2251e-01,  ..., -8.4619e-01,\n",
              "            -9.8926e-01, -1.1162e+00],\n",
              "           [-5.7617e-01, -3.5425e-01, -2.4304e-01,  ..., -8.7793e-01,\n",
              "            -9.8926e-01, -1.1475e+00],\n",
              "           [-5.7617e-01, -5.2881e-01, -4.4946e-01,  ..., -9.7314e-01,\n",
              "            -1.0205e+00, -1.0527e+00]],\n",
              " \n",
              "          [[ 2.1250e+00,  2.1250e+00,  2.1250e+00,  ...,  2.1250e+00,\n",
              "             2.1250e+00,  2.1250e+00],\n",
              "           [ 2.1250e+00,  2.1250e+00,  2.1250e+00,  ...,  2.1250e+00,\n",
              "             2.1250e+00,  2.1250e+00],\n",
              "           [ 2.1094e+00,  2.1094e+00,  2.1094e+00,  ...,  2.1094e+00,\n",
              "             2.1094e+00,  2.1094e+00],\n",
              "           ...,\n",
              "           [-4.4995e-01, -3.2104e-01, -1.7615e-01,  ..., -6.7529e-01,\n",
              "            -7.7197e-01, -8.5254e-01],\n",
              "           [-3.6938e-01, -2.0837e-01, -1.1176e-01,  ..., -6.9141e-01,\n",
              "            -7.5586e-01, -8.8477e-01],\n",
              "           [-3.6938e-01, -3.8550e-01, -3.2104e-01,  ..., -7.8809e-01,\n",
              "            -7.8809e-01, -8.0420e-01]],\n",
              " \n",
              "          [[ 2.1172e+00,  2.1172e+00,  2.1172e+00,  ...,  2.1172e+00,\n",
              "             2.1172e+00,  2.1172e+00],\n",
              "           [ 2.1172e+00,  2.1172e+00,  2.1172e+00,  ...,  2.1172e+00,\n",
              "             2.1172e+00,  2.1172e+00],\n",
              "           [ 2.1016e+00,  2.1016e+00,  2.1016e+00,  ...,  2.1016e+00,\n",
              "             2.1016e+00,  2.1016e+00],\n",
              "           ...,\n",
              "           [-3.5791e-01, -2.5293e-01, -1.1810e-01,  ..., -5.0781e-01,\n",
              "            -5.6787e-01, -6.4307e-01],\n",
              "           [-2.6807e-01, -8.8074e-02, -1.3123e-02,  ..., -4.9292e-01,\n",
              "            -5.3809e-01, -6.5771e-01],\n",
              "           [-2.5293e-01, -2.3804e-01, -2.0801e-01,  ..., -5.8301e-01,\n",
              "            -5.6787e-01, -5.6787e-01]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[-4.3359e-01, -4.0186e-01, -1.9543e-01,  ..., -8.9404e-01,\n",
              "             6.7773e-01,  1.0898e+00],\n",
              "           [-7.1924e-01, -7.0361e-01, -5.6055e-01,  ...,  4.2358e-01,\n",
              "             1.1221e+00,  1.6143e+00],\n",
              "           [-8.4619e-01, -8.6230e-01, -8.9404e-01,  ...,  9.9512e-01,\n",
              "             1.6455e+00,  1.9473e+00],\n",
              "           ...,\n",
              "           [-1.1797e+00, -1.1641e+00, -1.1318e+00,  ..., -1.7197e+00,\n",
              "            -1.5918e+00, -1.5127e+00],\n",
              "           [-1.1797e+00, -1.1953e+00, -1.0049e+00,  ..., -1.6240e+00,\n",
              "            -1.6719e+00, -1.4648e+00],\n",
              "           [-1.1162e+00, -1.1006e+00, -1.0205e+00,  ..., -1.3379e+00,\n",
              "            -1.5605e+00, -1.5449e+00]],\n",
              " \n",
              "          [[ 1.2891e+00,  1.3213e+00,  1.2568e+00,  ..., -6.7529e-01,\n",
              "             9.3506e-01,  1.4023e+00],\n",
              "           [ 1.1924e+00,  1.2090e+00,  1.1602e+00,  ...,  6.2891e-01,\n",
              "             1.3379e+00,  1.9014e+00],\n",
              "           [ 1.1602e+00,  1.1602e+00,  1.1123e+00,  ...,  1.1602e+00,\n",
              "             1.7568e+00,  2.0938e+00],\n",
              "           ...,\n",
              "           [-7.7197e-01, -8.2031e-01, -8.8477e-01,  ..., -1.3359e+00,\n",
              "            -1.1582e+00, -1.0615e+00],\n",
              "           [-8.3643e-01, -9.4922e-01, -8.3643e-01,  ..., -1.2871e+00,\n",
              "            -1.2871e+00, -1.0615e+00],\n",
              "           [-8.5254e-01, -9.1699e-01, -8.8477e-01,  ..., -1.1104e+00,\n",
              "            -1.3037e+00, -1.2227e+00]],\n",
              " \n",
              "          [[ 1.9814e+00,  2.0566e+00,  2.0703e+00,  ..., -4.0308e-01,\n",
              "             1.1719e+00,  1.6514e+00],\n",
              "           [ 1.9365e+00,  1.9658e+00,  2.0410e+00,  ...,  7.9639e-01,\n",
              "             1.4863e+00,  1.9814e+00],\n",
              "           [ 1.9658e+00,  1.8760e+00,  1.9658e+00,  ...,  1.3818e+00,\n",
              "             1.8467e+00,  2.0859e+00],\n",
              "           ...,\n",
              "           [-2.5293e-01, -3.5791e-01, -5.0781e-01,  ..., -7.4805e-01,\n",
              "            -5.6787e-01, -4.7803e-01],\n",
              "           [-3.1299e-01, -5.6787e-01, -6.2793e-01,  ..., -8.0811e-01,\n",
              "            -7.1777e-01, -4.9292e-01],\n",
              "           [-4.1797e-01, -6.4307e-01, -7.1777e-01,  ..., -9.4287e-01,\n",
              "            -9.1260e-01, -6.8799e-01]]],\n",
              " \n",
              " \n",
              "         [[[ 1.6621e+00,  1.6777e+00,  1.6934e+00,  ...,  1.1064e+00,\n",
              "             1.2334e+00,  1.3125e+00],\n",
              "           [ 1.5352e+00,  1.5508e+00,  1.5820e+00,  ...,  8.6816e-01,\n",
              "             9.3164e-01,  9.9512e-01],\n",
              "           [ 1.2168e+00,  1.2334e+00,  1.2490e+00,  ...,  7.8857e-01,\n",
              "             7.7295e-01,  8.2031e-01],\n",
              "           ...,\n",
              "           [-1.6553e+00, -4.9591e-03,  1.2334e+00,  ...,  1.5381e-01,\n",
              "            -2.4304e-01, -2.4304e-01],\n",
              "           [-1.3545e+00, -5.1270e-01,  9.9512e-01,  ..., -6.0791e-01,\n",
              "            -1.7354e+00, -1.2588e+00],\n",
              "           [-4.0186e-01, -1.4783e-01,  9.6338e-01,  ..., -6.8481e-02,\n",
              "            -1.4492e+00, -1.6719e+00]],\n",
              " \n",
              "          [[ 1.9014e+00,  1.9014e+00,  1.8857e+00,  ...,  1.3701e+00,\n",
              "             1.4180e+00,  1.4668e+00],\n",
              "           [ 1.7402e+00,  1.7246e+00,  1.7402e+00,  ...,  1.1602e+00,\n",
              "             1.2246e+00,  1.2568e+00],\n",
              "           [ 1.4990e+00,  1.4824e+00,  1.4824e+00,  ...,  1.0957e+00,\n",
              "             1.1445e+00,  1.1924e+00],\n",
              "           ...,\n",
              "           [-1.7549e+00, -9.5642e-02,  1.1768e+00,  ...,  1.4600e-01,\n",
              "            -2.7271e-01, -2.7271e-01],\n",
              "           [-1.4326e+00, -6.1084e-01,  9.0283e-01,  ..., -5.7861e-01,\n",
              "            -1.7705e+00, -1.3037e+00],\n",
              "           [-4.3384e-01, -1.9226e-01,  8.5449e-01,  ...,  1.7105e-02,\n",
              "            -1.4805e+00, -1.7383e+00]],\n",
              " \n",
              "          [[ 2.0703e+00,  2.0703e+00,  2.0703e+00,  ...,  1.7266e+00,\n",
              "             1.6963e+00,  1.7109e+00],\n",
              "           [ 1.9814e+00,  1.9814e+00,  1.9961e+00,  ...,  1.5918e+00,\n",
              "             1.5918e+00,  1.6064e+00],\n",
              "           [ 1.8760e+00,  1.8760e+00,  1.8760e+00,  ...,  1.5469e+00,\n",
              "             1.5762e+00,  1.6064e+00],\n",
              "           ...,\n",
              "           [-1.5576e+00, -1.9312e-01,  8.8672e-01,  ...,  1.9678e-01,\n",
              "            -1.9312e-01, -2.2302e-01],\n",
              "           [-1.2432e+00, -6.2793e-01,  7.0654e-01,  ..., -4.1797e-01,\n",
              "            -1.5127e+00, -1.1230e+00],\n",
              "           [-3.5791e-01, -2.0801e-01,  7.6660e-01,  ...,  2.4182e-01,\n",
              "            -1.2275e+00, -1.5127e+00]]],\n",
              " \n",
              " \n",
              "         [[[ 5.8533e-02,  2.3315e-01,  3.2837e-01,  ...,  1.2646e+00,\n",
              "             1.1221e+00,  9.9512e-01],\n",
              "           [ 1.3281e+00,  1.1377e+00,  9.9512e-01,  ...,  1.4395e+00,\n",
              "             1.1699e+00,  1.0586e+00],\n",
              "           [ 1.6777e+00,  1.6621e+00,  1.6777e+00,  ...,  1.5029e+00,\n",
              "             1.3760e+00,  1.3281e+00],\n",
              "           ...,\n",
              "           [ 1.0614e-01,  1.0614e-01,  1.0614e-01,  ...,  1.5664e+00,\n",
              "             1.4395e+00,  1.1064e+00],\n",
              "           [-6.8481e-02,  2.0142e-01,  2.8076e-01,  ...,  1.4717e+00,\n",
              "             7.7295e-01,  7.4121e-01],\n",
              "           [-4.8120e-01, -6.8481e-02,  1.8555e-01,  ...,  1.6968e-01,\n",
              "            -1.3196e-01, -2.4304e-01]],\n",
              " \n",
              "          [[-1.7615e-01,  1.0061e-03, -3.1204e-02,  ...,  1.4346e+00,\n",
              "             1.2412e+00,  1.0801e+00],\n",
              "           [ 1.1768e+00,  9.6729e-01,  6.7725e-01,  ...,  1.5312e+00,\n",
              "             1.2412e+00,  1.1123e+00],\n",
              "           [ 1.7725e+00,  1.7891e+00,  1.7725e+00,  ...,  1.5635e+00,\n",
              "             1.4346e+00,  1.4023e+00],\n",
              "           ...,\n",
              "           [ 1.7105e-02,  4.9316e-02,  6.5430e-02,  ...,  1.5957e+00,\n",
              "             1.4502e+00,  1.1123e+00],\n",
              "           [-1.7615e-01,  1.2988e-01,  1.9421e-01,  ...,  1.5312e+00,\n",
              "             8.3838e-01,  7.5781e-01],\n",
              "           [-5.6250e-01, -1.2781e-01,  9.7656e-02,  ...,  2.4255e-01,\n",
              "            -6.3416e-02, -1.7615e-01]],\n",
              " \n",
              "          [[ 1.8740e-03,  1.5186e-01,  1.6687e-01,  ...,  1.6514e+00,\n",
              "             1.4561e+00,  1.2910e+00],\n",
              "           [ 1.3213e+00,  1.1270e+00,  8.7158e-01,  ...,  1.7715e+00,\n",
              "             1.4561e+00,  1.3369e+00],\n",
              "           [ 1.9512e+00,  1.9658e+00,  1.9512e+00,  ...,  1.7715e+00,\n",
              "             1.6660e+00,  1.6211e+00],\n",
              "           ...,\n",
              "           [ 1.6876e-02,  6.1859e-02,  9.1858e-02,  ...,  1.5615e+00,\n",
              "             1.4111e+00,  1.1270e+00],\n",
              "           [-1.4807e-01,  9.1858e-02,  1.6687e-01,  ...,  1.5166e+00,\n",
              "             8.7158e-01,  8.2666e-01],\n",
              "           [-4.4800e-01, -1.0309e-01,  1.0681e-01,  ...,  3.7671e-01,\n",
              "             1.2183e-01,  1.6876e-02]]]], device='cuda:0', dtype=torch.float16),\n",
              " 'targets': tensor([6, 9, 9,  ..., 9, 1, 1], device='cuda:0')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC0-kca0xul4",
        "outputId": "6cfacbde-9c22-4d12-e2ab-f8d7c9842227"
      },
      "source": [
        "train['data'].shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 3, 40, 40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "_jTC2OCXx3VI",
        "outputId": "925c9047-ee5a-4350-c218-2c5402ddae1b"
      },
      "source": [
        "model3l(train['data'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3bce24164572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel3l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a57e9487eea1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m#only compute nodes that are not supplied as inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6yotGPJyJ0b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}